[
    {
        "abstract": "  Massive Ultracompact Galaxies (MUGs) are common at z=2-3, but very rare in\nthe nearby Universe. Simulations predict that the few surviving MUGs should\nreside in galaxy clusters, whose large relative velocities prevent them from\nmerging, thus maintaining their original properties (namely stellar\npopulations, masses, sizes and dynamical state). We take advantage of the\nhigh-completeness, large-area spectroscopic GAMA survey, complementing it with\ndeeper imaging from the KiDS and VIKING surveys. We find a set of 22 bona-fide\nMUGs, defined as having high stellar mass (>8x10^10 M_Sun) and compact size\n(R_e<2 Kpc) at 0.02 < z < 0.3. An additional set of 7 lower-mass objects\n(6x10^10 < M_star/M_Sun < 8x10^10) are also potential candidates according to\ntypical mass uncertainties. The comoving number density of MUGs at low redshift\n(z < 0.3) is constrained at $(1.0\\pm 0.4)x 10^-6 Mpc^-3, consistent with galaxy\nevolution models. However, we find a mixed distribution of old and young\ngalaxies, with a quarter of the sample representing (old) relics. MUGs have a\npredominantly early/swollen disk morphology (Sersic index 1<n<2.5) with high\nstellar surface densities (<Sigma_e> ~ 10^10 M_Sun Kpc^-2). Interestingly, a\nlarge fraction feature close companions -- at least in projection -- suggesting\nthat many (but not all) reside in the central regions of groups. Halo masses\nshow these galaxies inhabit average-mass groups. As MUGs are found to be almost\nequally distributed among environments of different masses, their relative\nfraction is higher in more massive overdensities, matching the expectations\nthat some of these galaxies fell in these regions at early times. However,\nthere must be another channel leading some of these galaxies to an abnormally\nlow merger history because our sample shows a number of objects that do not\ninhabit particularly dense environments. (abridged)\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We take advantage of the high-completeness, large-area spectroscopic GAMA survey, complementing it with deeper imaging from the KiDS and VIKING surveys.\""
    },
    {
        "abstract": "  Sequential state estimation in non-linear and non-Gaussian state spaces has a\nwide range of applications in statistics and signal processing. One of the most\neffective non-linear filtering approaches, particle filtering, suffers from\nweight degeneracy in high-dimensional filtering scenarios. Several avenues have\nbeen pursued to address high-dimensionality. Among these, particle flow\nparticle filters construct effective proposal distributions by using invertible\nflow to migrate particles continuously from the prior distribution to the\nposterior, and sequential Markov chain Monte Carlo (SMCMC) methods use a\nMetropolis-Hastings (MH) accept-reject approach to improve filtering\nperformance. In this paper, we propose to combine the strengths of invertible\nparticle flow and SMCMC by constructing a composite Metropolis-Hastings (MH)\nkernel within the SMCMC framework using invertible particle flow. In addition,\nwe propose a Gaussian mixture model (GMM)-based particle flow algorithm to\nconstruct effective MH kernels for multi-modal distributions. Simulation\nresults show that for high-dimensional state estimation example problems the\nproposed kernels significantly increase the acceptance rate with minimal\nadditional computational overhead and improve estimation accuracy compared with\nstate-of-the-art filtering algorithms.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We propose to combine the strengths of invertible particle flow and SMCMC by constructing a composite Metropolis-Hastings (MH) kernel within the SMCMC framework using invertible particle flow.\""
    },
    {
        "abstract": "  In this paper, we propose VLASE, a framework to use semantic edge features\nfrom images to achieve on-road localization. Semantic edge features denote edge\ncontours that separate pairs of distinct objects such as building-sky, road-\nsidewalk, and building-ground. While prior work has shown promising results by\nutilizing the boundary between prominent classes such as sky and building using\nskylines, we generalize this approach to consider semantic edge features that\narise from 19 different classes. Our localization algorithm is simple, yet very\npowerful. We extract semantic edge features using a recently introduced CASENet\narchitecture and utilize VLAD framework to perform image retrieval. Our\nexperiments show that we achieve improvement over some of the state-of-the-art\nlocalization algorithms such as SIFT-VLAD and its deep variant NetVLAD. We use\nablation study to study the importance of different semantic classes and show\nthat our unified approach achieves better performance compared to individual\nprominent features such as skylines.\n",
        "method": "We extract semantic edge features using a recently introduced CASENet architecture and utilize VLAD framework to perform image retrieval."
    },
    {
        "abstract": "  We introduce fully scalable Gaussian processes, an implementation scheme that\ntackles the problem of treating a high number of training instances together\nwith high dimensional input data. Our key idea is a representation trick over\nthe inducing variables called subspace inducing inputs. This is combined with\ncertain matrix-preconditioning based parametrizations of the variational\ndistributions that lead to simplified and numerically stable variational lower\nbounds. Our illustrative applications are based on challenging extreme\nmulti-label classification problems with the extra burden of the very large\nnumber of class labels. We demonstrate the usefulness of our approach by\npresenting predictive performances together with low computational times in\ndatasets with extremely large number of instances and input dimensions.\n",
        "method": "Our key idea is a representation trick over the inducing variables called subspace inducing inputs."
    },
    {
        "abstract": "  Gravitational wave detectors in the LIGO/Virgo frequency band are able to\nmeasure the individual masses and the composite tidal deformabilities of\nneutron-star binary systems. This paper demonstrates that high accuracy\nmeasurements of these quantities from an ensemble of binary systems can in\nprinciple be used to determine the high density neutron-star equation of state\nexactly. This analysis assumes that all neutron stars have the same\nthermodynamically stable equation of state, but does not use simplifying\napproximations for the composite tidal deformability or make additional\nassumptions about the high density equation of state.\n",
        "method": "This paper demonstrates that high accuracy measurements of these quantities from an ensemble of binary systems can in principle be used to determine the high density neutron-star equation of state exactly."
    },
    {
        "abstract": "  Monolayer-thick hexagonal boron nitride (h-BN) is grown on graphene on\nSiC(0001), by exposure of the graphene to borazine, (BH)3(NH)3, at 1100 C. The\nh-BN films form ~2-micrometer size grains with a preferred orientation of 30\ndegrees relative to the surface graphene. Low-energy electron microscopy is\nemployed to provide definitive signatures of the number and composition of\ntwo-dimensional (2D) planes across the surface. These grains are found to form\nby substitution for the surface graphene, with the C atoms produced by this\nsubstitution then being incorporated below the h-BN (at the interface between\nthe existing graphene and the SiC) to form a new graphene plane.\n",
        "method": "Monolayer-thick hexagonal boron nitride (h-BN) is grown on graphene on SiC(0001), by exposure of the graphene to borazine, (BH)3(NH)3, at 1100 C."
    },
    {
        "abstract": "  We study several important fine properties for the family of fractional\nBrownian motions with Hurst parameter $H$ under the $(p,r)$-capacity on\nclassical Wiener space introduced by Malliavin. We regard fractional Brownian\nmotions as Wiener functionals via the integral representation discovered by\nDecreusefond and \\\"{U}st\\\"{u}nel, and show non differentiability, modulus of\ncontinuity, law of iterated Logarithm(LIL) and self-avoiding properties of\nfractional Brownian motion sample paths using Malliavin calculus as well as the\ntools developed in the previous work by Fukushima, Takeda and etc. for Brownian\nmotion case.\n",
        "method": "We regard fractional Brownian motions as Wiener functionals via the integral representation discovered by Decreusefond and \\\"Ust\\\"unel."
    },
    {
        "abstract": "  We study the dynamics of dilute and ultracold bosonic gases in a quasi\ntwo-dimensional (2D) configuration and in the collisionless regime. We adopt\nthe 2D Landau-Vlasov equation to describe a three-dimensional gas under very\nstrong harmonic confinement along one direction. We use this effective equation\nto investigate the speed of sound in quasi 2D bosonic gases, i.e. the sound\npropagation around a Bose-Einstein distribution in collisionless 2D gases. We\nderive coupled algebraic equations for the real and imaginary parts of the\nsound velocity, which are then solved taking also into account the equation of\nstate of the 2D bosonic system. Above the Berezinskii-Kosterlitz-Thouless\ncritical temperature we find that there is rapid growth of the imaginary\ncomponent of the sound velocity which implies a strong Landau damping. Quite\nremarkably, our theoretical results are in good agreement with very recent\nexperimental data obtained with a uniform 2D Bose gas of $^{87}$Rb atoms.\n",
        "method": "Here is the methodological sentence:\n\nWe adopt the 2D Landau-Vlasov equation to describe a three-dimensional gas under very strong harmonic confinement along one direction."
    },
    {
        "abstract": "  We consider results for the master integrals of the kite family, given in\nterms of ELi-functions which are power series in the nome $q$ of an elliptic\ncurve. The analytic continuation of these results beyond the Euclidean region\nis reduced to the analytic continuation of the two period integrals which\ndefine $q.$ We discuss the solution to the latter problem from the perspective\nof the Picard-Lefschetz formula.\n",
        "method": "The methodological sentence is:\n\nWe consider results for the master integrals of the kite family, given in terms of ELi-functions which are power series in the nome $q$ of an elliptic curve."
    },
    {
        "abstract": "  In this paper we study continuous semigroups of positive operators on general\nvector lattices equipped with the relative uniform topology $\\tau_{ru}$. We\nintroduce the notions of strong continuity with respect to $\\tau_{ru}$ and\nrelative uniform continuity for semigroups. These notions allow us to study\nsemigroups on non-locally convex spaces such as $L^p(\\mathbb{R})$ for $0<p<1$\nand non-complete spaces such as $Lip(\\mathbb{R})$, $UC(\\mathbb{R})$, and\n$C_c(\\mathbb{R})$. We show that the (left) translation semigroup on the real\nline, the heat semigroup and some Koopman semigroups are relatively uniformly\ncontinuous on a variety of spaces.\n",
        "method": "We introduce the notions of strong continuity with respect to $\\tau_{ru}$ and relative uniform continuity for semigroups."
    },
    {
        "abstract": "  These notes were born out of a five-hour lecture series for graduate students\nat the May 2018 Snowbird workshop Crossing the Walls in Enumerative Geometry.\nAfter a short primer on equivariant cohomology and localization, we provide\nproofs of the genus-zero mirror theorems for the quintic threefold, first in\nFan-Jarvis-Ruan-Witten theory and then in Gromov-Witten theory. We make no\nclaim to originality, except in exposition, where special emphasis is placed on\npeeling away the standard technical machinery and viewing the mirror theorems\nas closed-formula manifestations of elementary localization recursions.\n",
        "method": "Here is the methodological sentence:\n\nWe provide proofs of the genus-zero mirror theorems for the quintic threefold, first in Fan-Jarvis-Ruan-Witten theory and then in Gromov-Witten theory."
    },
    {
        "abstract": "  This paper considers the lexicographical challenge of defining actions a\nperson takes while eating. The goal is to establish objective and repeatable\ngesture definitions based on discernible intent. Such a standard would support\nthe sharing of data and results between researchers working on the problem of\nautomatic monitoring of dietary intake. We define five gestures: taking a bite\nof food (bite), sipping a drink of liquid (drink), manipulating food for\npreparation of intake (utensiling), not moving (rest) and a non-eating category\n(other). To test this lexicography, we used our definitions to label a large\ndata set and tested for inter-rater reliability. The data set consists of a\ntotal of 276 participants eating a single meal while wearing a watch-like\ndevice to track wrist motion. Video was simultaneously recorded and\nsubsequently reviewed to label gestures. A total of 18 raters manually labeled\n51,614 gestures. Every meal was labeled by at least 1 rater, with 95 meals\nlabeled by 2 raters. Inter-rater reliability was calculated in terms of\nagreement, boundary ambiguity, and mistakes. Results were 92.5% agreement (75%\nexact agreement, 17.5% boundary ambiguity). Mistakes of intake gestures (0.6%\nbite and 1.9% drink) occur much less frequently than non-intake gestures (16.5%\nutensiling and 8.7% rest). Similar rates were found across all 18 raters.\nFinally, a comparison of gesture segments against single index labels of bites\nand drinks from a previous effort showed an agreement of 95.8% with 0.6%\nambiguity and 3.6% mistakes. Overall, these findings take a step towards\ndeveloping a consensus lexicography of eating gestures for the research\ncommunity.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We used our definitions to label a large data set and tested for inter-rater reliability.\n* The data set consists of a total of 276 participants eating a single meal while wearing a watch-like device to track wrist motion.\n* Video was simultaneously recorded and subsequently reviewed to label gestures.\n* A total of 18 raters manually labeled 51,614 gestures.\n* Every meal was labeled by at least 1 rater, with 95 meals labeled by 2 raters.\n* Inter-rater reliability was calculated in terms of agreement, boundary ambiguity, and mistakes."
    },
    {
        "abstract": "  Dirac states hosted by Sb/Bi square nets are known to exist in the layered\nantiferromagnetic AMnX$_2$ (A = Ca/Sr/Ba/Eu/Yb, X=Sb/Bi) material family the\nspace group to be P4/nmm or I4/mmm. In this paper, we present a comprehensive\nstudy of quantum transport behaviors, angle-resolved photoemission spectroscopy\n(ARPES) and first-principles calculations on SrZnSb2, a nonmagnetic analogue to\nAMnX2, which crystallizes in the pnma space group with distorted square nets.\nFrom the quantum oscillation measurements up to 35 T, three major frequencies\nincluding F$_1$ = 103 T, F$_2$ = 127 T and F$_3$ = 160 T, are identified. The\neffective masses of the quasiparticles associated with these frequencies are\nextracted, namely, m*$_1$ = 0.1 m$_e$, m*$_2$ = 0.1 m$_e$ and m*$_3$ =\n0.09m$_e$, where m$_e$ is the free electron mass. From the three-band\nLifshitz-Kosevich fit, the Berry phases accumulated along the cyclotron orbit\nof the quasiparticles are 0.06$\\pi$, 1.2$\\pi$ and 0.74$\\pi$ for F$_1$, F$_2$\nand F$_3$, respectively. Combined with the ARPES data and the first-principles\ncalculations, we reveal that F2 and F3 are associated with the two nontrivial\nFermi pockets at the Brillouin zone edge while F1 is associated with the\ntrivial Fermi pocket at the zone center. In addition, the first-principles\ncalculations further suggest the existence of Dirac nodal line in the band\nstructure of SrZnSb$_2$.\n",
        "method": "Here are the methodological sentences:\n\n* From quantum oscillation measurements up to 35 T, three major frequencies including F1 = 103 T, F2 = 127 T and F3 = 160 T, are identified.\n* The effective masses of the quasiparticles associated with these frequencies are extracted, namely, m*_1 = 0.1 m_e, m*_2 = 0.1 m_e and m*_3 = 0.09m_e, where m_e is the free electron mass.\n* From the three-band Lifshitz-Kosevich fit, the Berry phases accumulated along the cyclotron orbit of the quasiparticles are 0.06\u03c0, 1.2\u03c0 and 0.74\u03c0 for F1, F2 and F3, respectively."
    },
    {
        "abstract": "  We present a convolutional network that is equivariant to rigid body motions.\nThe model uses scalar-, vector-, and tensor fields over 3D Euclidean space to\nrepresent data, and equivariant convolutions to map between such\nrepresentations. These SE(3)-equivariant convolutions utilize kernels which are\nparameterized as a linear combination of a complete steerable kernel basis,\nwhich is derived analytically in this paper. We prove that equivariant\nconvolutions are the most general equivariant linear maps between fields over\nR^3. Our experimental results confirm the effectiveness of 3D Steerable CNNs\nfor the problem of amino acid propensity prediction and protein structure\nclassification, both of which have inherent SE(3) symmetry.\n",
        "method": "Methodological sentences:\n\n* The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations.\n* These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper."
    },
    {
        "abstract": "  In this work, we study the trade-off between the cache capacity and the user\ndelay for a cooperative Small Base Station (SBS) coded caching system with\nmobile users. First, a delay-aware coded caching policy, which takes into\naccount the popularity of the files and the maximum re-buffering delay to\nminimize the average rebuffering delay of a mobile user under a given cache\ncapacity constraint is introduced. Subsequently, we address a scenario where\nsome files are served by the macro-cell base station (MBS) when the cache\ncapacity of the SBSs is not sufficient to store all the files in the library.\nFor this scenario, we develop a coded caching policy that minimizes the average\namount of data served by the MBS under an average re-buffering delay\nconstraint.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* First, a delay-aware coded caching policy, which takes into account the popularity of the files and the maximum re-buffering delay to minimize the average rebuffering delay of a mobile user under a given cache capacity constraint is introduced.\n* For this scenario, we develop a coded caching policy that minimizes the average amount of data served by the MBS under an average re-booking delay constraint."
    },
    {
        "abstract": "  An analysis is given of inflation based on a supersymmetric Dirac-Born-Infeld\n(DBI) action in an axionic landscape. The DBI model we discuss involves a\nlandscape of chiral superfields with one $U(1)$ shift symmetry which is broken\nby instanton type non-perturbative terms in the superpotential. Breaking of the\nshift symmetry leads to one pseudo-Nambu-Goldstone-boson which acts as the\ninflaton while the remaining normalized phases of the chiral fields generically\nlabeled axions are invariant under the $U(1)$ shift symmetry. The analysis is\ncarried out in the vacuum with stabilized saxions, which are the magnitudes of\nthe chiral fields. Regions of the parameter space where slow-roll inflation\noccurs are exhibited and the spectral indices as well as the ratio of the\ntensor to the scalar power spectrum are computed. An interesting aspect of\nsupersymmetric DBI models analyzed is that in most of the parameter space\ntensor to scalar ratio and scalar spectral index are consistent with Planck\ndata if slow roll occurs and is not eternal. Also interesting is that the ratio\nof the tensor to the scalar power spectrum can be large and can lie close to\nthe experimental upper limit and thus testable in improved experiment.\nNon-Gaussianity in this class of models is explored.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The analysis is carried out in the vacuum with stabilized saxions, which are the magnitudes of the chiral fields.\n* Regions of the parameter space where slow-roll inflation occurs are exhibited and the spectral indices as well as the ratio of the tensor to the scalar power spectrum are computed."
    },
    {
        "abstract": "  Time-driven quantum systems are important in many different fields of physics\nlike cold atoms, solid state, optics, etc. Many of their properties are encoded\nin the time evolution operator which is calculated by using a time-ordered\nproduct of actions. The solution to this problem is equivalent to find an\neffective Hamiltonian. This task is usually very complex and either requires\napproximations, or in very particular and rare cases, a system-dependent method\ncan be found. Here we provide a general scheme that allows to find such\neffective Hamiltonian. The method is based in using the structure of the\nassociated Lie group and a decomposition of the evolution on each group\ngenerator. The time evolution is thus always transformed in a system of\nordinary non-linear differential equations for a set of coefficients. In many\ncases this system can be solved by symbolic computational algorithms. As an\nexample, an exact solution to three well known problems is provided. For two of\nthem, the modulated optical lattice and Kapitza pendulum, the exact solutions,\nwhich were already known, are reproduced. For the other example, the Paul trap,\nno exact solutions were known. Here we find such exact solution, and as\nexpected, contain the approximate solutions found by other authors.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe method is based in using the structure of the associated Lie group and a decomposition of the evolution on each group generator."
    },
    {
        "abstract": "  Sparse structures are frequently sought when pursuing tractability in\noptimization problems. They are exploited from both theoretical and\ncomputational perspectives to handle complex problems that become manageable\nwhen sparsity is present. An example of this type of structure is given by\ntreewidth: a graph theoretical parameter that measures how \"tree-like\" a graph\nis. This parameter has been used for decades for analyzing the complexity of\nvarious optimization problems and for obtaining tractable algorithms for\nproblems where this parameter is bounded. The goal of this work is to\ncontribute to the understanding of the limits of the treewidth-based\ntractability in optimization. Our results are as follows. First, we prove that,\nin a certain sense, the already known positive results on extension complexity\nbased on low treewidth are the best possible. Secondly, under mild assumptions,\nwe prove that treewidth is the only graph-theoretical parameter that yields\ntractability a wide class of optimization problems, a fact well known in\nGraphical Models in Machine Learning and in Constraint Satisfaction Problems,\nwhich here we extend to an approximation setting in Optimization.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Our results are as follows.\n* Firstly, we prove that... \n* Secondly, under mild assumptions, we prove that...\n\nNote: These sentences describe the methods or approaches used in the research, including the presentation of the results and the assumptions made."
    },
    {
        "abstract": "  Unsupervised domain adaptation techniques have been successful for a wide\nrange of problems where supervised labels are limited. The task is to classify\nan unlabeled `target' dataset by leveraging a labeled `source' dataset that\ncomes from a slightly similar distribution. We propose metric-based adversarial\ndiscriminative domain adaptation (M-ADDA) which performs two main steps. First,\nit uses a metric learning approach to train the source model on the source\ndataset by optimizing the triplet loss function. This results in clusters where\nembeddings of the same label are close to each other and those with different\nlabels are far from one another. Next, it uses the adversarial approach (as\nthat used in ADDA \\cite{2017arXiv170205464T}) to make the extracted features\nfrom the source and target datasets indistinguishable. Simultaneously, we\noptimize a novel loss function that encourages the target dataset's embeddings\nto form clusters. While ADDA and M-ADDA use similar architectures, we show that\nM-ADDA performs significantly better on the digits adaptation datasets of MNIST\nand USPS. This suggests that using metric-learning for domain adaptation can\nlead to large improvements in classification accuracy for the domain adaptation\ntask. The code is available at \\url{https://github.com/IssamLaradji/M-ADDA}.\n",
        "method": "We propose metric-based adversarial discriminative domain adaptation (M-ADDA) which performs two main steps. First, it uses a metric learning approach to train the source model on the source dataset by optimizing the triplet loss function."
    },
    {
        "abstract": "  Scheduling a set of jobs over a collection of machines is a fundamental\nproblem that needs to be solved millions of times a day in various computing\nplatforms: in operating systems, in large data clusters, and in data centers.\nAlong with makespan, flow-time, which measures the length of time a job spends\nin a system before it completes, is arguably the most important metric to\nmeasure the performance of a scheduling algorithm. In recent years, there has\nbeen a remarkable progress in understanding flow-time based objective functions\nin diverse settings such as unrelated machines scheduling, broadcast\nscheduling, multi-dimensional scheduling, to name a few.\n  Yet, our understanding of the flow-time objective is limited mostly to the\nscenarios where jobs have simple structures; in particular, each job is a\nsingle self contained entity. On the other hand, in almost all real world\napplications, think of MapReduce settings for example, jobs have more complex\nstructures. In this paper, we consider two classical scheduling models that\ncapture complex job structures: 1) concurrent open-shop scheduling and 2)\nprecedence constrained scheduling. Our main motivation to study these problems\nspecifically comes from their relevance to two scheduling problems that have\ngained importance in the context of data centers: co-flow scheduling and DAG\nscheduling. We design almost optimal approximation algorithms for open-shop\nscheduling and precedence constrained scheduling, and show hardness results.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We consider two classical scheduling models that capture complex job structures: 1) concurrent open-shop scheduling and 2) precedence constrained scheduling.\""
    },
    {
        "abstract": "  The place and role of parsing analysis in formation of professional\ninformatics competences of future informatics teachers is determined. Separated\nautomation tools for lexical (lex) and syntax (yacc) analysis invariant to the\nprogramming language used. The expediency of using functional programming\nlanguages Scheme and SML is shown for learning how to develop compilers in the\ncourse of programming theory. The example of the MosML dialect illustrates the\nmain components of the methodic of joint using the tools of automation of\nlexical and parsing analysis in the process of teaching the programming theory\nof future informatics teachers. The main conclusions and recommendations: 1)\nthe considered example of the expanded calculator can be refined by changing\nthe grammar, in particular - for the introduction of conditional and cyclic\nconstructions; 2) the proposed scheme can be used to implement the interpreter\nof any formal language with an arbitrary typing method - the appropriate\nexamples of study will be subsets of procedural languages Basic and C and\nfunctional languages Scheme and SML: provided the addition of the machine code\ngeneration phase, this provides an opportunity to demonstrate the full\ndevelopment cycle for programming language compiler.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n1. Separated automation tools for lexical (lex) and syntax (yacc) analysis invariant to the programming language used.\n2. The expediency of using functional programming languages Scheme and SML is shown for learning how to develop compilers in the course of programming theory.\n3. The main components of the methodic of joint using the tools of automation of lexical and parsing analysis in the process of teaching the programming theory of future informatics teachers are illustrated by the example of the MosML dialect."
    },
    {
        "abstract": "  We propose a scheme of a universal block of broadband quantum memory\nconsisting of three ring microresonators forming a controllable frequency comb\nand interacting with each other and with a common waveguide. We find the\noptimal parameters of the microresonators showing the possibility of highly\nefficient storage of light fields on this memory block and we demonstrate the\nprocedure for gluing several memory blocks for increasing spectral range of the\ncomposite quantum memory while maintaining high efficiency.\n",
        "method": "We propose a scheme of a universal block of broadband quantum memory consisting of three ring microresonators forming a controllable frequency comb and interacting with each other and with a common waveguide."
    },
    {
        "abstract": "  We present measurements of the X-ray observables of the intra-cluster medium\n(ICM), including luminosity $L_X$, ICM mass $M_{ICM}$, emission-weighted mean\ntemperature $T_X$, and integrated pressure $Y_X$, that are derived from\nXMM-Newton X-ray observations of a Sunyaev-Zel'dovich Effect (SZE) selected\nsample of 59 galaxy clusters from the South Pole Telescope SPT-SZ survey that\nspan the redshift range of $0.20 < z < 1.5$. We constrain the best-fit power\nlaw scaling relations between X-ray observables, redshift, and halo mass. The\nhalo masses are estimated based on previously published SZE observable to mass\nscaling relations, calibrated using information that includes the halo mass\nfunction. Employing SZE-based masses in this sample enables us to constrain\nthese scaling relations for massive galaxy clusters ($M_{500}\\geq 3\n\\times10^{14}$ $M_\\odot$) to the highest redshifts where these clusters exist\nwithout concern for X-ray selection biases. We find that the mass trends are\nsteeper than self-similarity in all cases, and with $\\geq 2.5{\\sigma}$\nsignificance in the case of $L_X$ and $M_{ICM}$. The redshift trends are\nconsistent with the self-similar expectation, but the uncertainties remain\nlarge. Core-included scaling relations tend to have steeper mass trends for\n$L_X$. There is no convincing evidence for a redshift-dependent mass trend in\nany observable. The constraints on the amplitudes of the fitted scaling\nrelations are currently limited by the systematic uncertainties on the\nSZE-based halo masses, however the redshift and mass trends are limited by the\nX-ray sample size and the measurement uncertainties of the X-ray observables.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We present measurements of the X-ray observables of the intra-cluster medium (ICM), including luminosity $L_X$, ICM mass $M_{ICM}$, emission-weighted mean temperature $T_X$, and integrated pressure $Y_X$, that are derived from XMM-Newton X-ray observations of a Sunyaev-Zel'dovich Effect (SZE) selected sample of 59 galaxy clusters...\""
    },
    {
        "abstract": "  The elastic properties of neutron star crusts are relevant for a variety of\ncurrently observable or near-future electromagnetic and gravitational wave\nphenomena. These phenomena may depend on the elastic properties of nuclear\npasta found in the inner crust. We present large scale classical molecular\ndynamics simulations where we deform nuclear pasta. We simulate idealized\nsamples of nuclear pasta and describe their breaking mechanism. We also deform\nnuclear pasta that is arranged into many domains, similar to what is known for\nthe ions in neutron star crusts. Our results show that nuclear pasta may be the\nstrongest known material, perhaps with a shear modulus of\n$10^{30}\\,\\text{ergs/cm}^3$ and breaking strain greater than 0.1.\n",
        "method": "We present large scale classical molecular dynamics simulations where we deform nuclear pasta. We simulate idealized samples of nuclear pasta and describe their breaking mechanism."
    },
    {
        "abstract": "  With the rapid growth of Internet of Things (IoT) devices, the next\ngeneration mobile networks demand for more operating frequency bands. By\nleveraging the underutilized radio spectrum, the cognitive radio (CR)\ntechnology is considered as a promising solution for spectrum scarcity problem\nof IoT applications. In parallel with the development of CR techniques,\nWireless Energy Harvesting (WEH) is considered as one of the emerging\ntechnologies to eliminate the need of recharging or replacing the batteries for\nIoT and CR networks. To this end, we propose to utilize WEH for CR networks in\nwhich the CR devices are not only capable of sensing the available radio\nfrequencies in a collaborative manner but also harvesting the wireless energy\ntransferred by an Access Point (AP). More importantly, we design an\noptimization framework that captures a fundamental tradeoff between energy\nefficiency (EE) and spectral efficiency (SE) of the network. In particular, we\nformulate a Mixed Integer Nonlinear Programming (MINLP) problem that maximizes\nEE while taking into consideration of users' buffer occupancy, data rate\nfairness, energy causality constraints and interference constraints. We further\nprove that the proposed optimization framework is an NP-Hard problem. Thus, we\npropose a low complex heuristic algorithm, called INSTANT, to solve the\nresource allocation and energy harvesting optimization problem. The proposed\nalgorithm is shown to be capable of achieving near optimal solution with high\naccuracy while having polynomial complexity. The efficiency of our proposal is\nvalidated through well designed simulations.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We propose to utilize Wireless Energy Harvesting (WEH) for CR networks in which the CR devices are not only capable of sensing the available radio frequencies in a collaborative manner but also harvesting the wireless energy transferred by an Access Point (AP).\n* We design an optimization framework that captures a fundamental tradeoff between energy efficiency (EE) and spectral efficiency (SE) of the network.\n* We formulate a Mixed Integer Nonlinear Programming (MINLP) problem that maximizes EE while taking into consideration of users' buffer occupancy, data rate fairness, energy causality constraints and interference constraints."
    },
    {
        "abstract": "  We present a catalogue of 73,221 white dwarf candidates extracted from the\nastrometric and photometric data of the recently published Gaia DR2 catalogue.\nWhite dwarfs were selected from the Gaia Hertzsprung-Russell diagram with the\naid of the most updated population synthesis simulator. Our analysis shows that\nGaia has virtually identified all white dwarfs within 100 pc from the Sun.\nHence, our sub-population of 8,555 white dwarfs within this distance limit and\nthe colour range considered, $-\\,0.52<(G_{\\rm BP}-G_{\\rm RP})<0.80$, is the\nlargest and most complete volume-limited sample of such objects to date. From\nthis sub-sample we identified 8,343 CO-core and 212 ONe-core white dwarf\ncandidates and derived a white dwarf space density of\n$4.9\\pm0.4\\times10^{-3}\\,{\\rm pc^{-3}}$. A bifurcation in the\nHertzsprung-Russell diagram for these sources, which our models do not predict,\nis clearly visible. We used the Virtual Observatory tool VOSA to derive\neffective temperatures and luminosities for our sources by fitting their\nspectral energy distributions, that we built from the UV to the NIR using\npublicly available photometry through the Virtual Observatory. From these\nparameters, we derived the white dwarf radii. Interpolating the radii and\neffective temperatures in hydrogen-rich white dwarf cooling sequences, we\nderived the surface gravities and masses. The Gaia 100 pc white dwarf\npopulation is clearly dominated by cool ($\\sim$ 8,000 K) objects and reveals a\nsignificant population of massive ($M \\sim 0.8 M_{\\odot}$) white dwarfs, of\nwhich no more than $\\sim$ $30-40 \\%$ can be attributed to hydrogen-deficient\natmospheres, and whose origin remains uncertain.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Our analysis shows that Gaia has virtually identified all white dwarfs within 100 pc from the Sun.\n* From this sub-sample we identified 8,343 CO-core and 212 ONe-core white dwarf candidates...\n* We used the Virtual Observatory tool VOSA to derive effective temperatures and luminosities for our sources by fitting their spectral energy distributions...\n* From these parameters, we derived the white dwarf radii.\n* Interpolating the radii and effective temperatures in hydrogen-rich white dwarf cooling sequences, we derived the surface gravities and masses."
    },
    {
        "abstract": "  We present high angular resolution imaging ($23.9 \\times 11.3$ mas, $138.6\n\\times 65.5$ pc) of the radio-loud quasar PSO~J352.4034$-$15.3373 at $z=5.84$\nwith the Very Long Baseline Array (VLBA) at 1.54 GHz. This quasar has the\nhighest radio-to-optical flux density ratio at such a redshift, making it the\nradio-loudest source known to date at $z \\sim 6$. The VLBA observations\npresented here resolve this quasar into multiple components with an overall\nlinear extent of 1.62 kpc ($0\\rlap{.}{''}28$) and with a total flux density of\n$6.57 \\pm 0.38$ mJy, which is about half of the emission measured at a much\nlower angular resolution. The morphology of the source is comparable with\neither a radio core with a one-sided jet, or a compact or a medium-size\nSymmetric Object (CSO/MSO). If the source is a CSO/MSO, and assuming an advance\nspeed of $0.2c$, then the estimated kinematic age is $\\sim 10^4$ yr.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We present high angular resolution imaging ($23.9 \\times 11.3$ mas, $138.6 \\times 65.5$ pc) of the radio-loud quasar PSO~J352.4034$-$15.3373 at $z=5.84$ with the Very Long Baseline Array (VLBA) at 1.54 GHz.\n* The VLBA observations presented here resolve this quasar into multiple components..."
    },
    {
        "abstract": "  We show that the superhard boride WB$_{4.2}$ is a superconductor with a T$_c$\nof 2.05(5) K. Temperature-dependent magnetic susceptibility, electrical\nresistivity, and specific heat measurements were used to characterize the\nsuperconducting transition. The Sommerfeld constant {\\gamma} for WB$_{4.2}$ is\n2.07(3) mJ mol$^{-1}$ K$^{-2}$ and the {\\Delta}C/{\\gamma}T$_c$ = 1.56, which is\nsomewhat higher than what is expected for weakly coupled BCS type\nsuperconductors. The H$_{c2}$ vs T plot is linear over a wide temperature range\nbut does show signs of flattening by the lowest temperatures studied and\ntherefore the zero-temperature upper critical field ({\\mu}$_0$H$_{c2}$(0)) for\nWB$_{4.2}$ lies somewhere between the linear extrapolation of\n{\\mu}$_0$H$_{c2}$(T) to 0 K and expectations based on the WHH model.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Temperature-dependent magnetic susceptibility, electrical resistivity, and specific heat measurements were used to characterize the superconducting transition.\""
    },
    {
        "abstract": "  One of the major performance and scalability bottlenecks in large scientific\napplications is parallel reading and writing to supercomputer I/O systems. The\nusage of parallel file systems and consistency requirements of POSIX, that all\nthe traditional HPC parallel I/O interfaces adhere to, pose limitations to the\nscalability of scientific applications. Object storage is a widely used storage\ntechnology in cloud computing and is more frequently proposed for HPC workload\nto address and improve the current scalability and performance of I/O in\nscientific applications. While object storage is a promising technology, it is\nstill unclear how scientific applications will use object storage and what the\nmain performance benefits will be. This work addresses these questions, by\nemulating an object storage used by a traditional scientific application and\nevaluating potential performance benefits. We show that scientific applications\ncan benefit from the usage of object storage on large scales.\n",
        "method": "We emulate an object storage used by a traditional scientific application and evaluate potential performance benefits."
    },
    {
        "abstract": "  We introduce an approximation technique for nonlinear hyperbolic systems with\nsources that is invariant domain preserving. The method is\ndiscretization-independent provided elementary symmetry and skew-symmetry\nproperties are satisfied by the scheme. The method is formally first-order\naccurate in space. A series of higher-order methods is also introduced. When\nthese methods violate the invariant domain properties, they are corrected by a\nlimiting technique that we call convex limiting. After limiting, the resulting\nmethods satisfy all the invariant domain properties that are imposed by the\nuser (see Theorem~7.24). A key novelty is that the bounds that are enforced on\nthe solution at each time step are necessarily satisfied by the low-order\napproximation.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We introduce an approximation technique for nonlinear hyperbolic systems with sources that is invariant domain preserving.\n* The method is discretization-independent provided elementary symmetry and skew-symmetry properties are satisfied by the scheme.\n* A series of higher-order methods is also introduced."
    },
    {
        "abstract": "  Due to the recent advances in vehicular ad hoc networks (VANETs), smart\napplications have been incorporating the data generated from these networks to\nprovide quality of life services. In this paper, we have proposed taxonomy of\ndata mining techniques that have been applied in this domain in addition to a\nclassification of these techniques. Our contribution is to highlight the\nresearch methodologies in the literature and allow for comparing among them\nusing different characteristics. The proposed taxonomy covers elementary data\nmining techniques such as: preprocessing, outlier detection, clustering, and\nclassification of data. In addition, it covers centralized, distributed,\noffline, and online techniques from the literature.\n",
        "method": "Our contribution is to highlight the research methodologies in the literature and allow for comparing among them using different characteristics."
    },
    {
        "abstract": "  Ultra-dense networks (UDNs) envision the massive deployment of heterogenous\nbase stations (BSs) to meet the desired traffic demands. Furthermore, UDNs are\nexpected to support the diverse devices e.g., personal mobile devices and\nunmanned ariel vehicles. User mobility and the resulting excessive changes in\nuser to BS associations in such highly dense networks may however nullify the\ncapacity gains foreseen through BS densification. Thus there exists a need to\nquantify the effect of user mobility in UDNs. In this article, we consider a\nthree-dimensional N-tier downlink network and determine the association\nprobabilities and inter/intra tier handover rates using tools from stochastic\ngeometry. In particular, we incorporate user and BSs' antenna heights into the\nmathematical analysis and study the impact of user height on the association\nand handover rate. The numerical trends show that the intra-tier handovers are\ndominant for the tiers with shortest relative elevation w.r.t. the user and\nthis dominance is more prominent when there exists a high discrepancy among the\ntiers' heights. However, biasing can be employed to balance the handover load\namong the network tiers.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In this article, we consider a three-dimensional N-tier downlink network and determine the association probabilities and inter/intra tier handover rates using tools from stochastic geometry.\n* We incorporate user and BSs' antenna heights into the mathematical analysis and study the impact of user height on the association and handover rate."
    },
    {
        "abstract": "  The paper extends Bayesian networks (BNs) by a mechanism for dynamic changes\nto the probability distributions represented by BNs. One application scenario\nis the process of knowledge acquisition of an observer interacting with a\nsystem. In particular, the paper considers condition/event nets where the\nobserver's knowledge about the current marking is a probability distribution\nover markings. The observer can interact with the net to deduce information\nabout the marking by requesting certain transitions to fire and observing their\nsuccess or failure.\n  Aiming for an efficient implementation of dynamic changes to probability\ndistributions of BNs, we consider a modular form of networks that form the\narrows of a free PROP with a commutative comonoid structure, also known as term\ngraphs. The algebraic structure of such PROPs supplies us with a compositional\nsemantics that functorially maps BNs to their underlying probability\ndistribution and, in particular, it provides a convenient means to describe\nstructural updates of networks.\n",
        "method": "Here are the methodological sentences:\n\n* One application scenario is the process of knowledge acquisition of an observer interacting with a system.\n* The observer can interact with the net to deduce information about the marking by requesting certain transitions to fire and observing their success or failure.\n* Aiming for an efficient implementation of dynamic changes to probability distributions of BNs, we consider a modular form of networks that form the arrows of a free PROP with a commutative comonoid structure, also known as term graphs."
    },
    {
        "abstract": "  An adversarial machine learning approach is introduced to launch jamming\nattacks on wireless communications and a defense strategy is presented. A\ncognitive transmitter uses a pre-trained classifier to predict the current\nchannel status based on recent sensing results and decides whether to transmit\nor not, whereas a jammer collects channel status and ACKs to build a deep\nlearning classifier that reliably predicts the next successful transmissions\nand effectively jams them. This jamming approach is shown to reduce the\ntransmitter's performance much more severely compared with random or\nsensing-based jamming. The deep learning classification scores are used by the\njammer for power control subject to an average power constraint. Next, a\ngenerative adversarial network (GAN) is developed for the jammer to reduce the\ntime to collect the training dataset by augmenting it with synthetic samples.\nAs a defense scheme, the transmitter deliberately takes a small number of wrong\nactions in spectrum access (in form of a causative attack against the jammer)\nand therefore prevents the jammer from building a reliable classifier. The\ntransmitter systematically selects when to take wrong actions and adapts the\nlevel of defense to mislead the jammer into making prediction errors and\nconsequently increase its throughput.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"A cognitive transmitter uses a pre-trained classifier to predict the current channel status based on recent sensing results and decides whether to transmit or not...\""
    },
    {
        "abstract": "  Absolute parameters of 509 main-sequence stars selected from the components\nof detached-eclipsing spectroscopic binaries in the Solar neighbourhood are\nused to study mass-luminosity, mass-radius and mass-effective temperature\nrelations (MLR, MRR and MTR). The MLR function is found better if expressed by\na six-piece classical MLR ($L \\propto M^{\\alpha}$) rather than a fifth or a\nsixth degree polynomial within the mass range of $0.179\\leq M/M_{\\odot}\\leq\n31$. The break points separating the mass-ranges with classical MLR do not\nappear to us to be arbitrary. Instead, the data indicate abrupt changes along\nthe mass axis in the mean energy generation per unit of stellar mass. Unlike\nthe MLR function, the MRR and MTR functions cannot be determined over the full\nrange of masses. A single piece MRR function is calibrated from the radii of\nstars with $M\\leq1.5M_{\\odot}$, while a second single piece MTR function is\nfound for stars with $M>1.5M_{\\odot}$. The missing part of the MRR is computed\nfrom the MLR and MTR, while the missing part of the MTR is computed from the\nMLR and MRR. As a result, we have interrelated MLR, MRR and MTR, which are\nuseful in determining the typical absolute physical parameters of main-sequence\nstars of given masses. These functions are also useful to estimate typical\nabsolute physical parameters from typical $T_{eff}$ values. Thus, we were able\nto estimate the typical absolute physical parameters of main-sequence stars\nobserved in the Sejong Open Cluster survey, based on that survey's published\nvalues for $T_{eff}$. Since typical absolute physical parameters of main\nsequence stars cannot normally be determined in such photometric surveys, the\ninterrelated functions are shown to be useful to compute such missing\nparameters from similar surveys.\n",
        "method": "The methodological sentence is:\n\nAbsolute parameters of 509 main-sequence stars selected from the components of detached-eclipsing spectroscopic binaries in the Solar neighbourhood are used to study mass-luminosity, mass-radius and mass-effective temperature relations (MLR, MRR and MTR)."
    },
    {
        "abstract": "  The electrocardiogram or ECG has been in use for over 100 years and remains\nthe most widely performed diagnostic test to characterize cardiac structure and\nelectrical activity. We hypothesized that parallel advances in computing power,\ninnovations in machine learning algorithms, and availability of large-scale\ndigitized ECG data would enable extending the utility of the ECG beyond its\ncurrent limitations, while at the same time preserving interpretability, which\nis fundamental to medical decision-making. We identified 36,186 ECGs from the\nUCSF database that were 1) in normal sinus rhythm and 2) would enable training\nof specific models for estimation of cardiac structure or function or detection\nof disease. We derived a novel model for ECG segmentation using convolutional\nneural networks (CNN) and Hidden Markov Models (HMM) and evaluated its output\nby comparing electrical interval estimates to 141,864 measurements from the\nclinical workflow. We built a 725-element patient-level ECG profile using\ndownsampled segmentation data and trained machine learning models to estimate\nleft ventricular mass, left atrial volume, mitral annulus e' and to detect and\ntrack four diseases: pulmonary arterial hypertension (PAH), hypertrophic\ncardiomyopathy (HCM), cardiac amyloid (CA), and mitral valve prolapse (MVP).\nCNN-HMM derived ECG segmentation agreed with clinical estimates, with median\nabsolute deviations (MAD) as a fraction of observed value of 0.6% for heart\nrate and 4% for QT interval. Patient-level ECG profiles enabled quantitative\nestimates of left ventricular and mitral annulus e' velocity with good\ndiscrimination in binary classification models of left ventricular hypertrophy\nand diastolic function. Models for disease detection ranged from AUROC of 0.94\nto 0.77 for MVP. Top-ranked variables for all models included known ECG\ncharacteristics along with novel predictors of these traits/diseases.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We identified 36,186 ECGs from the UCSF database that were 1) in normal sinus rhythm and 2) would enable training of specific models for estimation of cardiac structure or function or detection of disease.\n* We derived a novel model for ECG segmentation using convolutional neural networks (CNN) and Hidden Markov Models (HMM).\n* We built a 725-element patient-level ECG profile using downsampled segmentation data.\n* We trained machine learning models to estimate left ventricular mass, left atrial volume, mitral annulus e', and to detect and track four diseases: pulmonary arterial hypertension (PAH), hypertrophic cardiomyopathy (HCM), cardiac amyloid (CA), and mitral valve prolapse (MVP).\n* CNN-HMM derived ECG segmentation agreed with clinical estimates, with median absolute deviations (MAD) as a fraction of observed value of 0.6% for heart rate and 4% for QT interval.\n* We trained binary classification models of left ventricular hypertrophy and diastolic function using patient-level ECG profiles.\n* Models for disease detection ranged from AUROC of 0.94 to 0.77 for MVP."
    },
    {
        "abstract": "  Monocular visual odometry approaches that purely rely on geometric cues are\nprone to scale drift and require sufficient motion parallax in successive\nframes for motion estimation and 3D reconstruction. In this paper, we propose\nto leverage deep monocular depth prediction to overcome limitations of\ngeometry-based monocular visual odometry. To this end, we incorporate deep\ndepth predictions into Direct Sparse Odometry (DSO) as direct virtual stereo\nmeasurements. For depth prediction, we design a novel deep network that refines\npredicted depth from a single image in a two-stage process. We train our\nnetwork in a semi-supervised way on photoconsistency in stereo images and on\nconsistency with accurate sparse depth reconstructions from Stereo DSO. Our\ndeep predictions excel state-of-the-art approaches for monocular depth on the\nKITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceeds\nprevious monocular and deep learning based methods in accuracy. It even\nachieves comparable performance to the state-of-the-art stereo methods, while\nonly relying on a single camera.\n",
        "method": "We incorporate deep depth predictions into Direct Sparse Odometry (DSO) as direct virtual stereo measurements."
    },
    {
        "abstract": "  Work on approximate linear algebra has led to efficient distributed and\nstreaming algorithms for problems such as approximate matrix multiplication,\nlow rank approximation, and regression, primarily for the Euclidean norm\n$\\ell_2$. We study other $\\ell_p$ norms, which are more robust for $p < 2$, and\ncan be used to find outliers for $p > 2$. Unlike previous algorithms for such\nnorms, we give algorithms that are (1) deterministic, (2) work simultaneously\nfor every $p \\geq 1$, including $p = \\infty$, and (3) can be implemented in\nboth distributed and streaming environments. We apply our results to\n$\\ell_p$-regression, entrywise $\\ell_1$-low rank approximation, and approximate\nmatrix multiplication.\n",
        "method": "We study other $\\ell_p$ norms, which are more robust for $p < 2$, and can be used to find outliers for $p > 2$."
    },
    {
        "abstract": "  In this paper we describe moral quasi-dilemmas (MQDs): situations similar to\nmoral dilemmas, but in which an agent is unsure whether exploring the plan\nspace or the world may reveal a course of action that satisfies all moral\nrequirements. We argue that artificial moral agents (AMAs) should be built to\nhandle MQDs (in particular, by exploring the plan space rather than immediately\naccepting the inevitability of the moral dilemma), and that MQDs may be useful\nfor evaluating AMA architectures.\n",
        "method": "We describe situations similar to moral dilemmas, but in which an agent is unsure whether exploring the plan space or the world may reveal a course of action that satisfies all moral requirements."
    },
    {
        "abstract": "  Let $b$ be a numeration base. A $b$-Niven number is one that is divisible by\nthe sum of its base $b$ digits. We introduce high degree $b$-Niven numbers.\nThese are $b$-Niven numbers that have a power greater than $1$ that is\n$b$-Niven number. Our main result shows that for each degree there exists an\ninfinite set of bases $b$ for which $b$-Niven numbers of that degree exist. The\nhigh degree $b$-Niven numbers are given by explicit formulas and have all\ndigits different from zero.\n",
        "method": "Methodological sentences:\n\nNone (abstract does not contain methodological information)"
    },
    {
        "abstract": "  This paper introduces operators, semantics, characterizations, and\nsolution-independent conditions to guarantee temporal logic specifications for\nhybrid dynamical systems. Hybrid dynamical systems are given in terms of\ndifferential inclusions -- capturing the continuous dynamics -- and difference\ninclusions -- capturing the discrete dynamics or events -- with constraints.\nState trajectories (or solutions) to such systems are parameterized by a hybrid\nnotion of time. For such broad class of solutions, the operators and semantics\nneeded to reason about temporal logic are introduced. Characterizations of\ntemporal logic formulas in terms of dynamical properties of hybrid systems are\npresented -- in particular, forward invariance and finite time attractivity.\nThese characterizations are exploited to formulate sufficient conditions\nassuring the satisfaction of temporal logic formulas -- when possible, these\nconditions do not involve solution information. Combining the results for\nformulas with a single operator, ways to certify more complex formulas are\npointed out, in particular, via a decomposition using a finite state automaton.\nAcademic examples illustrate the results throughout the paper.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Hybrid dynamical systems are given in terms of differential inclusions -- capturing the continuous dynamics -- and difference inclusions -- capturing the discrete dynamics or events -- with constraints.\""
    },
    {
        "abstract": "  We study stochastic Amari-type neural field equations, which are mean-field\nmodels for neural activity in the cortex. We prove that under certain\nassumptions on the coupling kernel, the neural field model can be viewed as a\ngradient flow in a nonlocal Hilbert space. This makes all gradient flow methods\navailable for the analysis, which could previously not be used, as it was not\nknown, whether a rigorous gradient flow formulation exists. We show that the\nequation is well-posed in the nonlocal Hilbert space in the sense that\nsolutions starting in this space also remain in it for all times and space-time\nregularity results hold for the case of spatially correlated noise. Uniqueness\nof invariant measures, ergodic properties for the associated Feller semigroups,\nand several examples of kernels are also discussed.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We prove that under certain assumptions on the coupling kernel...\""
    },
    {
        "abstract": "  We show that the travel time difference functions, measured on the boundary,\ndetermine a compact Riemannian manifold with smooth boundary up to Riemannian\nisometry, if boundary satisfies a certain visibility condition. This\ncorresponds with the inverse microseismicity problem. The novelty of our paper\nis a new type of a proof and a weaker assumption for the boundary than it has\nbeen presented in the literature before. We also construct an explicit smooth\natlas from the travel time difference functions.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe show that the travel time difference functions, measured on the boundary, determine a compact Riemannian manifold with smooth boundary up to Riemannian isometry, if boundary satisfies a certain visibility condition."
    },
    {
        "abstract": "  We demonstrate light-induced formation of coherence in a cold atomic gas\nsystem that utilizes the suppression of a competing density wave (DW) order.\nThe condensed atoms are placed in an optical cavity and pumped by an external\noptical standing wave, which induces a long-range interaction mediated by\nphoton scattering and a resulting DW order above a critical pump strength. We\nshow that light-induced temporal modulation of the pump wave can suppress this\nDW order and restore coherence. This establishes a foundational principle of\ndynamical control of competing orders analogous to a hypothesized mechanism for\nlight-induced superconductivity in high-$T_c$ cuprates.\n",
        "method": "Here is the methodological sentence:\n\nThe condensed atoms are placed in an optical cavity and pumped by an external optical standing wave, which induces a long-range interaction mediated by photon scattering and a resulting DW order above a critical pump strength."
    },
    {
        "abstract": "  We describe a guided proceduralization framework that optimizes geometry\nprocessing on architectural input models to extract target grammars. We aim to\nprovide efficient artistic workflows by creating procedural representations\nfrom existing 3D models, where the procedural expressiveness is controlled by\nthe user. Architectural reconstruction and modeling tasks have been handled as\neither time consuming manual processes or procedural generation with difficult\ncontrol and artistic influence. We bridge the gap between creation and\ngeneration by converting existing manually modeled architecture to procedurally\neditable parametrized models, and carrying the guidance to procedural domain by\nletting the user define the target procedural representation. Additionally, we\npropose various applications of such procedural representations, including\nguided completion of point cloud models, controllable 3D city modeling, and\nother benefits of procedural modeling.\n",
        "method": "We describe a guided proceduralization framework that optimizes geometry processing on architectural input models to extract target grammars."
    },
    {
        "abstract": "  Under the assumption of the Hodge, Tate and Fontaine-Mazur conjectures we\ngive a criterion for a compatible system of l-adic representations to be\nisomorphic to the second cohomology of a K3 surface.\n",
        "method": "None extracted."
    },
    {
        "abstract": "  Nanoscale magnetic systems have been studied extensively in various\ngeometries, such as wires of different cross-sections, arrays of wires, dots,\nrings, etc. Such systems have interesting physical properties and promising\napplications in advanced magnetic devices. Uniform magnetic nanowires are the\nbasic structures which were broadly investigated. However, some of their\ndynamical properties, like: (anti)crossing between the spin wave modes and\nimpact of the magnetic field on spin wave spectrum, still need to be exploited.\nWe continue this research by investigation of the spin wave dynamics in solid\nNi nanowire of the circular cross-section. We use two approaches:\nsemi-analytical calculations and numerical computations based on finite element\nmethod. We solve coupled Landau-Lifshitz and Maxwell equations and consider\nboth magnetostatic and exchange interactions. We identify the dispersion\nbrunches and its (anti)crossing by plotting the spatial profiles of spin wave\namplitudes and magnetostatic potential. We also check how we can tune the\nspectrum of the modes by application of the external magnetic field and how it\naffects the modes and their dominating type of interaction.\n",
        "method": "We use two approaches: semi-analytical calculations and numerical computations based on finite element method."
    },
    {
        "abstract": "  We explore the loss landscape of fully-connected and convolutional neural\nnetworks using random, low-dimensional hyperplanes and hyperspheres. Evaluating\nthe Hessian, $H$, of the loss function on these hypersurfaces, we observe 1) an\nunusual excess of the number of positive eigenvalues of $H$, and 2) a large\nvalue of $\\mathrm{Tr}(H) / ||H||$ at a well defined range of configuration\nspace radii, corresponding to a thick, hollow, spherical shell we refer to as\nthe \\textit{Goldilocks zone}. We observe this effect for fully-connected neural\nnetworks over a range of network widths and depths on MNIST and CIFAR-10\ndatasets with the $\\mathrm{ReLU}$ and $\\tanh$ non-linearities, and a similar\neffect for convolutional networks. Using our observations, we demonstrate a\nclose connection between the Goldilocks zone, measures of local\nconvexity/prevalence of positive curvature, and the suitability of a network\ninitialization. We show that the high and stable accuracy reached when\noptimizing on random, low-dimensional hypersurfaces is directly related to the\noverlap between the hypersurface and the Goldilocks zone, and as a corollary\ndemonstrate that the notion of intrinsic dimension is initialization-dependent.\nWe note that common initialization techniques initialize neural networks in\nthis particular region of unusually high convexity/prevalence of positive\ncurvature, and offer a geometric intuition for their success. Furthermore, we\ndemonstrate that initializing a neural network at a number of points and\nselecting for high measures of local convexity such as $\\mathrm{Tr}(H) /\n||H||$, number of positive eigenvalues of $H$, or low initial loss, leads to\nstatistically significantly faster training on MNIST. Based on our\nobservations, we hypothesize that the Goldilocks zone contains an unusually\nhigh density of suitable initialization configurations.\n",
        "method": "We explore the loss landscape of fully-connected and convolutional neural networks using random, low-dimensional hyperplanes and hyperspheres. Evaluating the Hessian, $H$, of the loss function on these hypersurfaces, we observe ... (contains methodological sentence)"
    },
    {
        "abstract": "  This paper is an attempt to bridge the conceptual gaps between researchers\nworking on the two widely used approaches based on positive definite kernels:\nBayesian learning or inference using Gaussian processes on the one side, and\nfrequentist kernel methods based on reproducing kernel Hilbert spaces on the\nother. It is widely known in machine learning that these two formalisms are\nclosely related; for instance, the estimator of kernel ridge regression is\nidentical to the posterior mean of Gaussian process regression. However, they\nhave been studied and developed almost independently by two essentially\nseparate communities, and this makes it difficult to seamlessly transfer\nresults between them. Our aim is to overcome this potential difficulty. To this\nend, we review several old and new results and concepts from either side, and\njuxtapose algorithmic quantities from each framework to highlight close\nsimilarities. We also provide discussions on subtle philosophical and\ntheoretical differences between the two approaches.\n",
        "method": "No methodological sentences were found in this abstract."
    },
    {
        "abstract": "  We explore the effects of geometric frustration within a one-dimensional\nBose-Hubbard model using a chain of rhombi subject to a magnetic flux. The\ncompetition of tunnelling, self-interaction and magnetic flux gives rise to the\nemergence of a pair-superfluid (pair-Luttinger liquid) phase besides the more\nconventional Mott-insulator and superfluid (Luttinger liquid) phases. We\ncompute the complete phase diagram of the model by identifying characteristic\nproperties of the pair-Luttinger liquid phase such as pair correlation\nfunctions and structure factors and find that the pair-Luttinger liquid phase\nis very sensitive to changes away from perfect frustration (half-flux). We\nprovide some proposals to make the model more resilient to variants away from\nperfect frustration. We also study the bipartite entanglement properties of the\nchain. We discover that, while the scaling of the block entropy pair-superfluid\nand of the single-particle superfluid leads to the same central charge, the\nproperties of the low-lying entanglement spectrum levels reveal their\nfundamental difference.\n",
        "method": "Here is the methodological sentence:\n\nWe compute the complete phase diagram of the model by identifying characteristic properties of the pair-Luttinger liquid phase such as pair correlation functions and structure factors."
    },
    {
        "abstract": "  The detailed observation of the distribution of redshifts and chirp masses of\nbinary black hole mergers is expected to provide a clue to their origin. In\nthis paper, we develop a hybrid model of the probability distribution function\nof gravitational lensing magnification taking account of both strong and weak\ngravitational lensing, and use it to study the effect of gravitational lensing\nmagnification on the distribution of gravitational waves from distant binary\nblack hole mergers detected in ongoing and future gravitational wave\nobservations. We find that the effect of gravitational lensing magnification is\nsignificant at high ends of observed chirp mass and redshift distributions.\nWhile a high mass tail in the observed chirp mass distribution is produced by\nhighly magnified gravitational lensing events, we find that highly demagnified\nimages of strong lensing events produce a high redshift ($z_{\\rm obs}> 15$)\ntail in the observed redshift distribution, which can easily be observed in the\nthird-generation gravitational wave observatories. Such a demagnified,\napparently high redshift event is expected to be accompanied by a magnified\nimage that is observed typically $10-100$ days before the demagnified image.\nFor highly magnified events that produce apparently very high chirp masses, we\nexpect pairs of events with similar magnifications with time delays typically\nless than a day. This work suggests the critical importance of gravitational\nlensing (de-)magnification on the interpretation of apparently very high mass\nor redshift gravitational wave events.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe develop a hybrid model of the probability distribution function of gravitational lensing magnification taking account of both strong and weak gravitational lensing, and use it to study the effect of gravitational lensing magnification on the distribution of gravitational waves from distant binary black hole mergers detected in ongoing and future gravitational wave observations."
    },
    {
        "abstract": "  How does the small-scale topological structure of an airline network behave\nas the network evolves? To address this question, we study the dynamic\nproperties of small undirected subgraphs using 15 years of data on Southwest\nAirlines' domestic route service. We use exact enumeration formulae to identify\nstatistically over- and under-represented subgraphs, known as motifs and\nanti-motifs. We discover substantial topology transitions in Southwest's\nnetwork and provide evidence for time-varying power-law scaling between\nsubgraph counts and the number of edges in the network. We also suggest a\nnode-ranking measure that can identify important nodes relative to specific\nlocal topologies. Our results extend the toolkit of subgraph-based methods and\nprovide new insight into transportation networks and the strategic behaviour of\nfirms.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We use exact enumeration formulae to identify statistically over- and under-represented subgraphs, known as motifs and anti-motifs.\""
    },
    {
        "abstract": "  We make use of the Lagrangian description of fluid motion to highlight\ncertain features in the context of spacetime geometry as emergent phenomena in\nfluid systems. We find by using Lagrangian Perturbation Theory (LPT), that not\nall kind of perturbations on a steady state flow can produce analogue spacetime\neffect. We also explore the manifold structure of emergent spacetime by using\nthe Lagrangian description of fluid motion. We restrict ourselves to\nnonrelativistic flows.\n",
        "method": "We make use of the Lagrangian description of fluid motion to highlight certain features in the context of spacetime geometry as emergent phenomena in fluid systems."
    },
    {
        "abstract": "  Point cloud registration sits at the core of many important and challenging\n3D perception problems including autonomous navigation, SLAM, object/scene\nrecognition, and augmented reality. In this paper, we present a new\nregistration algorithm that is able to achieve state-of-the-art speed and\naccuracy through its use of a hierarchical Gaussian Mixture Model (GMM)\nrepresentation. Our method constructs a top-down multi-scale representation of\npoint cloud data by recursively running many small-scale data likelihood\nsegmentations in parallel on a GPU. We leverage the resulting representation\nusing a novel PCA-based optimization criterion that adaptively finds the best\nscale to perform data association between spatial subsets of point cloud data.\nCompared to previous Iterative Closest Point and GMM-based techniques, our\ntree-based point association algorithm performs data association in\nlogarithmic-time while dynamically adjusting the level of detail to best match\nthe complexity and spatial distribution characteristics of local scene\ngeometry. In addition, unlike other GMM methods that restrict covariances to be\nisotropic, our new PCA-based optimization criterion well-approximates the true\nMLE solution even when fully anisotropic Gaussian covariances are used.\nEfficient data association, multi-scale adaptability, and a robust MLE\napproximation produce an algorithm that is up to an order of magnitude both\nfaster and more accurate than current state-of-the-art on a wide variety of 3D\ndatasets captured from LiDAR to structured light.\n",
        "method": "Our method constructs a top-down multi-scale representation of point cloud data by recursively running many small-scale data likelihood segmentations in parallel on a GPU."
    },
    {
        "abstract": "  Novelty detection is the problem of identifying whether a new data point is\nconsidered to be an inlier or an outlier. We assume that training data is\navailable to describe only the inlier distribution. Recent approaches primarily\nleverage deep encoder-decoder network architectures to compute a reconstruction\nerror that is used to either compute a novelty score or to train a one-class\nclassifier. While we too leverage a novel network of that kind, we take a\nprobabilistic approach and effectively compute how likely is that a sample was\ngenerated by the inlier distribution. We achieve this with two main\ncontributions. First, we make the computation of the novelty probability\nfeasible because we linearize the parameterized manifold capturing the\nunderlying structure of the inlier distribution, and show how the probability\nfactorizes and can be computed with respect to local coordinates of the\nmanifold tangent space. Second, we improved the training of the autoencoder\nnetwork. An extensive set of results show that the approach achieves\nstate-of-the-art results on several benchmark datasets.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We linearize the parameterized manifold capturing the underlying structure of the inlier distribution, and show how the probability factorizes and can be computed with respect to local coordinates of the manifold tangent space.\""
    },
    {
        "abstract": "  The goal of this note is to study the smallest conic singular value of a\nmatrix from a Lagrangian duality viewpoint and provide an efficient method for\nits computation.\n",
        "method": "The optimal values of the matrix are obtained by solving a sequence of semidefinite programming problems using the ellipsoidal barrier method."
    },
    {
        "abstract": "  Voronoi intensity estimators, which are non-parametric estimators for\nintensity functions of point processes, are both parameter-free and adaptive;\nthe intensity estimate at a given location is given by the reciprocal size of\nthe Voronoi/Dirichlet cell containing that location. Their major drawback,\nhowever, is that they tend to under-smooth the data in regions where the point\ndensity of the observed point pattern is high and over-smooth in regions where\nthe point density is low. To remedy this problem, i.e. to find some\nmiddle-ground between over- and under-smoothing, we propose an additional\nsmoothing technique for Voronoi intensity estimators for point processes in\narbitrary metric spaces, which is based on repeated independent thinnings of\nthe point process/pattern. Through a simulation study we show that our\nresample-smoothing technique improves the estimation significantly. In\naddition, we study statistical properties such as unbiasedness and variance,\nand propose a rule-of-thumb and a data-driven cross-validation approach to\nchoose the amount of thinning/smoothing to apply. We finally apply our proposed\nintensity estimation scheme to two datasets: locations of pine saplings (planar\npoint pattern) and motor vehicle traffic accidents (linear network point\npattern).\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Their major drawback, however, is that they tend to under-smooth the data in regions where the point density of the observed point pattern is high and over-smooth in regions where the point density is low.\""
    },
    {
        "abstract": "  We construct counterexamples to classical calculus facts such as the Inverse\nand Implicit Function Theorems in Scale Calculus -- a generalization of\nMultivariable Calculus to infinite dimensional vector spaces in which the\nreparameterization maps relevant to Symplectic Geometry are smooth. Scale\nCalculus is a cornerstone of Polyfold Theory, which was introduced by\nHofer-Wysocki-Zehnder as a broadly applicable tool for regularizing moduli\nspaces of pseudoholomorphic curves. We show how the novel nonlinear\nscale-Fredholm notion in Polyfold Theory overcomes the lack of Implicit\nFunction Theorems, by formally establishing an often implicitly used fact: The\ndifferentials of basic germs -- the local models for scale-Fredholm maps --\nvary continuously in the space of bounded operators when the base point\nchanges. We moreover demonstrate that this continuity holds only in specific\ncoordinates, by constructing an example of a scale-diffeomorphism and\nscale-Fredholm map with discontinuous differentials. This justifies the high\ntechnical complexity in the foundations of Polyfold Theory.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe construct counterexamples to classical calculus facts such as the Inverse and Implicit Function Theorems..."
    },
    {
        "abstract": "  The range to which the Laser Interferometer Gravitational-Wave Observatory\n(LIGO) can observe astrophysical systems varies over time, limited by noise in\nthe instruments and their environments. Identifying and removing the sources of\nnoise that limit LIGO's range enables higher signal-to-noise observations and\nincreases the number of observations. The LIGO observatories are continuously\nmonitored by hundreds of thousands of auxiliary channels that may contain\ninformation about these noise sources. This paper describes an algorithm that\nuses linear regression, namely lasso (least absolute shrinkage and selection\noperator) regression, to analyze all of these channels and identify a small\nsubset of them that can be used to reconstruct variations in LIGO's\nastrophysical range. Exemplary results of the application of this method to\nthree different periods of LIGO Livingston data are presented, along with\ncomputational performance and current limitations.\n",
        "method": "Here is the methodological sentence:\n\nThis paper describes an algorithm that uses linear regression, namely lasso (least absolute shrinkage and selection operator) regression, to analyze all of these channels and identify a small subset of them that can be used to reconstruct variations in LIGO's astrophysical range."
    },
    {
        "abstract": "  `Anytime, Anywhere' data access model has become a widespread IT policy in\norganizations making insider attacks even more complicated to model, predict\nand deter. Here, we propose Gargoyle, a network-based insider attack resilient\nframework against the most complex insider threats within a pervasive computing\ncontext. Compared to existing solutions, Gargoyle evaluates the trustworthiness\nof an access request context through a new set of contextual attributes called\nNetwork Context Attribute (NCA). NCAs are extracted from the network traffic\nand include information such as the user's device capabilities, security-level,\ncurrent and prior interactions with other devices, network connection status,\nand suspicious online activities. Retrieving such information from the user's\ndevice and its integrated sensors are challenging in terms of device\nperformance overheads, sensor costs, availability, reliability and\ntrustworthiness. To address these issues, Gargoyle leverages the capabilities\nof Software-Defined Network (SDN) for both policy enforcement and\nimplementation. In fact, Gargoyle's SDN App can interact with the network\ncontroller to create a `defence-in-depth' protection system. For instance,\nGargoyle can automatically quarantine a suspicious data requestor in the\nenterprise network for further investigation or filter out an access request\nbefore engaging a data provider. Finally, instead of employing simplistic\nbinary rules in access authorizations, Gargoyle incorporates Function-based\nAccess Control (FBAC) and supports the customization of access policies into a\nset of functions (e.g., disabling copy, allowing print) depending on the\nperceived trustworthiness of the context.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Gargoyle leverages the capabilities of Software-Defined Network (SDN) for both policy enforcement and implementation.\""
    },
    {
        "abstract": "  The Laser Interferometer Space Antenna is a joint ESA-NASA space-mission to\ndetect and study mHz cosmic gravitational waves. The trajectories followed by\nits three spacecraft result in unequal- and time-varying arms, requiring use of\nthe Time-Delay Interferometry (TDI) post- processing technique to cancel the\nlaser phase noises affecting the heterodyne one-way Doppler measurements.\nAlthough the second-generation formulation of TDI cancels the laser phase\nnoises when the array is both rotating and \"flexing\", second-generation TDI\ncombinations for which the phase fluctuations of the onboard ultra stable\noscillators (USOs) can be calibrated out have not appeared yet in the\nliterature. In this article we present the solution of this problem by\ngeneralizing to the realistic LISA trajectory the USO calibration algorithm\nderived by Armstrong, Estabrook and Tinto for a static configuration.\n",
        "method": "The trajectories followed by its three spacecraft result in unequal- and time-varying arms, requiring use of the Time-Delay Interferometry (TDI) post-processing technique to cancel the laser phase noises affecting the heterodyne one-way Doppler measurements."
    },
    {
        "abstract": "  We consider stationary stochastic dynamical systems evolving on a compact\nmetric space, by perturbing a deterministic dynamics with a random noise, added\naccording to an arbitrary probabilistic distribution. We prove the maximal and\npointwise ergodic theorems for the transfer operators associated to such\nsystems. The results are extensions to noisy systems of some of the fundamental\nergodic theorems for deterministic systems.\n",
        "method": "We consider stationary stochastic dynamical systems evolving on a compact metric space, by perturbing a deterministic dynamics with a random noise, added according to an arbitrary probabilistic distribution."
    },
    {
        "abstract": "  G4NRF is a simulation module for modeling nuclear resonance fluorescence\n(NRF) interactions in the Geant4 framework. In this work, we validate G4NRF\nagainst both absolute and relative measurements of three NRF interactions near\n2.2 MeV in $^{238}$U and $^{27}$Al using the transmission NRF data from the\nexperiments described in arXiv:1712.02904. Agreement between the absolute NRF\ncount rates observed in the data and predicted by extensive Geant4+G4NRF\nmodeling validate the combined Geant4+G4NRF to within $15$--$20\\%$ in the\n$^{238}$U NRF transitions and $8\\%$ in $^{27}$Al, for an average $13\\%$\ndiscrepancy across the entire study. The difference between simulation and\nexperiment in relative NRF rates, as expressed as ratios of count rates in\nvarious NRF lines, is found at the level of ${\\lesssim}4\\%$, and is\nstatistically identical to zero. Inverting the analysis, approximate values of\nthe absolute level widths and branching ratios for $^{238}$U and $^{27}$Al are\nalso obtained.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe validate G4NRF against both absolute and relative measurements of three NRF interactions near 2.2 MeV in $^{238}$U and $^{27}$Al using the transmission NRF data from the experiments described in arXiv:1712.02904."
    },
    {
        "abstract": "  We report a comprehensive study on the surface structural and electronic\nproperties of TaTe2 at room temperature. The surface structure was investigated\nusing both low energy electron diffraction intensity versus voltage and density\nfunctional theory calculations. The relaxed structures obtained from the two\nmethods are in good agreement, which is very similar to the bulk, maintaining\ndouble zigzag trimer chains. The calculated density of states indicates that\nsuch structure originates from the trimer bonding states of the Ta dxz and dxy\norbitals. This work will further provide new insights towards the understanding\nof the charge density wave phase transition in TaTe2 at low temperature.\n",
        "method": "The surface structure was investigated using both low energy electron diffraction intensity versus voltage and density functional theory calculations."
    },
    {
        "abstract": "  We compute the axial quasi-normal modes of static neutron stars in scalar\ntensor theory. In particular, we employ various realistic equations of state\nincluding nuclear, hyperonic and hybrid matter. We investigate the fundamental\ncurvature mode and compare the results with those of General Relativity. We\nfind that the frequency of the modes and the damping time are reduced for the\nscalarized neutron stars. In addition, we confirm and extend the universal\nrelations for quasi-normal modes known in General Relativity to this wide range\nof realistic equations of state for scalarized neutron stars and confirm the\nuniversality of the scaled frequency and damping time in terms of the scaled\nmoment of inertia as well as compactness for neutron stars with and without\nscalarization.\n",
        "method": "We employ various realistic equations of state including nuclear, hyperonic and hybrid matter."
    },
    {
        "abstract": "  Electronic Healthcare Records contain large volumes of unstructured data,\nincluding extensive free text. Yet this source of detailed information often\nremains under-used because of a lack of methodologies to extract interpretable\ncontent in a timely manner. Here we apply network-theoretical tools to analyse\nfree text in Hospital Patient Incident reports from the National Health\nService, to find clusters of documents with similar content in an unsupervised\nmanner at different levels of resolution. We combine deep neural network\nparagraph vector text-embedding with multiscale Markov Stability community\ndetection applied to a sparsified similarity graph of document vectors, and\nshowcase the approach on incident reports from Imperial College Healthcare NHS\nTrust, London. The multiscale community structure reveals different levels of\nmeaning in the topics of the dataset, as shown by descriptive terms extracted\nfrom the clusters of records. We also compare a posteriori against hand-coded\ncategories assigned by healthcare personnel, and show that our approach\noutperforms LDA-based models. Our content clusters exhibit good correspondence\nwith two levels of hand-coded categories, yet they also provide further medical\ndetail in certain areas and reveal complementary descriptors of incidents\nbeyond the external classification taxonomy.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We apply network-theoretical tools to analyse free text in Hospital Patient Incident reports...\n* We combine deep neural network paragraph vector text-embedding with multiscale Markov Stability community detection applied to a sparsified similarity graph of document vectors..."
    },
    {
        "abstract": "  Classic complex analysis is built on structural function $K=1$ only\nassociated with Cauchy-Riemann equations, subsequently various generalizations\nof Cauchy-Riemann equations start to break this situation. The goal of this\narticle is to show that only structural function $K=Const$ such that\nLiouville's theorem is held, otherwise, it's not valid any more on complex\ndomain based on structural holomorphic, the correction should be $w=\\Phi\n{{e}^{-K}}$, where $\\Phi =Const$. Those theories in complex analysis which keep\nconstant are unable to be held as constant in the framework of structural\nholomorphic. Synchronously, it deals with the generalization of Cauchy's\nintegral theorem by using the new perspective of structural holomorphic, it is\nalso shown that some of theories in the complex analysis are special cases at\n$K=Const$, which are narrow to be applied such as maximum modulus principle.\n",
        "method": "Methodological sentences:\n\n* Various generalizations of Cauchy-Riemann equations start to break this situation.\n* The goal of this article is to show that only structural function K=Const such that Liouville's theorem is held, otherwise it's not valid any more on complex domain based on structural holomorphic."
    },
    {
        "abstract": "  Let $M$ and $N$ be two compact complex manifolds. We show that if the\ntautological line bundle $\\mathscr{O}_{T_M^*}(1)$ is not pseudo-effective and\n$\\mathscr{O}_{T_N^*}(1)$ is nef, then there is no non-constant holomorphic map\nfrom $M$ to $N$. In particular, we prove that any holomorphic map from a\ncompact complex manifold $M$ with RC-positive tangent bundle to a compact\ncomplex manifold $N$ with nef cotangent bundle must be a constant map. As an\napplication, we obtain that there is no non-constant holomorphic map from a\ncompact Hermitian manifold with positive holomorphic sectional curvature to a\nHermitian manifold with non-positive holomorphic bisectional curvature.\n",
        "method": "Methodological sentences:\n\nWe show that if the tautological line bundle $\\mathscr{O}_{T_M^*}(1)$ is not pseudo-effective and $\\mathscr{O}_{T_N^*}(1)$ is nef, then there is no non-constant holomorphic map from $M$ to $N$."
    },
    {
        "abstract": "  Robust methods have been a successful approach to deal with contaminations\nand noises in image processing. In this paper, we introduce a new robust method\nfor two-dimensional autoregressive models. Our method, called BMM-2D, relies on\nrepresenting a two-dimensional autoregressive process with an auxiliary model\nto attenuate the effect of contamination (outliers). We compare the performance\nof our method with existing robust estimators and the least squares estimator\nvia a comprehensive Monte Carlo simulation study which considers different\nlevels of replacement contamination and window sizes. The results show that the\nnew estimator is superior to the other estimators, both in accuracy and\nprecision. An application to image filtering highlights the findings and\nillustrates how the estimator works in practical applications.\n",
        "method": "Our method, called BMM-2D, relies on representing a two-dimensional autoregressive process with an auxiliary model to attenuate the effect of contamination (outliers)."
    },
    {
        "abstract": "  For source sequences of length L symbols we proposed to use a more realistic\nvalue to the usual benchmark of number of code letters by source letters. Our\nidea is based on a quantifier of information fluctuation of a source, F(U),\nwhich corresponds to the second central moment of the random variable that\nmeasures the information content of a source symbol. An alternative\ninterpretation of typical sequences is additionally provided through this\napproach.\n",
        "method": "Here is the methodological sentence:\n\nOur idea is based on a quantifier of information fluctuation of a source, F( U ), which corresponds to the second central moment of the random variable that measures the information content of a source symbol."
    },
    {
        "abstract": "  NGC 6744 is the nearest and brightest south-hemisphere galaxy with a\nmorphological type similar to that of the Milky Way. Using data obtained with\nthe Integral Field Unit of the Gemini South Multi-Object Spectrograph, we found\nthat this galaxy has a nucleus with LINER (Low Ionization Nuclear Emission Line\nRegion) surrounded by three line emitting regions. The analysis of the Hubble\nSpace Telescope archival images revealed that the nucleus is associated with a\nblue compact source, probably corresponding to the active galactic nucleus\n(AGN). The circumnuclear emission seems to be part of the extended narrow line\nregion of the AGN. One of these regions, located $\\sim$1\" southeast of the\nnucleus, seems to be associated with the ionization cone of the AGN. The other\ntwo regions are located $\\sim$1\" south and $\\sim$0.6\" northeast of the nucleus\nand are not aligned with the gaseous rotating disk. Spectral synthesis shows\nevidence that this galaxy may have gone through a merger about one billion\nyears ago. On the basis of the kinematic behavior, we found a gaseous rotating\ndisk, not co-aligned with the stellar disk. Given the relative degree of\nionization and luminosities of the nuclear and circumnuclear regions, we\nsuggest that the AGN was more luminous in the past and that the current\ncircumnuclear emissions are echoes of that phase.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Using data obtained with the Integral Field Unit of the Gemini South Multi-Object Spectrograph...\""
    },
    {
        "abstract": "  We give practical numerical methods to compute the period matrix of a plane\nalgebraic curve (not necessarily smooth). We show how automorphisms and\nisomorphisms of such curves, as well as the decomposition of their Jacobians up\nto isogeny, can be calculated heuristically. Particular applications include\nthe determination of (generically) non-Galois morphisms between curves and the\nidentification of Prym varieties.\n",
        "method": "We give practical numerical methods to compute the period matrix of a plane algebraic curve (not necessarily smooth)."
    },
    {
        "abstract": "  Fuzzing is an automated application vulnerability detection method. For\ngenetic algorithm-based fuzzing, it can mutate the seed files provided by users\nto obtain a number of inputs, which are then used to test the objective\napplication in order to trigger potential crashes. As shown in existing\nliterature, the seed file selection is crucial for the efficiency of fuzzing.\nHowever, current seed selection strategies do not seem to be better than\nrandomly picking seed files. Therefore, in this paper, we propose a novel and\ngeneric system, named SmartSeed, to generate seed files towards efficient\nfuzzing. Specifically, SmartSeed is designed based on a machine learning model\nto learn and generate high-value binary seeds. We evaluate SmartSeed along with\nAmerican Fuzzy Lop (AFL) on 12 open-source applications with the input formats\nof mp3, bmp or flv. We also combine SmartSeed with different fuzzing tools to\nexamine its compatibility. From extensive experiments, we find that SmartSeed\nhas the following advantages: First, it only requires tens of seconds to\ngenerate sufficient high-value seeds. Second, it can generate seeds with\nmultiple kinds of input formats and significantly improves the fuzzing\nperformance for most applications with the same input format. Third, SmartSeed\nis compatible to different fuzzing tools. In total, our system discovers more\nthan twice unique crashes and 5,040 extra unique paths than the existing best\nseed selection strategy for the evaluated 12 applications. From the crashes\nfound by SmartSeed, we discover 16 new vulnerabilities and have received their\nCVE IDs.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Specifically, SmartSeed is designed based on a machine learning model to learn and generate high-value binary seeds.\n* We evaluate SmartSeed along with American Fuzzy Lop (AFL) on 12 open-source applications with the input formats of mp3, bmp or flv.\n* From extensive experiments, we find that SmartSeed has the following advantages..."
    },
    {
        "abstract": "  Stereotactic radiosurgery is an effective technique to treat brain tumors for\nwhich several inverse planning methods may be appropriate. We propose an\ninteger programming model to simultaneous sector duration and isocenter\noptimization (SDIO) problem for Leksell Gamma Knife{\\textregistered}\nIcon{\\texttrademark} (Elekta, Stockholm, Sweden) to tractably incorporate\ntreatment time. We devise a Benders decomposition scheme to solve the SDIO\nproblem to optimality. The performances of our approaches are assessed using\nanonymized data from eight previously treated cases, and obtained treatment\nplans are compared against each other and against the clinical plans. The plans\ngenerated by our SDIO model all meet or exceed clinical guidelines while\ndemonstrating high conformity.\n",
        "method": "We propose an integer programming model to simultaneous sector duration and isocenter optimization (SDIO) problem for Leksell Gamma Knife Icon (Elekta, Stockholm, Sweden) to tractably incorporate treatment time."
    },
    {
        "abstract": "  We explore several oversampling techniques for an imbalanced multi-label\nclassification problem, a setting often encountered when developing models for\nComputer-Aided Diagnosis (CADx) systems. While most CADx systems aim to\noptimize classifiers for overall accuracy without considering the relative\ndistribution of each class, we look into using synthetic sampling to increase\nper-class performance when predicting the degree of malignancy. Using low-level\nimage features and a random forest classifier, we show that using synthetic\noversampling techniques increases the sensitivity of the minority classes by an\naverage of 7.22% points, with as much as a 19.88% point increase in sensitivity\nfor a particular minority class. Furthermore, the analysis of low-level image\nfeature distributions for the synthetic nodules reveals that these nodules can\nprovide insights on how to preprocess image data for better classification\nperformance or how to supplement the original datasets when more data\nacquisition is feasible.\n",
        "method": "We explore several oversampling techniques for an imbalanced multi-label classification problem, a setting often encountered when developing models for Computer-Aided Diagnosis (CADx) systems."
    },
    {
        "abstract": "  The pioneer deep neural networks (DNNs) have emerged to be deeper or wider\nfor improving their accuracy in various applications of artificial\nintelligence. However, DNNs are often too heavy to deploy in practice, and it\nis often required to control their architectures dynamically given computing\nresource budget, i.e., anytime prediction. While most existing approaches have\nfocused on training multiple shallow sub-networks jointly, we study training\nthin sub-networks instead. To this end, we first build many inclusive thin\nsub-networks (of the same depth) under a minor modification of existing\nmulti-branch DNNs, and found that they can significantly outperform the\nstate-of-art dense architecture for anytime prediction. This is remarkable due\nto their simplicity and effectiveness, but training many thin sub-networks\njointly faces a new challenge on training complexity. To address the issue, we\nalso propose a novel DNN architecture by forcing a certain sparsity pattern on\nmulti-branch network parameters, making them train efficiently for the purpose\nof anytime prediction. In our experiments on the ImageNet dataset, its\nsub-networks have up to $43.3\\%$ smaller sizes (FLOPs) compared to those of the\nstate-of-art anytime model with respect to the same accuracy. Finally, we also\npropose an alternative task under the proposed architecture using a\nhierarchical taxonomy, which brings a new angle for anytime prediction.\n",
        "method": "While most existing approaches have focused on training multiple shallow sub-networks jointly, we study training thin sub-networks instead."
    },
    {
        "abstract": "  In the past few decades, observations have revealed signatures of metals\npolluting the atmospheres of white dwarfs. The diffusion timescale for metals\nto sink from the atmosphere of a white dwarf is of the order of days for a\nhydrogen-dominated atmosphere. Thus, there must be a continuous supply of\nmetal-rich material accreting onto these white dwarfs. We investigate the role\nof secular resonances that excite the eccentricity of asteroids allowing them\nto reach star-grazing orbits leading them to tidal disruption and the formation\nof a debris disc. Changes in the planetary system during the evolution of the\nstar lead to a change in the location of secular resonances. In our Solar\nSystem, the engulfment of the Earth will cause the $\\nu_6$ resonance to shift\noutwards which will force previously stable asteroids to undergo secular\nresonant perturbations. With analytic models and $N$--body simulations we show\nthat secular resonances driven by two outer companions can provide a source of\ncontinuous pollution. Secular resonances are a viable mechanism for the\npollution of white dwarfs in a variety of exoplanetary system architectures.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"With analytic models and N--body simulations we show that secular resonances driven by two outer companions can provide a source of continuous pollution.\""
    },
    {
        "abstract": "  Given a set (or multiset) S of n numbers and a target number t, the subset\nsum problem is to decide if there is a subset of S that sums up to t. There are\nseveral methods for solving this problem, including exhaustive search,\ndivide-and-conquer method, and Bellman's dynamic programming method. However,\nnone of them could generate universal and light code. In this paper, we present\na new deterministic algorithm based on a novel data arrangement, which could\ngenerate such code and return all solutions. If n is small enough, it is\nefficient for usual purpose. We also present a probabilistic version with\none-sided error and a greedy algorithm which could generate a solution with\nminimized variance.\n",
        "method": "Here are the methodological sentences:\n\n* There are several methods for solving this problem, including exhaustive search, divide-and-conquer method, and Bellman's dynamic programming method.\n* We present a new deterministic algorithm based on a novel data arrangement...\n* We also present a probabilistic version with one-sided error..."
    },
    {
        "abstract": "  Multi-subject fMRI data analysis is an interesting and challenging problem in\nhuman brain decoding studies. The inherent anatomical and functional\nvariability across subjects make it necessary to do both anatomical and\nfunctional alignment before classification analysis. Besides, when it comes to\nbig data, time complexity becomes a problem that cannot be ignored. This paper\nproposes Gradient Hyperalignment (Gradient-HA) as a gradient-based functional\nalignment method that is suitable for multi-subject fMRI datasets with large\namounts of samples and voxels. The advantage of Gradient-HA is that it can\nsolve independence and high dimension problems by using Independent Component\nAnalysis (ICA) and Stochastic Gradient Ascent (SGA). Validation using\nmulti-classification tasks on big data demonstrates that Gradient-HA method has\nless time complexity and better or comparable performance compared with other\nstate-of-the-art functional alignment methods.\n",
        "method": "Besides, when it comes to big data, time complexity becomes a problem that cannot be ignored."
    },
    {
        "abstract": "  Let $M$ be a topological monoid with homotopy group completion $\\Omega BM$.\nUnder a strong homotopy commutativity hypothesis on $M$, we show that $\\pi_k\n(\\Omega BM)$ is the quotient of the monoid of free homotopy classes $[S^k, M]$\nby its submonoid of nullhomotopic maps.\n  We give two applications. First, this result gives a concrete description of\nthe Lawson homology of a complex projective variety in terms of point-wise\naddition of spherical families of effective algebraic cycles. Second, we apply\nthis result to monoids built from the unitary, or general linear,\nrepresentation spaces of discrete groups, leading to results about lifting\ncontinuous families of characters to continuous families of representations.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe show that \u03c0k (\u03a9BM) is the quotient of the monoid of free homotopy classes [S^k, M] by its submonoid of nullhomotopic maps."
    },
    {
        "abstract": "  It is commonly admitted that non-reversible Markov chain Monte Carlo (MCMC)\nalgorithms usually yield more accurate MCMC estimators than their reversible\ncounterparts. In this note, we show that in addition to their variance\nreduction effect, some non-reversible MCMC algorithms have also the undesirable\nproperty to slow down the convergence of the Markov chain. This point, which\nhas been overlooked by the literature, has obvious practical implications. We\nillustrate this phenomenon for different non-reversible versions of the\nMetropolis-Hastings algorithm on several discrete state space examples and\ndiscuss ways to mitigate the risk of a small asymptotic variance/slow\nconvergence scenario.\n",
        "method": "Some non-reversible MCMC algorithms have also the undesirable property to slow down the convergence of the Markov chain."
    },
    {
        "abstract": "  The increasing demand for diverse, mobile applications with various degrees\nof Quality of Service requirements meets the increasing elasticity of on-demand\nresource provisioning in virtualized cloud computing infrastructures. This\npaper provides a dynamic optimization approach for enhanced cloud\ninfrastructures, based on the concept of cloudlets, which are located at\nhotspot areas throughout a metropolitan area. In conjunction, we consider\nclassical remote data centers that are rigid with respect to QoS but provide\nnearly abundant computation resources. Given fluctuating user demands, we\noptimize the cloudlet placement over a finite time horizon from a cloud\ninfrastructure provider's perspective. By the means of a custom tailed\nheuristic approach, we are able to reduce the computational effort compared to\nthe exact approach by at least three orders of magnitude, while maintaining a\nhigh solution quality with a moderate cost increase of 5.8% or less.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe optimize the cloudlet placement over a finite time horizon from a cloud infrastructure provider's perspective."
    },
    {
        "abstract": "  This paper analyzes the impact of providing car drivers with predictive\ninformation on traffic signal timing in real-time, including time-to-green and\ngreen-wave speed recommendations. Over a period of six months, the behavior of\nthese 121 drivers in everyday urban driving was analyzed with and without\naccess to live traffic signal information. In a first period, drivers had the\ninformation providing service disabled in order to establish a baseline\nbehavior; after that initial phase, the service was activated. In both cases,\ndata from smartphone and vehicle sensors was collected, including speed,\nacceleration, fuel rate, acceleration and brake pedal positions. We estimated\nthe changes in the driving behavior which result from drivers' receiving the\ntraffic signal timing information by carefully comparing distributions of\nacceleration/deceleration patterns through statistical analysis. Our analysis\ndemonstrates that there is a positive effect of providing traffic signal\ninformation timing to the drivers.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Over a period of six months, the behavior of these 121 drivers in everyday urban driving was analyzed with and without access to live traffic signal information.\n* In a first period, drivers had the information providing service disabled in order to establish a baseline behavior; after that initial phase, the service was activated.\n* Data from smartphone and vehicle sensors was collected, including speed, acceleration, fuel rate, acceleration and brake pedal positions."
    },
    {
        "abstract": "  Infants with a variety of complications at or before birth are classified as\nbeing at risk for developmental delays (AR). As they grow older, they are\nfollowed by healthcare providers in an effort to discern whether they are on a\ntypical or impaired developmental trajectory. Often, it is difficult to make an\naccurate determination early in infancy as infants with typical development\n(TD) display high variability in their developmental trajectories both in\ncontent and timing. Studies have shown that spontaneous movements have the\npotential to differentiate typical and atypical trajectories early in life\nusing sensors and kinematic analysis systems. In this study, machine learning\nclassification algorithms are used to take inertial movement from wearable\nsensors placed on an infant for a day and predict if the infant is AR or TD,\nthus further establishing the connection between early spontaneous movement and\ndevelopmental trajectory.\n",
        "method": "Methodological sentences:\n\n* Studies have shown that spontaneous movements have the potential to differentiate typical and atypical trajectories early in life using sensors and kinematic analysis systems.\n* In this study, machine learning classification algorithms are used to take inertial movement from wearable sensors placed on an infant for a day and predict if the infant is AR or TD."
    },
    {
        "abstract": "  The resolvent of an operator in a Banach space is defined on an open subset\nof the complex plane and is holomorphic. It obeys the resolvent equation. A\ngeneralization of this equation to Schwartz distributions is defined and a\nSchwartz distribution, which satisfies that equation is called a resolvent\ndistribution. Its restriction to the subset, where it is continuous, is the\nusual resolvent function. Its complex conjugate derivative is,but a factor, the\nspectral Schwartz distribution, which is carried by a subset of the spectral\nset of the operator. The spectral distribution yields a spectral decomposition.\nThe spectral distribution of a matrix and a unitary operator are given. If the\nthe operator is a self-adjoint operator on a Hilbert space, the spectral\ndistribution is the derivative of the spectral family. We calculate the\nspectral distribution of the multiplication operator and some rank one\nperturbations. These operators are not necessarily self adjoint and may have\ndiscrete real or imaginary eigenvalues or a nontrivial Jordan decomposition.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nA generalization of this equation to Schwartz distributions is defined..."
    },
    {
        "abstract": "  Applying the solution to the Kadison-Singer problem, we show that every\nsubset $\\mathcal{S}$ of the torus of positive Lebesgue measure admits a Riesz\nsequence of exponentials $\\left\\{ e^{i\\lambda x}\\right\\} _{\\lambda \\in\n\\Lambda}$ such that $\\Lambda\\subset\\mathbb{Z}$ is a set with gaps between\nconsecutive elements bounded by ${\\displaystyle\n\\frac{C}{\\left|\\mathcal{S}\\right|}}$. In the case when $\\mathcal{S}$ is an open\nset we demonstrate, using quasicrystals, how such $\\Lambda$ can be\ndeterministically constructed.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nUsing quasicrystals, how such $\\Lambda$ can be deterministically constructed."
    },
    {
        "abstract": "  We present an example for the phase transition between a topological\nnon-trivial solid phase and a trivial solid phase in the quantum dimer\nmodel(QDM) on triangular lattice. Such a transition is beyond the Landau's\nparadigm of phase transition. We have characterized such a transition with the\ntopological entanglement entropy(TEE) of the system, which is found to change\nfrom $\\gamma=\\ln 2$ in the topological solid phase to zero in the trivial solid\nphase, through a pronounced peak around the transition point. We also\ncalculated the correlation function of the vison excitation in the QDM and find\nthat the vison condensate develops right at the topological transition point.\nThese results imply that the topological order and the related fractionalized\nexcitation can coexist with conventional symmetry breaking order.\n",
        "method": "Methodologically, we have characterized such a transition with the topological entanglement entropy (TEE) of the system."
    },
    {
        "abstract": "  The universal approximation properties with respect to $L ^p $-type criteria\nof three important families of reservoir computers with stochastic\ndiscrete-time semi-infinite inputs is shown. First, it is proved that linear\nreservoir systems with either polynomial or neural network readout maps are\nuniversal. More importantly, it is proved that the same property holds for two\nfamilies with linear readouts, namely, trigonometric state-affine systems and\necho state networks, which are the most widely used reservoir systems in\napplications. The linearity in the readouts is a key feature in supervised\nmachine learning applications. It guarantees that these systems can be used in\nhigh-dimensional situations and in the presence of large datasets. The $L ^p $\ncriteria used in this paper allow the formulation of universality results that\ndo not necessarily impose almost sure uniform boundedness in the inputs or the\nfading memory property in the filter that needs to be approximated.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nFirst, it is proved that linear reservoir systems with either polynomial or neural network readout maps are universal."
    },
    {
        "abstract": "  Following a recent proof of Shannon's entropy power inequality (EPI), a\ncomprehensive framework for deriving various EPIs for the R\\'enyi entropy is\npresented that uses transport arguments from normal densities and a change of\nvariable by rotation. Simple arguments are given to recover the previously\nknown R\\'enyi EPIs and derive new ones, by unifying a multiplicative form with\nconstant c and a modification with exponent {\\alpha} of previous works. In\nparticular, for log-concave densities, we obtain a simple transportation proof\nof a sharp varentropy bound.\n",
        "method": "Methodological sentences:\n\n* Following a recent proof of Shannon's entropy power inequality (EPI), ...\n* Simple arguments are given to recover the previously known R\\'enyi EPIs and derive new ones, by unifying a multiplicative form with constant c and a modification with exponent {\\alpha} of previous works."
    },
    {
        "abstract": "  Recent advances in the field of network representation learning are mostly\nattributed to the application of the skip-gram model in the context of graphs.\nState-of-the-art analogues of skip-gram model in graphs define a notion of\nneighbourhood and aim to find the vector representation for a node, which\nmaximizes the likelihood of preserving this neighborhood.\n  In this paper, we take a drastic departure from the existing notion of\nneighbourhood of a node by utilizing the idea of coreness. More specifically,\nwe utilize the well-established idea that nodes with similar core numbers play\nequivalent roles in the network and hence induce a novel and an organic notion\nof neighbourhood. Based on this idea, we propose core2vec, a new algorithmic\nframework for learning low dimensional continuous feature mapping for a node.\nConsequently, the nodes having similar core numbers are relatively closer in\nthe vector space that we learn.\n  We further demonstrate the effectiveness of core2vec by comparing word\nsimilarity scores obtained by our method where the node representations are\ndrawn from standard word association graphs against scores computed by other\nstate-of-the-art network representation techniques like node2vec, DeepWalk and\nLINE. Our results always outperform these existing methods\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We utilize the well-established idea that nodes with similar core numbers play equivalent roles in the network and hence induce a novel and an organic notion of neighbourhood.\n* Based on this idea, we propose core2vec, a new algorithmic framework for learning low dimensional continuous feature mapping for a node."
    },
    {
        "abstract": "  Since the expense of the numerical integration of large scale dynamical\nsystems is often computationally prohibitive, model reduction methods, which\napproximate such systems by simpler and much lower order ones, are often\nemployed to reduce the computational effort. In this paper, for dynamical\nsystems with a first integral, new structure-preserving model reduction\napproaches are presented that yield reduced-order systems while preserving the\nfirst integral. We apply energy-preserving integrators to the reduced-order\nsystems and show some numerical experiments that demonstrate the favourable\nbehaviour of the proposed approaches.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We apply energy-preserving integrators to the reduced-order systems...\""
    },
    {
        "abstract": "  The study of the effects of scaling on magnetic tunnel junction (MTJ) devices\nhas become an important topic in the field of spin-based memory devices. Here,\nwe investigate the effect of elastic dephasing on trilayer and pentalayer MTJ\nconsidered at small transverse cross-sectional areas using the non-equilibrium\nGreen's function spin transport formalism. We consider the structures with and\nwithout dephasing effects and clearly point out as to how the tunnel\nmagnetoresistance effect gets affected by dephasing. We attribute the trends\nnoted by analyzing the transmission spectra and hence the currents across the\ndevices. Although dephasing affects the TMR values for both devices, we note\nthat the obtained TMR values are still in a reasonable range that may not\nhinder their usability for practical applications.\n",
        "method": "Here is the methodological sentence:\n\nWe investigate the effect of elastic dephasing on trilayer and pentalayer MTJ considered at small transverse cross-sectional areas using the non-equilibrium Green's function spin transport formalism."
    },
    {
        "abstract": "  A huge optical luminosity of the supercritical accretion disc and powerful\nstellar wind in the high-mass X-ray binary SS433 make it difficult to reliably\nestimate the mass ratio of the binary components from spectroscopic\nobservations. We analyze different indirect methods of the mass ratio estimate.\nWe show that with an account of the possible Roche lobe overflow by the optical\nstar, the analysis of X-ray eclipses in the standard and hard X-ray bands\nsuggests the estimate $q=M_\\mathrm{x}/M_\\mathrm{v}\\gtrsim 0.3$. We argue that\nthe double-peak hydrogen Brackett lines in SS433 should form not in the\naccretion disc but in a circumbinary envelope, suggesting a total mass of\n$M_\\mathrm{v}+M_\\mathrm{x}\\gtrsim 40 M_\\odot$. The observed long-term stability\nof the orbital period in SS433 $|\\dot P_b/P_b|\\le 1.793\\times\n10^{-14}$~s$^{-1}$ over $\\sim 28$ year period is used to place an independent\nconstraint of $q\\gtrsim 0.6$ in SS433, confirming its being a Galactic\nmicroquasar hosting a superaccreting black hole.\n",
        "method": "Here are the methodological sentences:\n\n* We analyze different indirect methods of the mass ratio estimate.\n* The analysis of X-ray eclipses in the standard and hard X-ray bands suggests...\n* ...suggesting a total mass of $M_\\mathrm{v}+M_\\mathrm{x}\\gtrsim 40 M_\\odot$.\n* The observed long-term stability of the orbital period in SS433 is used to place an independent constraint..."
    },
    {
        "abstract": "  We prove Carlos Simpson's \"semi-strictification\" (or \"weak unit\") conjecture\nin the case of infinity-groupoids. More precisely, we introduce two precise\nversions of the conjecture, the \"general\" and the \"regular\" conjecture,\ninvolving two different notions of \"non-unital categories\". The \"general\"\nversion involve infinity-categories where absolutely all composition operations\n(horizontal, vertical and whiskering) are defined and compatible, the \"regular\"\nversion involve infinity-categories where all the composition operations\ncorresponding to \"regular\" pasting diagram are defined and compatible. In both\ncase we construct (weak) model structures on these categories such that fibrant\nobjects have weak units and weak inverse. We prove the regular version of the\nconjecture using the original strategy of Kapranov and Voevodsky, together with\nour previous work on polygraphs. The general version cannot be proved by these\nmethods and is still open. In order to do this we also study some subtle\nproperty of the combinatorics of polygraphs, and we construct a new counting\nfunction for polygraphs, inspired by previous work of Makkai.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We introduce two precise versions of the conjecture, the \"general\" and the \"regular\" conjecture, involving two different notions of \"non-uniatal categories\".\n* We construct (weak) model structures on these categories such that fibrant objects have weak units and weak inverse.\n* We prove the regular version of the conjecture using the original strategy of Kapranov and Voevodsky, together with our previous work on polygraphs.\n* We also study some subtle property of the combinatorics of polygraphs, and we construct a new counting function for polygraphs, inspired by previous work of Makkai."
    },
    {
        "abstract": "  We consider the simplest parabolic-elliptic model of chemotaxis in the whole\nspace in several dimensions. Criteria for the existence of radial\nglobal-in-time solutions in terms of suitable Morrey norms are derived.\n",
        "method": "Criteria for the existence of radial global-in-time solutions in terms of suitable Morrey norms are derived."
    },
    {
        "abstract": "  Owing to their connection with generative adversarial networks (GANs),\nsaddle-point problems have recently attracted considerable interest in machine\nlearning and beyond. By necessity, most theoretical guarantees revolve around\nconvex-concave (or even linear) problems; however, making theoretical inroads\ntowards efficient GAN training depends crucially on moving beyond this classic\nframework. To make piecemeal progress along these lines, we analyze the\nbehavior of mirror descent (MD) in a class of non-monotone problems whose\nsolutions coincide with those of a naturally associated variational inequality\n- a property which we call coherence. We first show that ordinary, \"vanilla\" MD\nconverges under a strict version of this condition, but not otherwise; in\nparticular, it may fail to converge even in bilinear models with a unique\nsolution. We then show that this deficiency is mitigated by optimism: by taking\nan \"extra-gradient\" step, optimistic mirror descent (OMD) converges in all\ncoherent problems. Our analysis generalizes and extends the results of\nDaskalakis et al. (2018) for optimistic gradient descent (OGD) in bilinear\nproblems, and makes concrete headway for establishing convergence beyond\nconvex-concave games. We also provide stochastic analogues of these results,\nand we validate our analysis by numerical experiments in a wide array of GAN\nmodels (including Gaussian mixture models, as well as the CelebA and CIFAR-10\ndatasets).\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We analyze the behavior of mirror descent (MD) in a class of non-monotone problems...\""
    },
    {
        "abstract": "  We propose and numerically demonstrate a new very robust and highly selective\nmethod for femtosecond time-resolved chiral spectroscopy using high harmonic\ngeneration (HHG). The method is based on dynamical symmetry breaking from\nchiral media, and relies only on intense electric-dipole transitions, and not\non the interplay of electric and magnetic dipoles. The symmetry breaking\nresults in the emission of a strong chiral signal in the form of otherwise\n'forbidden' harmonics (i.e., that are not emitted from achiral media). The\nintensity of these symmetry-forbidden harmonics is directly correlated to the\nmedia's enantiomeric excess, yielding chiral selectivity. On the contrary, the\nstrength of the 'allowed' harmonics is chiral-independent, hence they can be\nused as a reference to provide chiral selectivity from a single measurement,\nunlike previous time-resolved schemes that require multiple measurements. We\ndemonstrate numerically 96% discrimination level from microscopic gas phase\nemission, outperforming by far previous time-resolved methods (the selectivity\nshould be further enhanced when the HHG process is phase matched). We expect\nthe new method to give rise to precise table-top characterization of chiral\nmedia in the gas-phase, and for highly sensitive time-resolved ultrafast\nprobing of dynamical chiral processes.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The method is based on dynamical symmetry breaking from chiral media...\n* The intensity of these symmetry-forbidden harmonics is directly correlated to the media's enantiomeric excess, yielding chiral selectivity.\n* ...yielding chiral selectivity from a single measurement..."
    },
    {
        "abstract": "  This paper revisits the problem of optimal control law design for linear\nsystems using the global optimal control framework introduced by Vadim Krotov.\nKrotov's approach is based on the idea of total decomposition of the original\noptimal control problem (OCP) with respect to time, by an $ad$ $hoc$ choice of\nthe so-called Krotov's function or solving function, thereby providing\nsufficient conditions for the existence of global solution based on another\noptimization problem, which is completely equivalent to the original OCP. It is\nwell known that the solution of this equivalent optimization problem is\nobtained using an iterative method. In this paper, we propose suitable Krotov's\nfunctions for linear quadratic OCP and subsequently, show that by imposing\nconvexity condition on this equivalent optimization problem, there is no need\nto compute an iterative solution. We also give some key insights into the\nsolution procedure of the linear quadratic OCP using the proposed methodology\nin contrast to the celebrated Calculus of Variations (CoV) and\nHamilton-Jacobi-Bellman (HJB) equation based approach.\n",
        "method": "Krotov's approach is based on the idea of total decomposition of the original optimal control problem (OCP) with respect to time, by an $ad$ $hoc$ choice of the so-called Krotov's function or solving function, thereby providing sufficient conditions for the existence of global solution based on another optimization problem, which is completely equivalent to the original OCP."
    },
    {
        "abstract": "  Reconstruction of the shape and motion of humans from RGB-D is a challenging\nproblem, receiving much attention in recent years. Recent approaches for\nfull-body reconstruction use a statistic shape model, which is built upon\naccurate full-body scans of people in skin-tight clothes, to complete invisible\nparts due to occlusion. Such a statistic model may still be fit to an RGB-D\nmeasurement with loose clothes but cannot describe its deformations, such as\nclothing wrinkles. Observed surfaces may be reconstructed precisely from actual\nmeasurements, while we have no cues for unobserved surfaces. For full-body\nreconstruction with loose clothes, we propose to use lower dimensional\nembeddings of texture and deformation referred to as eigen-texturing and\neigen-deformation, to reproduce views of even unobserved surfaces. Provided a\nfull-body reconstruction from a sequence of partial measurements as 3D meshes,\nthe texture and deformation of each triangle are then embedded using\neigen-decomposition. Combined with neural-network-based coefficient regression,\nour method synthesizes the texture and deformation from arbitrary viewpoints.\nWe evaluate our method using simulated data and visually demonstrate how our\nmethod works on real data.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Provided a full-body reconstruction from a sequence of partial measurements as 3D meshes, the texture and deformation of each triangle are then embedded using eigen-decomposition.\""
    },
    {
        "abstract": "  We consider the simplest parabolic-elliptic model of chemotaxis in the whole\nspace in several dimensions. Criteria for the blowup of radially symmetric\nsolutions in terms of suitable Morrey spaces norms are derived.\n",
        "method": "Criteria for the blowup of radially symmetric solutions in terms of suitable Morrey spaces norms are derived."
    },
    {
        "abstract": "  A simple theoretical method for deducing the effective bond-orbital model\n(EBOM) of III-nitride wurtzite (WZ) semiconductors is presented. In this model,\nthe interaction parameters for zinc-blende (ZB) structures are used as an\ninitial guess for WZ structure based on the two-center approximation. The\nelectronic band structure of III-nitride WZ semiconductors can hence be\nproduced by utilizing this set of parameters modified to include effects due to\nthree-center integrals and fitting with first-principles calculations. Details\nof the semi-empirical fitting procedure for constructing the EBOM Hamiltonian\nfor bulk III-nitride WZ semiconductors are presented. The electronic band\nstructure of bulk AlN, GaN, and InN with WZ structure calculated by EBOM with\nmodified interaction parameters are shown and compared to the results obtained\nfrom density functional (DFT) theory with meta-generalized gradient\napproximation (mGGA). The set of parameters are further optimized by using a\ngenetic algorithm. In the end, electronic band structures and electron (hole)\neffective masses near the zone center calculated by the proposed model with\nbest fitting parameters are analyzed and compared with the $\\mathbf{k}\\cdot\n\\mathbf{p}$ model.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe interaction parameters for zinc-blende (ZB) structures are used as an initial guess for WZ structure based on the two-center approximation."
    },
    {
        "abstract": "  Video prediction aims to generate realistic future frames by learning dynamic\nvisual patterns. One fundamental challenge is to deal with future uncertainty:\nHow should a model behave when there are multiple correct, equally probable\nfuture? We propose an Appearance-Motion Conditional GAN to address this\nchallenge. We provide appearance and motion information as conditions that\nspecify how the future may look like, reducing the level of uncertainty. Our\nmodel consists of a generator, two discriminators taking charge of appearance\nand motion pathways, and a perceptual ranking module that encourages videos of\nsimilar conditions to look similar. To train our model, we develop a novel\nconditioning scheme that consists of different combinations of appearance and\nmotion conditions. We evaluate our model using facial expression and human\naction datasets and report favorable results compared to existing methods.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe provide appearance and motion information as conditions that specify how the future may look like, reducing the level of uncertainty."
    },
    {
        "abstract": "  The problem of creation of high energy neutrino beams on the basis of modern\nand future circular proton accelerators with the help of traditional technology\nseems to be expensive and difficult. Because of this, we propose the solution\nof this problem based on the usage of focusing bend single crystals. In the\npaper we demonstrate the possibilities of acceptance and focusing of a pion\nbeam with the help of a crystal optical lens system. As an illustration of\nthese features the calculated neutrino fluxes for energy of circulating proton\nbeam equal to 6.5 TeV are presented.\n",
        "method": "We propose the solution of this problem based on the usage of focusing bend single crystals."
    },
    {
        "abstract": "  Today's software industry requires individuals who are proficient in as many\nprogramming languages as possible. Structured query language (SQL), as an\nadopted standard, is no exception, as it is the most widely used query language\nto retrieve and manipulate data. However, the process of learning SQL turns out\nto be challenging. The need for a computer-aided solution to help users learn\nSQL and improve their proficiency is vital. In this study, we present a new\napproach to help users conceptualize basic building blocks of the language\nfaster and more efficiently. The adaptive design of the proposed approach aids\nusers in learning SQL by supporting their own path to the solution and\nemploying successful previous attempts, while not enforcing the ideal solution\nprovided by the instructor. Furthermore, we perform an empirical evaluation\nwith 93 participants and demonstrate that the employment of hints is\nsuccessful, being especially beneficial for users with lower prior knowledge.\n",
        "method": "The proposed approach employs an adaptive design that supports learners' own path to the solution and utilizes their successful previous attempts, rather than enforcing the ideal solution provided by the instructor."
    },
    {
        "abstract": "  It is believed that satellites of giant planets form in circumplanetary\ndisks. Many of the previous contributions assumed that their formation process\nproceeds similarly to rocky planet formation, via accretion of the satellite\nseeds, called satellitesimals. However, the satellitesimal formation itself\nposes a nontrivial problem as the dust evolution in the circumplanetary disk is\nheavily impacted by fast radial drift and thus dust growth to satellitesimals\nis hindered. To address this problem, we connected state-of-the-art\nhydrodynamical simulations of a circumplanetary disk around a Jupiter-mass\nplanet with dust growth and drift model in a post-processing step. We found\nthat there is an efficient pathway to satellitesimal formation if there is a\ndust trap forming within the disk. Thanks to the natural existence of an\noutward gas flow region in the hydrodynamical simulation, a significant dust\ntrap arises at the radial distance of 85~R$_{\\rm J}$ from the planet, where the\ndust-to-gas ratio becomes high enough to trigger streaming instability. The\nstreaming instability leads to the efficient formation of the satellite seeds.\nBecause of the constant infall of material from the circumstellar disk and the\nvery short timescale of dust evolution, the circumplanetary disk acts as a\nsatellitesimal factory, constantly processing the infalling dust to pebbles\nthat gather in the dust trap and undergo the streaming instability.\n",
        "method": "Here is the methodological sentence:\n\nWe connected state-of-the-art hydrodynamical simulations of a circumplanetary disk around a Jupiter-mass planet with dust growth and drift model in a post-processing step."
    },
    {
        "abstract": "  While the relation between visualization and scientific understanding has\nbeen a topic of long-standing discussion, recent developments in physics have\npushed the boundaries of this debate to new and still unexplored realms. For it\nis claimed that, in certain theories of quantum gravity, spacetime\n'disappears': and this suggests that one may have sensible physical theories in\nwhich spacetime is completely absent. This makes the philosophical question\nwhether such theories are intelligible, even more pressing. And if such\ntheories are intelligible, the question then is how they manage to do so. In\nthis paper, we adapt the contextual theory of scientific understanding,\ndeveloped by one of us, to fit the novel challenges posed by physical theories\nwithout spacetime. We construe understanding as a matter of skill rather than\njust knowledge. The appeal is thus to understanding, rather than explanation,\nbecause we will be concerned with the tools that scientists have at their\ndisposal for understanding these theories. Our central thesis is that such\nphysical theories can provide scientific understanding, and that such\nunderstanding does not require spacetimes of any sort. Our argument consists of\nfour consecutive steps: (a) We argue, from the general theory of scientific\nunderstanding, that although visualization is an oft-used tool for\nunderstanding, it is not a necessary condition for it; (b) we criticise certain\nmetaphysical preconceptions which can stand in the way of recognising how\nintelligibility without spacetime can be had; (c) we catalogue tools for\nrendering theories without a spacetime intelligible; and (d) we give examples\nof cases in which understanding is attained without a spacetime, and explain\nwhat kind of understanding these examples provide.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We adapt the contextual theory of scientific understanding, developed by one of us, to fit the novel challenges posed by physical theories without spacetime.\n* Our central thesis is that such physical theories can provide scientific understanding, and that such understanding does not require spacetimes of any sort.\n* Our argument consists of four consecutive steps: (a) ...; (b) ...; (c) ...; and (d) ...\n* We catalogue tools for rendering theories without a spacetime intelligible;\n* We give examples of cases in which understanding is attained without a spacetime, and explain what kind of understanding these examples provide."
    },
    {
        "abstract": "  For a normalized root system $R$ in $\\mathbb R^N$ and a multiplicity function\n$k\\geq 0$ let $\\mathbf N=N+\\sum_{\\alpha \\in R} k(\\alpha)$. Denote by\n$dw(\\mathbf x)=\\prod_{\\alpha\\in R}|\\langle \\mathbf\nx,\\alpha\\rangle|^{k(\\alpha)}\\, d\\mathbf x $ the associated measure in $\\mathbb\nR^N$. Let $\\mathcal F$ stands for the Dunkl transform. Given a bounded function\n$m$ on $\\mathbb R^N$, we prove that if there is $s>\\mathbf N$ such that $m$\nsatisfies the classical H\\\"ormander condition with the smoothness $s$, then the\nmultiplier operator $\\mathcal T_mf=\\mathcal F^{-1}(m\\mathcal Ff)$ is of weak\ntype $(1,1)$, strong type $(p,p)$ for $1<p<\\infty$, and bounded on a relevant\nHardy space $H^1$. To this end we study the Dunkl translations and the Dunkl\nconvolution operators and prove that if $F$ is sufficiently regular, for\nexample its certain Schwartz class seminorm is finite, then the Dunkl\nconvolution operator with the function $F$ is bounded on $L^p(dw)$ for $1\\leq\np\\leq \\infty$. We also consider boundedness of maximal operators associated\nwith the Dunkl convolutions with Schwartz class functions.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* None (there are no explicit methodological sentences in this abstract)"
    },
    {
        "abstract": "  In this work, we calculate the $CP$-averaged branching ratios and direct\n$CP$-violating asymmetries of the quasi-two-body decays $B_{(s)}\\to P\nf_2(1270)\\to P\\pi\\pi$ with the two-pion distribution amplitude\n$\\Phi_{\\pi\\pi}^{\\rm D}$ by using the perturbative QCD factorization approach,\nwhere $P$ represents a light pseudoscalar meson $K, \\pi, \\eta$ and\n$\\eta^{\\prime}$. The relativistic Breit-Wigner formula for the $D$-wave\nresonance $f_2(1270)$ is adopted to parameterize the timelike form factor\n$F_{\\pi}$, which contains the final state interactions between the pions in the\nresonant regions. The consistency of theoretical results with data can be\nachieved by determining the Gegenbauer moments of the $D$-wave two-pion\ndistribution amplitudes. The decay rates for the considered decay modes are\ngenerally in the order of $10^{-9}$ to $ 10^{-6}$. The integrated direct $CP$\nasymmetries for the charged modes agree with the {\\it BABAR} and Belle\nmeasurements. As a by-product, we extract the branching ratios of $B_{(s)}\\to\nPf_2(1270)$ from the corresponding quasi-two-body decay modes, which still need\nexperimental tests at the ongoing and forthcoming experiments.\n",
        "method": "In this work, we calculate the $CP$-averaged branching ratios and direct $CP$-violating asymmetries of the quasi-two-body decays $B_{(s)}\\to P f_2(1270)\\to P\\pi\\pi$ with the two-pion distribution amplitude $\\Phi_{\\pi\\pi}^{\\rm D}$ by using the perturbative QCD factorization approach..."
    },
    {
        "abstract": "  We give some necessary conditions for maximality of $0/1$-determinant. Let\n${\\bf M}$ be a nondegenerate $0/1$-matrix of order $n$. Denote by $\\bf A$ the\nmatrix of order $n+1$ which appears from ${\\bf M}$ after adding the $(n+1)$th\nrow $(0,0,\\ldots,0,1)$ and the $(n+1)$th column consisting of $1$'s. Suppose\n${\\bf A}^{-1}=(l_{ij}),$ then for all $i=1,\\ldots,n$ we have $\\sum_{j=1}^{n+1}\n|l_{ij}|\\geq 2.$ Moreover, if $|\\det({\\bf M})|$ is equal to the maximum value\nof a $0/1$-determinant of order $n$, then $\\sum_{j=1}^{n+1} |l_{ij}|= 2$ for\nall $i=1,\\ldots,n$.\n  Keywords: maximum 0/1-deteminant, simplex, cube, axial diameter\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nSuppose ${\\bf A}^{-1}=(l_{ij}),$ then for all $i=1,\\ldots,n$ we have $\\sum_{j=1}^{n+1} |l_{ij}| \\geq 2.$"
    },
    {
        "abstract": "  The present contribution does not aim at replacing the huge and often\nexcellent literature on DFT for atomic nuclei, but tries to provide an updated\nintroduction to this topic. The goal would be, ideally, to help a fresh M.Sc.\nor Ph.D. student (or a researcher from other fields) to become acquainted with\nsome basic concepts, and then move to the specialized textbooks or papers with\nsome ability for orienteering. We first introduce the basics of DFT, and show\nthe difference with the \"naive\" mean-field theory, that is doomed to fail as a\nmodel even in the simple case of uniform nuclear matter. We introduce the\nEnergy Density Functionals (EDFs) that are used in nuclear structure, with few\nexamples of their applications. The concepts of symmetry breaking and\nrestoration are briefly discussed. We also include an introduction to the\ntime-dependent extension of DFT that, so far, has been implemented essentially\nonly in the adiabatic approximation and has been applied mainly to the study of\nnuclear vibrations. With this material, we hope that any reader is able to deal\nwith the texts that go deeper into each of the topics, having understood that\nDFT is probably the best compromise in nuclear structure theory between\nsimplicity, accuracy, and broad range of applicability.\n",
        "method": "We introduce the basics of DFT, and show the difference with the \"naive\" mean-field theory, that is doomed to fail as a model even in the simple case of uniform nuclear matter."
    },
    {
        "abstract": "  Using a single quantum probe to sense other quantum objects offers distinct\nadvantages but suffers from some limitations that may degrade the sensing\nprecision severely, especially when the probe-target coupling is weak. Here we\npropose a strategy to improve the sensing precision by using the quantum probe\nto engineer the evolution of the target. We consider an exactly solvable model,\nin which a qubit is used as the probe to sense the frequency of a harmonic\noscillator. We show that by applying adaptive periodic quantum control on the\nqubit, the sensing precision can be enhanced from 1/T scaling with the total\ntime cost T to 1/T^{2} scaling, thus improving the precision by several orders\nof magnitudes. Such improvement can be achieved without any direct access to\nthe oscillator and the improvement increases with decreasing probe-target\ncoupling. This provides a useful routine to ultrasensitive quantum sensing of\nweakly coupled quantum objects.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We consider an exactly solvable model, in which a qubit is used as the probe to sense the frequency of a harmonic oscillator.\""
    },
    {
        "abstract": "  We prove that a totally real manifold (of maximal dimension) is a boundary\nuniqueness set for a psh function on an almost complex manifold.\n",
        "method": "No methodological sentences found in this abstract."
    },
    {
        "abstract": "  In this paper, we prove the irreducibility of the monodromy action on the\nanti-invariant part of the vanishing cohomology on a double cover of a very\ngeneral element in an ample hypersurface of a complex smooth projective variety\nbranched at an ample divisor. As an application, we study dominant rational\nmaps from a double cover of a very general surface $S$ of degree$\\geq 7$ in\n${\\mathbb P}^3$ branched at a very general quadric surface to smooth projective\nsurfaces $Z$. Our method combines the classification theory of algebraic\nsurfaces, deformation theory, and Hodge theory.\n",
        "method": "Our method combines the classification theory of algebraic surfaces, deformation theory, and Hodge theory."
    },
    {
        "abstract": "  In this paper, a novel image encryption algorithm, which involves a chaotic\nblock image scrambling followed by a two-dimensional (2-D) discrete linear\nchirp transform, is proposed. The definition of the 2-D discrete linear chirp\ntransform is introduced and then it is used to construct the basis of the novel\nencryption algorithm. Finally, security analysis are performed to show the\nquality of the encryption process using different metrics.\n",
        "method": "Here is the methodological sentence:\n\nThe definition of the 2-D discrete linear chirp transform is introduced and then it is used to construct the basis of the novel encryption algorithm."
    },
    {
        "abstract": "  Agent based simulation of social organizations, via the investigation of\nagents' training and learning tactics and strategies, has been inspired by the\nability of humans to learn from social environments which are rich in agents,\ninteractions and partial or hidden information. Such richness is a source of\ncomplexity that an effective learner has to be able to navigate. This paper\nfocuses on the investigation of the impact of the environmental complexity on\nthe game playing-and-learning behavior of synthetic agents. We demonstrate our\napproach using two independent turn-based zero-sum games as the basis of\nforming social events which are characterized both by competition and\ncooperation. The paper's key highlight is that as the complexity of a social\nenvironment changes, an effective player has to adapt its learning and playing\nprofile to maintain a given performance profile\n",
        "method": "Here is the methodological sentence:\n\n\"We demonstrate our approach using two independent turn-based zero-sum games as the basis of forming social events which are characterized both by competition and cooperation.\""
    },
    {
        "abstract": "  Coronagraphs on future space telescopes will require precise wavefront\ncorrection to detect Earth-like exoplanets near their host stars. High-actuator\ncount microelectromechanical system (MEMS) deformable mirrors provide wavefront\ncontrol with low size, weight, and power. The Deformable Mirror Demonstration\nMission (DeMi) payload will demonstrate a 140 actuator MEMS deformable mirror\n(DM) with \\SI{5.5}{\\micro\\meter} maximum stroke. We present the flight\noptomechanical design, lab tests of the flight wavefront sensor and wavefront\nreconstructor, and simulations of closed-loop control of wavefront aberrations.\nWe also present the compact flight DM controller, capable of driving up to 192\nactuator channels at 0-250V with 14-bit resolution. Two embedded Raspberry Pi 3\ncompute modules are used for task management and wavefront reconstruction. The\nspacecraft is a 6U CubeSat (30 cm x 20 cm x 10 cm) and launch is planned for\n2019.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe present the flight optomechanical design, lab tests of the flight wavefront sensor and wavefront reconstructor, and simulations of closed-loop control of wavefront aberrations."
    },
    {
        "abstract": "  We introduce a notion of \"weak model category\" which is a weakening of the\nnotion of Quillen model category, still sufficient to define a homotopy\ncategory, Quillen adjunctions, Quillen equivalences and most of the usual\nconstruction of categorical homotopy theory. Both left and right semi-model\ncategories are weak model categories, and the opposite of a weak model category\nis again a weak model category. The main advantages of weak model categories is\nthat they are easier to construct than Quillen model categories. In particular\nwe give some simple criteria on two weak factorization systems for them to form\na weak model category. The theory is developed in a very weak constructive\nframework and we use it to produce, completely constructively (even\npredicatively), weak versions of various standard model categories, including\nthe Kan-Quillen model structure, the variant of the Joyal model structure on\nmarked simplicial sets, and the Verity model structure for weak complicial\nsets. We also construct semi-simplicial versions of all these.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The main advantages of weak model categories is that they are easier to construct than Quillen model categories.\""
    },
    {
        "abstract": "  This work addresses the challenge of minimizing the energy consumption of a\nwireless communication network by joint optimization of the base station\ntransmit power and the cell activity. A mixed-integer nonlinear optimization\nproblem is formulated, for which a computationally tractable linear inner\napproximation algorithm is provided. The proposed method offers great\nflexibility in optimizing the network operation by considering multiple system\nparameters jointly, which mitigates a major drawback of existing\nstate-of-the-art schemes that are mostly based on heuristics. Simulation\nresults show that the proposed method exhibits high performance in decreasing\nthe energy consumption, and provides implicit load balancing in difficult high\ndemand scenarios.\n",
        "method": "A mixed-integer nonlinear optimization problem is formulated, for which a computationally tractable linear inner approximation algorithm is provided."
    },
    {
        "abstract": "  A two-dimensional scandium monochloride sheet was investigated by using\ndensity functional theory. It could be exfoliated from a known bulk material\nwith a cleavage energy slightly lower than that of graphene. The sheet has a\nferromagnetic ground state with a Curie temperature of 100 K. Moreover, the\nsheet becomes a half-metal under hole doping. The Curie temperature increases\nto 250 K with the doping amount of 0.4 per primitive cell, which is close to\nthe ice point. The two-dimensional scandium monochloride sheet should be a good\ncandidate for two-dimensional spintronics.\n",
        "method": "It could be exfoliated from a known bulk material with a cleavage energy slightly lower than that of graphene."
    },
    {
        "abstract": "  Numerous pattern recognition applications can be formed as learning from\ngraph-structured data, including social network, protein-interaction network,\nthe world wide web data, knowledge graph, etc. While convolutional neural\nnetwork (CNN) facilitates great advances in gridded image/video understanding\ntasks, very limited attention has been devoted to transform these successful\nnetwork structures (including Inception net, Residual net, Dense net, etc.) to\nestablish convolutional networks on graph, due to its irregularity and\ncomplexity geometric topologies (unordered vertices, unfixed number of adjacent\nedges/vertices). In this paper, we aim to give a comprehensive analysis of when\nwork matters by transforming different classical network structures to graph\nCNN, particularly in the basic graph recognition problem. Specifically, we\nfirstly review the general graph CNN methods, especially in its spectral\nfiltering operation on the irregular graph data. We then introduce the basic\nstructures of ResNet, Inception and DenseNet into graph CNN and construct these\nnetwork structures on graph, named as G_ResNet, G_Inception, G_DenseNet. In\nparticular, it seeks to help graph CNNs by shedding light on how these\nclassical network structures work and providing guidelines for choosing\nappropriate graph network frameworks. Finally, we comprehensively evaluate the\nperformance of these different network structures on several public graph\ndatasets (including social networks and bioinformatic datasets), and\ndemonstrate how different network structures work on graph CNN in the graph\nrecognition task.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We then introduce the basic structures of ResNet, Inception and DenseNet into graph CNN and construct these network structures on graph, named as G_ResNet, G_Inception, G_DenseNet.\n* Specifically, it seeks to help graph CNNs by shedding light on how these classical network structures work and providing guidelines for choosing appropriate graph network frameworks."
    },
    {
        "abstract": "  We introduce one-shot texture segmentation: the task of segmenting an input\nimage containing multiple textures given a patch of a reference texture. This\ntask is designed to turn the problem of texture-based perceptual grouping into\nan objective benchmark. We show that it is straight-forward to generate large\nsynthetic data sets for this task from a relatively small number of natural\ntextures. In particular, this task can be cast as a self-supervised problem\nthereby alleviating the need for massive amounts of manually annotated data\nnecessary for traditional segmentation tasks. In this paper we introduce and\nstudy two concrete data sets: a dense collage of textures (CollTex) and a\ncluttered texturized Omniglot data set. We show that a baseline model trained\non these synthesized data is able to generalize to natural images and videos\nwithout further fine-tuning, suggesting that the learned image representations\nare useful for higher-level vision tasks.\n",
        "method": "We show that it is straight-forward to generate large synthetic data sets for this task from a relatively small number of natural textures."
    },
    {
        "abstract": "  We study two new models of two particle species invading a surface from\nopposite sides. Collisions of particles of different species lead to the\nformation of congestion fronts. One of the models implements a reversible\nprocess whereas in the other model the congestion front forms irreversibly. For\nboth models we find that the congestion fronts are self-affine but with\ndifferent roughness exponents. For low densities the system does not congest\nand we find a phase transition between a phase of freely moving particles and a\ncongestion phase.\n",
        "method": "Collisions of particles of different species lead to the formation of congestion fronts."
    },
    {
        "abstract": "  An overview of nuclei and anti-nuclei production with results from different\nexperiments are discussed. The comparison of data with the thermal and\ncoalescence models is also discussed to understand their production mechanisms\nin high energy collisions.\n",
        "method": "None. This abstract does not contain any methodological sentences. It provides an overview and discusses comparisons between data and models, but it does not describe specific methods or approaches used in the research."
    },
    {
        "abstract": "  Solving the classification problem, unbalanced number of dataset among the\nclasses often causes performance degradation. Especially when some classes\ndominate the other classes with its large number of datasets, trained model\nshows low performance in identifying the dominated classes. This is common case\nwhen it comes to medical dataset. Because the case with a serious degree is not\nquite usual, there are imbalance in number of dataset between severe case and\nnormal cases of diseases. Also, there is difficulty in precisely identifying\ngrade of medical data because of vagueness between them. To solve these\nproblems, we propose new architecture of convolutional neural network named\nTournament based Ranking CNN which shows remarkable performance gain in\nidentifying dominated classes while trading off very small accuracy loss in\ndominating classes. Our Approach complemented problems that occur when method\nof Ranking CNN that aggregates outputs of multiple binary neural network models\nis applied to medical data. By having tournament structure in aggregating\nmethod and using very deep pretrained binary models, our proposed model\nrecorded 68.36% of exact match accuracy, while Ranking CNN recorded 53.40%,\npretrained Resnet recorded 56.12% and CNN with linear regression recorded\n57.48%. As a result, our proposed method is applied efficiently to cataract\ngrading which have ordinal labels with imbalanced number of data among classes,\nalso can be applied further to medical problems which have similar features to\ncataract and similar dataset configuration.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* To solve these problems, we propose new architecture of convolutional neural network named Tournament based Ranking CNN...\n* Our Approach complemented problems that occur when method of Ranking CNN that aggregates outputs of multiple binary neural network models is applied to medical data."
    },
    {
        "abstract": "  Deep learning models are often not easily adaptable to new tasks and require\ntask-specific adjustments. The differentiable neural computer (DNC), a\nmemory-augmented neural network, is designed as a general problem solver which\ncan be used in a wide range of tasks. But in reality, it is hard to apply this\nmodel to new tasks. We analyze the DNC and identify possible improvements\nwithin the application of question answering. This motivates a more robust and\nscalable DNC (rsDNC). The objective precondition is to keep the general\ncharacter of this model intact while making its application more reliable and\nspeeding up its required training time. The rsDNC is distinguished by a more\nrobust training, a slim memory unit and a bidirectional architecture. We not\nonly achieve new state-of-the-art performance on the bAbI task, but also\nminimize the performance variance between different initializations.\nFurthermore, we demonstrate the simplified applicability of the rsDNC to new\ntasks with passable results on the CNN RC task without adaptions.\n",
        "method": "Here is the methodological sentence:\n\nWe analyze the DNC and identify possible improvements within the application of question answering."
    },
    {
        "abstract": "  In electronic band structures, nodal lines may arise when two (or more) bands\ncontact and form a one-dimensional manifold of degeneracy in the Brillouin\nzone. Around a nodal line, the dispersion for the energy difference between the\nbands is typically linear in any plane transverse to the line. Here, we perform\nan exhaustive search over all 230 space groups for nodal lines with\nhigher-order dispersions that can be stabilized by crystalline symmetry in\nsolid state systems with spin-orbit coupling and time reversal symmetry. We\nfind that besides conventional linear nodal lines, only lines with quadratic or\ncubic dispersions are possible, for which the allowed degeneracy cannot be\nlarger than two. We derive effective Hamiltonians to characterize the novel\nlow-energy fermionic excitations for the quadratic and cubic nodal lines, and\nexplicitly construct minimal lattice models to further demonstrate their\nexistence. Their signatures can manifest in a variety of physical properties\nsuch as the (joint) density of states, magneto-response, transport behavior,\nand topological surface states. Using ab-initio calculations, we also identify\npossible material candidates that realize these exotic nodal lines.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe perform an exhaustive search over all 230 space groups for nodal lines with higher-order dispersions that can be stabilized by crystalline symmetry in solid state systems with spin-orbit coupling and time reversal symmetry."
    },
    {
        "abstract": "  The double carrier transport has been observed in thin film black phosphorus\n(BP) field effect transistor (FET) devices in highly electron doped region. BP\nthin films with typical thickness of 15 nm were encapsulated by hexagonal boron\nnitride (h-BN) thin films to avoid degradation by air exposure. Their Hall\nmobility has reached 5300 cm2/Vs and 5400 cm2/Vs at 4.2 K in the hole and\nelectron doped regions, respectively. The gate voltage dependence of\nconductivity exhibits an anomalous shoulder structure in electron doped region.\nIn addition, at gate voltages above the shoulder, the magnetoresistance changes\nto positive, and there appears an additional slow Shubnikov-de Haas\noscillation. These results strongly suggest the appearance of the second\ncarriers, which originate from the second subband with localized band edge.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"BP thin films with typical thickness of 15 nm were encapsulated by hexagonal boron nitride (h-BN) thin films to avoid degradation by air exposure.\""
    },
    {
        "abstract": "  We extend results of Bongiovanni et al. on double bubbles on the line with\nlog-convex density to the case where the derivative of the log of the density\nis bounded. We show that the tie function between the double interval and the\ntriple interval still exists but may blow up to infinity in finite time. For\nthe first time, a density is presented for which the blowup time is positive\nand finite.\n",
        "method": "We extend results of Bongiovanni et al. on double bubbles on the line with log-convex density to the case where the derivative of the log of the density is bounded."
    },
    {
        "abstract": "  PSR B1259-63 is a $\\gamma$-ray emitting high mass X-ray binary system, in\nwhich the compact object is a millisecond pulsar. The system has an orbital\nperiod of 1236.7 d and shows peculiar $\\gamma$-ray flares when the neutron star\nmoves out of the stellar disk of the companion star. The $\\gamma$-ray flare\nevents were firstly discovered by using Fermi-LAT around the 2010 periastron\npassage, which was repeated for the 2014 and 2017 periastron passages. We\nanalyze the Fermi-LAT data for all the three periastron passages and found that\nin each flare the energy spectrum can be represented well by a simple power\nlaw. The $\\gamma$-ray light curves show that in 2010 and 2014 after each\nperiastron there are two main flares, but in 2017 there are four flares\nincluding one precursor about 10 d after the periastron passage. The first main\nflares in 2010 and 2014 are located at around 35 d after the periastron\npassage, and the main flare in 2014 is delayed by roughly 1.7 d with respect to\nthat in 2010. In the 2017 flare, the source shows a precursor about 10 d after\nthe periastron passage, but the following two flares become weaker and lag\nbehind those in 2010 by roughly 5 d. The strongest flares in 2017 occurred 58 d\nand 70 d after the periastron passage. These results challenge the previous\nmodels.\n",
        "method": "We analyze the Fermi-LAT data for all the three periastron passages and found that in each flare the energy spectrum can be represented well by a simple power law."
    },
    {
        "abstract": "  By applying density functional theory calculations, we predict that the\ngroundstate of bilayer silicene at certain interlayer distances can be\nantiferromagnetic. At small electron or hole doping, it becomes half metallic\nunder applied out-of-plane electric field, which can be used to produce fully\nspin-polarized field-effect-driven current even in the absence of external\nmagnetic field, ferromagnetic substrates, doped magnetic ions, or spin-orbital\ncoupling. Our finding points out a new route to overcome the major challenge of\nspintronics.\n",
        "method": "By applying density functional theory calculations, we predict that the groundstate of bilayer silicene at certain interlayer distances can be antiferromagnetic."
    },
    {
        "abstract": "  The purpose of this note is to record a consequence, for general metric\nspaces, of a recent result of David Bate. We prove the following fact: Let $X$\nbe a compact metric space of topological dimension $n$. Suppose that the\n$n$-dimensional Hausdorff measure of $X$, $\\mathcal H^n(X)$, is finite. Suppose\nfurther that the lower n-density of the measure $\\mathcal H^n$ is positive,\n$\\mathcal H^n$-almost everywhere in $X$. Then $X$ contains an $n$-rectifiable\nsubset of positive $\\mathcal H^n$-measure. Moreover, the assumption on the\nlower density is unnecessary if one uses recently announced results of\nCs\\\"ornyei-Jones.\n",
        "method": "Let $X$ be a compact metric space of topological dimension $n$. Suppose that the $n$-dimensional Hausdorff measure of $X$, $\\mathcal H^n( X )$, is finite."
    },
    {
        "abstract": "  We describe the power distribution systems and grounding schemes built for\nthe near and far detectors of the NOvA long-baseline neutrino experiment. They\nare used to power the avalanche photodiodes and their thermoelectric coolers,\nthe front-end boards that read out, digitize and time stamp the signals from\nthe avalanche photodiodes, and the data concentrator modules used to receive\nand format the data from the front-end boards before sending them to a farm of\ncomputers used to build the events. The system powers 344,064 readout channels\nin the far detector and 20,192 channels in the near detector.\n",
        "method": "We describe the power distribution systems and grounding schemes built for the near and far detectors of the NOvA long-baseline neutrino experiment. They are used to power the avalanche photodiodes and their thermoelectric coolers, the front-end boards that read out, digitize and time stamp the signals from the avalanche photodiodes, and the data concentrator modules used to receive and format the data from the front-end boards before sending them to a farm of computers used to build the events."
    },
    {
        "abstract": "  A perturbational vector duality approach for objective functions $f\\colon\nX\\to \\bar{L}^0$ is developed, where $X$ is a Banach space and $\\bar{L}^0$ is\nthe space of extended real valued functions on a measure space, which extends\nthe perturbational approach from the scalar case. The corresponding strong\nduality statement is proved under a closedness type regularity condition.\nOptimality conditions and a Moreau-Rockafellar type formula are provided. The\nresults are specialized for constrained and unconstrained problems. Examples of\nintegral operators and risk measures are discussed.\n",
        "method": "Here is the methodological sentence:\n\nA perturbational vector duality approach for objective functions $f\\colon X \\to \\bar{L}^0$ is developed..."
    },
    {
        "abstract": "  In this paper we study the problem of energy conservation for the solutions\nof the initial boundary value problem associated to the 3D Navier-Stokes\nequations, with Dirichlet boundary conditions. First, we consider Leray-Hopf\nweak solutions and we prove some new criteria, involving the gradient of the\nvelocity. Next, we compare them with the existing literature in scaling\ninvariant spaces and with the Onsager conjecture. Then, we consider the problem\nof energy conservation for very-weak solutions, proving energy equality for\ndistributional solutions belonging to the so-called Shinbrot class. A possible\nexplanation of the role of this classical class of solutions, which is not\nscaling invariant, is also given.\n",
        "method": "First, we consider Leray-Hopf weak solutions and we prove some new criteria, involving the gradient of the velocity."
    },
    {
        "abstract": "  Animal hair examination at a criminal scene may provide valuable information\nin forensic investigations. However, local reference databases for animal hair\nidentification are rare. In the present study, we provide differential\nhistological analysis of hair of some domestic animals in Upper Egypt. For this\npurpose, guard hair of large ruminants (buffalo, camel and cow), small\nruminants (sheep and goat), equine (horse and donkey) and canine (dog and cat)\nwere collected and comparative analysis was performed by light microscopy.\nBased on the hair cuticle scale pattern, type and diameter of the medulla, and\nthe pigmentation, characteristic differential features of each animal species\nwere identified. The cuticle scale pattern was imbricate in all tested animals\nexcept in donkey, in which coronal scales were identified. The cuticle scale\nmargin type, shape and the distance in between were characteristic for each\nanimal species. The hair medulla was continuous in most of the tested animal\nspecies with the exception of sheep, in which fragmental medulla was detected.\nThe diameter of the hair medulla and the margins differ according to the animal\nspecies. Hair shaft pigmentation were not detected in all tested animals with\nthe exception of camel and buffalo, in which granules and streak-like\npigmentation were detected. In conclusion, the present study provides a\nfirst-step towards preparation of a complete local reference database for\nanimal hair identification that can be used in forensic investigations.\n",
        "method": "For this purpose, guard hair of large ruminants (buffalo, camel and cow), small ruminants (sheep and goat), equine (horse and donkey) and canine (dog and cat) were collected and comparative analysis was performed by light microscopy."
    },
    {
        "abstract": "  We investigate the splashback features of dark-matter halos based on cosmic\ndensity and velocity fields. Besides the density correlation function binned by\nthe halo orientation angle which was used in the literature, we introduce, for\nthe first time, the corresponding velocity statistic, alignment momentum\ncorrelation function, to take into account the asphericity of halos. Using\nlarge-volume, high-resolution N-body simulations, we measure the alignment\nstatistics of density and velocity. On halo scales, $x\\sim R_\\mathrm{200m} \\sim\n1Mpc/h$, we detect a sharp steepening in the momentum correlation associated\nwith the physical halo boundary, or the splashback feature, which is found more\nprominent than in the density correlation. We also find that the splashback\nradius determined from the density correlation becomes $\\sim 3.5\\%$ smaller\nthan that from the momentum correlation, with their correlation coefficient\nbeing 0.605. Moreover, the orientation-dependent splashback feature due to halo\nasphericity is measured when the density profile is determined by dark-matter\nparticles, which can be used as a test of collisional CDM since the halo shape\nis predicted to be rounder in such a model.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Using large-volume, high-resolution N-body simulations, we measure the alignment statistics of density and velocity.\n* On halo scales, $x\\sim R_\\mathrm{200m} \\sim 1Mpc/h$, we detect a sharp steepening in the momentum correlation associated with the physical halo boundary, or the splashback feature."
    },
    {
        "abstract": "  Among the five-year government-funded World Class University Projects in\nKorea, the category-3 program approved at Hanyang University in Seoul led to an\nexploratory effort to go from neutron-rich nuclei to dense matter in neutron\nstars. The principal results in what transpired in the effort -- and what\nfollowed afterwards -- are described with the focus on the possibly important,\nhitherto unexplored, role played in nuclear dynamics of topology and hidden\nsymmetries of QCD. The potential link to the proton mass problem is pointed\nout.\n",
        "method": "None found. This abstract does not contain a methodological sentence."
    },
    {
        "abstract": "  Existing methods for diagnosing predictability in climate indices often make\na number of unjustified assumptions about the climate system that can lead to\nmisleading conclusions. We present a flexible family of state-space models\ncapable of separating the effects of external forcing on inter-annual time\nscales, from long-term trends and decadal variability, short term weather\nnoise, observational errors and changes in autocorrelation. Standard potential\npredictability models only estimate the fraction of the total variance in the\nindex attributable to external forcing. In addition, our methodology allows us\nto partition individual seasonal means into forced, slow, fast and error\ncomponents. Changes in the predictable signal within the season can also be\nestimated. The model can also be used in forecast mode to assess both intra-\nand inter-seasonal predictability.\n  We apply the proposed methodology to a North Atlantic Oscillation index for\nthe years 1948-2017. Around 60% of the inter-annual variance in the\nDecember-January-February mean North Atlantic Oscillation is attributable to\nexternal forcing, and 8% to trends on longer time-scales. In some years, the\nexternal forcing remains relatively constant throughout the winter season, in\nothers it changes during the season. Skillful statistical forecasts of the\nDecember-January-February mean North Atlantic Oscillation are possible from the\nend of November onward and predictability extends into March. Statistical\nforecasts of the December-January-February mean achieve a correlation with the\nobservations of 0.48.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe present a flexible family of state-space models capable of separating the effects of external forcing on inter-annual time scales, from long-term trends and decadal variability, short term weather noise, observational errors and changes in autocorrelation."
    },
    {
        "abstract": "  We investigate deposit patterns and associated morphology formed after the\nevaporation of an aqueous droplet containing mono- and bi-dispersed colloidal\nparticles. In particular, the combined effect of substrate heating and particle\ndiameter is investigated. We employ high-speed visualization, optical\nmicroscopy and scanning electron microscopy to characterize the evaporating\ndroplets, particle motion, and deposit morphology, respectively. In the context\nof mono-dispersed colloidal particles, an inner deposit and a typical ring form\nfor smaller and larger particles, respectively, on a nonheated surface. At\nlarger substrate temperature, a thin ring with inner deposit forms, explained\nby the self-pinning of the contact line and advection of the particles from the\ncontact line to the center of the droplet due to Marangoni flow. In the context\nof bi-dispersed colloidal particles, self-sorting of the colloidal particles\nwithin the ring occurs at larger substrate temperature. The smaller particles\ndeposit at the outermost edge as compared to the larger diameter particles and\nthis preferential deposition in a stagnation region near the contact line is\ndue to the spatially-varying height of the liquid-gas interface above the\nsubstrate. The sorting occurs at a smaller ratio of the diameter of the smaller\nand larger particle. At the larger substrate temperature and a larger ratio,\nthe particles do not get sorted and mix into each other. Our measurements show\nthat there exists a critical substrate temperature as well as a diameter ratio\nin order to achieve the sorting. We propose regime maps on substrate\ntemperature-particle diameter and substrate temperature-diameter ratio plane\nfor mono- and bi-dispersed solutions, respectively.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We employ high-speed visualization, optical microscopy and scanning electron microscopy to characterize the evaporating droplets, particle motion, and deposit morphology, respectively.\n\nNote that there is only one methodological sentence in this abstract."
    },
    {
        "abstract": "  We investigate evaporation of a sessile droplet on a non-wetted surface in\nthe framework of diffusion-limited and quasi-steady evaporation. We extend\nprevious models and numerically solve Laplace equation for the diffusion of\nliquid vapor in ambient. We propose a unified, simple and accurate expression\nof the evaporation mass flux valid for 90^o < theta < 180^o, where theta is the\nequilibrium contact angle. In addition, using the derived expression of the\nevaporation mass flux, we propose a simple and accurate expression of the\nevaporation mass rate for a non-wetted surface, which does not exhibit\nsingularity at theta = 180^o. Finally, using the scaling analysis, the\nexpression of the evaporation mass flux is utilized to estimate the direction\nand magnitude of the characteristic evaporation-driven flow velocity inside the\ndroplet on a non-wetted surface. The predicted flow direction is found to be\nconsistent with the previous measurements.\n",
        "method": "We numerically solve Laplace equation for the diffusion of liquid vapor in ambient."
    },
    {
        "abstract": "  We derive some consequences of the Liouville theorem for plurisubharmonic\nfunctions of L.-F. Tam and the author. The first result provides a nonlinear\nversion of the complex splitting theorem (which splits off a factor of\n$\\mathbb{C}$ isometrically from the simply-connected K\\\"ahler manifold with\nnonnegative bisectional curvature and a linear growth holomorphic function) of\nL.-F. Tam and the author. The second set of results concerns the so-called\n$k$-hyperbolicity and its connection with the negativity of the $k$-scalar\ncurvature (when $k=1$ they are the negativity of holomorphic sectional\ncurvature and Kobayashi hyperbolicity) introduced recently by F. Zheng and the\nauthor. We lastly prove a new Schwarz Lemma type estimate in terms of {\\it only\nthe holomorphic sectional curvatures of both domain and target manifolds}.\n",
        "method": "Methodological sentences are not present in this abstract, as it primarily focuses on stating results and theorems rather than describing methods or approaches."
    },
    {
        "abstract": "  Fractal structures pervade nature and are receiving increasing engineering\nattention towards the realization of broadband resonators and antennas. We show\nthat fractal resonators can support the emergence of high-dimensional chaotic\ndynamics even in the context of an elementary, single-transistor oscillator\ncircuit. Sierpi\\'nski gaskets of variable depth are constructed using discrete\ncapacitors and inductors, whose values are scaled according to a simple\nsequence. It is found that in regular fractals of this kind each iteration\neffectively adds a conjugate pole/zero pair, yielding gradually more complex\nand broader frequency responses, which can also be implemented as much smaller\nFoster equivalent networks. The resonators are instanced in the circuit as\none-port devices, replacing the inductors found in the initial version of the\noscillator. By means of a highly simplified numerical model, it is shown that\nincreasing the fractal depth elevates the dimension of the chaotic dynamics,\nleading to high-order hyperchaos. This result is overall confirmed by SPICE\nsimulations and experiments, which however also reveal that the non-ideal\nbehavior of physical components hinders obtaining high-dimensional dynamics.\nThe issue could be practically mitigated by building the Foster equivalent\nnetworks rather than the verbatim fractals. Furthermore, it is shown that\nconsiderably more complex resonances, and consequently richer dynamics, can be\nobtained by rendering the fractal resonators irregular through reshuffling the\ninductors, or even by inserting a limited number of focal imperfections. The\npresent results draw attention to the potential usefulness of fractal\nresonators for generating high-dimensional chaotic dynamics, and underline the\nimportance of irregularities and component non-idealities.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Sierpi\\'nski gaskets of variable depth are constructed using discrete capacitors and inductors, whose values are scaled according to a simple sequence.\n* The resonators are instanced in the circuit as one-port devices, replacing the inductors found in the initial version of the oscillator.\n* By means of a highly simplified numerical model, it is shown that increasing the fractal depth elevates the dimension of the chaotic dynamics...\n* This result is overall confirmed by SPICE simulations and experiments..."
    },
    {
        "abstract": "  The analytical exact solutions to the mixed quantum Rabi model (QRM)\nincluding both one- and two-photon terms are found by using Bogoliubov\noperators. Transcendental functions in terms of $4 \\times 4$ determinants\nresponsible for the exact solutions are derived. These so-called $G$-functions\nwith pole structures can be reduced to the previous ones in the unmixed QRMs.\nThe zeros of $G$-functions reproduce completely the regular spectra. The\nexceptional eigenvalues can also be obtained by another transcendental\nfunction. From the pole structure, we can derive two energy limits when the\ntwo-photon coupling strength tends to the collapse point. All energy levels\nonly collapse to the lower one, which diverges negatively. The level crossings\nin the unmixed QRMs are relaxed to avoided crossings in the present mixed QRM\ndue to absence of parity symmetry. In the weak two-photon coupling regime, the\nmixed QRM is equivalent to an one-photon QRM with an effective positive bias,\nsuppressed photon frequency and enhanced one-photon coupling, which may pave a\nhighly efficient and economic way to access the deep-strong one-photon coupling\nregime.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Using Bogoliubov operators.\n* Deriving transcendental functions in terms of $4 \\times 4$ determinants responsible for the exact solutions.\n* Reducing the so-called $G$-functions with pole structures to previous ones in the unmixed QRMs."
    },
    {
        "abstract": "  In this paper, we consider the set of r-symbols in a full generality. We\nconstruct Hall-Littlewood functions and Kostka functions associated to those\nr-symbols. We also discuss a multi-parameter version of those functions. We\nshow that there exists a general algorithm of computing multi-parameter Kostka\nfunctions. As an application, we show that the generalized Green functions of\nsymplectic groups can be described combinatorially in terms of those\n(one-parameter) Kostka functions.\n",
        "method": "We construct Hall-Littlewood functions and Kostka functions associated to those r-symbols."
    },
    {
        "abstract": "  Symmetry, dimensionality, and interaction are crucial ingredients for phase\ntransitions and quantum states of matter. As a prominent example, the integer\nquantum Hall effect (QHE) represents a topological phase generally regarded as\ncharacteristic for two-dimensional (2D) electronic systems, and its many\naspects can be understood without invoking electron-electron interaction. The\nintriguing possibility of generalizing QHE to three-dimensional (3D) systems\nwas proposed decades ago, yet it remains elusive experimentally. Here, we\nreport clear experimental evidence for the 3D QHE observed in bulk ZrTe5\ncrystals. Owing to the extremely high sample quality, the extreme quantum limit\nwith only the lowest Landau level occupied can be achieved by an applied\nmagnetic field as low as 1.5 T. Remarkably, in this regime, we observe a\ndissipationless longitudinal resistivity rho_xx=0 accompanied with a\nwell-developed Hall resistivity plateau rho_xy=(1\\pm0.1) h/e^2\n(\\lambda_(F,z)/2), where \\lambda_(F,z) is the Fermi wavelength along the field\ndirection (z axis). This striking result strongly suggests a Fermi surface\ninstability driven by the enhanced interaction effects in the extreme quantum\nlimit. In addition, with further increasing magnetic field, both rho_xx and\nrho_xy increase dramatically and display an interesting metal-insulator\ntransition, representing another magnetic field driven quantum phase\ntransition. Our findings not only unambiguously reveal a novel quantum state of\nmatter resulting from an intricate interplay among dimensionality, interaction,\nand symmetry breaking, but also provide a promising platform for further\nexploration of more exotic quantum phases and transitions in 3D systems.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nOwing to the extremely high sample quality, the extreme quantum limit with only the lowest Landau level occupied can be achieved by an applied magnetic field as low as 1.5 T."
    },
    {
        "abstract": "  Near Horizon Geometries with multiply degenerate Killing horizons\n$\\mathcal{H}$ are considered, and their degenerate Killing vector fields\nidentified. We prove that they all arise from hypersurface-orthogonal Killing\nvectors of any cut of $\\mathcal{H}$ with the inherited metric -- cuts are\nspacelike co-dimension two submanifolds contained in $\\mathcal{H}$. For each of\nthese Killing vectors on a given cut, there are three different possibilities\nfor the Near Horizon metric which are presented explicitly. The structure of\nthe metric for Near Horizon Geometries with multiple Killing horizons of order\n$m\\geq 3$ is thereby completely determined, and in particular we prove that the\ncuts on $\\mathcal{H}$ must be warped products with maximally symmetric fibers\n(ergo of constant curvature). The question whether multiple degenerate Killing\nhorizons may lead to inequivalent Near Horizon Geometries by using different\ndegenerate Killings is addressed, and answered on the negative: all Near\nHorizon geometries built from a given multiple degenerate Killing horizon\n(using different degenerate Killings) are isometric.\n",
        "method": "We prove that they all arise from hypersurface-orthogonal Killing vectors of any cut of $\\mathcal{H}$ with the inherited metric -- cuts are spacelike co-dimension two submanifolds contained in $\\mathcal{H}$."
    },
    {
        "abstract": "  We show that a linear Young differential equation generates a topological\ntwo-parameter flow, thus the notions of Lyapunov exponents and Lyapunov\nspectrum are well-defined. The spectrum can be computed using the discretized\nflow and is independent of the driving path for triangular systems which are\nregular in the sense of Lyapunov. In the stochastic setting, the system\ngenerates a stochastic two-parameter flow which satisfies the integrability\ncondition, hence the Lyapunov exponents are random variables of finite moments.\nFinally, we prove a Millionshchikov theorem stating that almost all, in a sense\nof an invariant measure, linear nonautonomous Young differential equations are\nLyapunov regular.\n",
        "method": "The notions of Lyapunov exponents and Lyapunov spectrum are well-defined."
    },
    {
        "abstract": "  We propose a systematic approach to the systems of correlated electrons, the\nso-called $\\mathbf{k}$-DE-GWF method, based on reciprocal-space\n($\\mathbf{k}$-resolved) diagrammatic expansion of the variational\nGutzwiller-type wave function for parametrized models of correlated fermions.\nThe present approach, in contrast to either variational Monte-Carlo (VMC), or\nthe recently developed real-space diagrammatic expansion of the Gutzwiller-type\nwave function (direct-space DE-GWF technique), is applicable directly in the\nthermodynamic limit and thus is suitable for describing selected singular\nfeatures of the wave-vector-dependent quantities. We employ the\n$\\mathbf{k}$-DE-GWF method to extract the non-analytic part of the two leading\nmoments of the fermion spectral-density function across the (two-dimensional)\nBrillouin zone for the Hubbard model and away from the half-filling. Those\nmoments are used to evaluate the nodal quasiparticle velocities and their\nspectral weights in the correlated superconducting state. The two velocities\ndetermined in that manner exhibit scaling with the electron concentration\nqualitatively different from that obtained earlier for the excited states of\nthe high-$T_c$ cuprates within the projected quasi-particle ansatz, and the\nresults are in a very good quantitative agreement with experimental data if\ninterpreted as those characterizing the spectrum below and above the observed\nkink. We provide a detailed discussion of the two gaps and two excitation\nbranches (two velocities) appearing naturally within our DE-GWF approach. The\ntwo separate sets of characteristics distinguish the renormalized quasiparticle\nstates very close to the Fermi surface from the deeper correlated-state\nproperties. Also, an enhancement of the $\\mathbf{k}$-dependent magnetic\nsusceptibility is shown to contain a spin-fluctuation contribution within our\nlanguage.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We propose a systematic approach, the $\\mathbf{k}$-DE-GWF method, based on reciprocal-space diagrammatic expansion of the variational Gutzwiller-type wave function for parametrized models of correlated fermions.\n* The present approach is applicable directly in the thermodynamic limit and thus is suitable for describing selected singular features of the wave-vector-dependent quantities."
    },
    {
        "abstract": "  The lack of proper class discrimination among the Hyperspectral (HS) data\npoints poses a potential challenge in HS classification. To address this issue,\nthis paper proposes an optimal geometry-aware transformation for enhancing the\nclassification accuracy. The underlying idea of this method is to obtain a\nlinear projection matrix by solving a nonlinear objective function based on the\nintrinsic geometrical structure of the data. The objective function is\nconstructed to quantify the discrimination between the points from dissimilar\nclasses on the projected data space. Then the obtained projection matrix is\nused to linearly map the data to more discriminative space. The effectiveness\nof the proposed transformation is illustrated with three benchmark real-world\nHS data sets. The experiments reveal that the classification and dimensionality\nreduction methods on the projected discriminative space outperform their\ncounterpart in the original space.\n",
        "method": "To address this issue, this paper proposes an optimal geometry-aware transformation for enhancing the classification accuracy."
    },
    {
        "abstract": "  Diffusive molecular communication (DMC) is one of the most promising\napproaches for realizing nano-scale communications in biological environments\nfor healthcare applications. In this paper, a DMC system in biological\ncylindrical environment is considered, inspired by blood vessel structures in\nthe body. The internal surface of the cylinder boundary is assumed to be\ncovered by the biological receptors which may irreversibly react with hitting\nmolecules. Also, information molecules diffusing in the fluid medium are\nsubject to a degradation reaction and flow. The concentration Green's function\nof diffusion in this environment is analytically derived which takes into\naccount asymmetry in all radial, axial and azimuthal coordinates. Employing\nobtained Green's function, information channel between transmitter and\ntransparent receiver of DMC is characterized. To evaluate the DMC system in the\nbiological cylinder, a simple on-off keying modulation scheme is adopted and\ncorresponding error probability is derived. Particle based simulation results\nconfirm the proposed analysis. Also, the effect of different system parameters\non the concentration Green's function are examined. Our results reveal that the\ndegradation reaction and the boundary covered by biological receptors may be\nutilized to mitigate intersymbol interference and outperform corresponding\nerror probability.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The concentration Green's function of diffusion in this environment is analytically derived which takes into account asymmetry in all radial, axial and azimuthal coordinates.\""
    },
    {
        "abstract": "  Ventricular Fibrillation (VF), one of the most dangerous arrhythmias, is\nresponsible for sudden cardiac arrests. Thus, various algorithms have been\ndeveloped to predict VF from Electrocardiogram (ECG), which is a binary\nclassification problem. In the literature, we find a number of algorithms based\non signal processing, where, after some robust mathematical operations the\ndecision is given based on a predefined threshold over a single value. On the\nother hand, some machine learning based algorithms are also reported in the\nliterature; however, these algorithms merely combine some parameters and make a\nprediction using those as features. Both the approaches have their perks and\npitfalls; thus our motivation was to coalesce them to get the best out of the\nboth worlds. Hence we have developed, VFPred that, in addition to employing a\nsignal processing pipeline, namely, Empirical Mode Decomposition and Discrete\nTime Fourier Transform for useful feature extraction, uses a Support Vector\nMachine for efficient classification. VFPred turns out to be a robust algorithm\nas it is able to successfully segregate the two classes with equal confidence\n(Sensitivity = 99.99%, Specificity = 98.40%) even from a short signal of 5\nseconds long, whereas existing works though requires longer signals, flourishes\nin one but fails in the other.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"In addition to employing a signal processing pipeline, namely, Empirical Mode Decomposition and Discrete Time Fourier Transform for useful feature extraction, [the algorithm] uses a Support Vector Machine for efficient classification.\""
    },
    {
        "abstract": "  The recent growing trend to develop large-scale satellite constellations\n(i.e., mega-constellation) with low-cost small satellites has brought the need\nfor an efficient and scalable maintenance strategy decision plan. Traditional\nspare strategies for satellite constellations cannot handle these\nmega-constellations due to their limited scalability in number of satellites\nand/or frequency of failures. In this paper, we propose a novel spare strategy\nusing an inventory management approach. We consider a set of parking orbits at\na lower altitude than the constellation for spare storage, and model satellite\nconstellation spare strategy problem using a multi-echelon (s,Q)-type inventory\npolicy, viewing Earth's ground as a supplier, parking orbits as warehouses, and\nin-plane spare stocks as retailers. This inventory model is unique in that the\nparking orbits (warehouses) drift away from the orbital planes over time due to\norbital mechanics' effects, and the in-plane spare stocks (retailers) would\nreceive the resupply from the closest (i.e., minimum waiting time) available\nwarehouse at the time of delivery. The parking orbits (warehouses) are also\nresupplied from the ground (supplier) with stochastic lead time caused by the\norder processing and launch opportunities, leveraging the cost saving effects\nby launching many satellites in one rocket (i.e., batch launch discount). The\nproposed analytical model is validated against simulations using Latin\nHypercube Sampling. Furthermore, based on the proposed model, an optimization\nformulation is introduced to identify the optimal spare strategy, comprising\nthe parking orbits characteristics and all locations policies, to minimize the\nmaintenance cost of the system given performance requirements. The proposed\nmodel and optimization method are applied to a real-world case study of\nsatellite mega-constellation to demonstrate their value.\n",
        "method": "Here is the methodological sentence:\n\nWe model satellite constellation spare strategy problem using a multi-echelon (s,Q)-type inventory policy, viewing Earth's ground as a supplier, parking orbits as warehouses, and in-plane spare stocks as retailers."
    },
    {
        "abstract": "  We investigate the new quantum phases on the extended Kane-Mele-Hubbard model\nof honeycomb lattice in the Hofstadter regime. In this regime, orbital motion\nof the electrons can induce various topological phases with spontaneously\nbroken symmetries when the spin orbit coupling and electron correlations\ncoexist. Here, we consider the interaction effects in the Kane-Mele model and\ndiscuss possible phases in the presence of magnetic field at integer fillings\nof electrons. In particular, focusing on 2{\\pi}/3 magnetic flux per plaquette,\nthe realization of numerous quantum phases are discussed within the mean field\nframework; insulator with coplanar magnetic ordering, ferrimagnetic Chern\ninsulator with nematic charge order, ferrimagnetic-ferrielectric Chern\ninsulators etc. Many of these phase transitions are also accompanied with the\nchange in the topological invariants of the system. Based on our theoretical\nstudy, we propose topological multiferroic phases with a scope of realization\nin 2D van-der Waals materials and optical lattice system where the significant\ninterplay of magnetic field, spin orbit coupling and interactions can be\nengineered.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWithin the mean field framework;..."
    },
    {
        "abstract": "  Electrophoresis has been shown as a novel methodology to enhance heat\nconduction capabilities of nanocolloidal dispersions. A thoroughly designed\nexperimental system has been envisaged to solely probe heat conduction across\nnanofluids by specifically eliminating the buoyancy driven convective\ncomponent. Electric field is applied across the test specimen in order to\ninduce electrophoresis in conjunction with the existing thermal gradient. It is\nobserved that the electrophoretic drift of the nanoparticles acts as an\nadditional thermal transport drift mechanism over and above the already\nexistent Brownian diffusion and thermophoresis dominated thermal conduction. A\nscaling analysis of the thermophoretic and electrophoretic velocities from\nclassical Huckel-Smoluchowski formalism is able to mathematically predict the\nthermal performance enhancement due to electrophoresis. It is also inferred\nthat the dielectric characteristics of the particle material is the major\ndetermining component of the electrophoretic amplification of heat transfer.\nInfluence of surfactants has also been probed into and it is observed that\nenhancing the stability via surface charge modulation can in fact enhance the\nelectrophoretic drift, thereby enhancing heat transfer calibre. Also,\nsurfactants ensure colloidal stability as well as chemical gradient induced\nrecirculation, thus ensuring colloidal phase equilibrium and low hysteresis in\nspite of the directional drift in presence of electric field forcing. The\nfindings may have potential implications in enhanced and tunable thermal\nmanagement of micro nanoscale devices.\n",
        "method": "Here are the methodological sentences:\n\nA thoroughly designed experimental system has been envisaged to solely probe heat conduction across nanofluids by specifically eliminating the buoyancy driven convective component.\n\nElectric field is applied across the test specimen in order to induce electrophoresis in conjunction with the existing thermal gradient.\n\nA scaling analysis of the thermophoretic and electrophoretic velocities from classical Huckel-Smoluchowski formalism is able to mathematically predict the thermal performance enhancement due to electrophoresis."
    },
    {
        "abstract": "  In this paper, we study stochastic coupon probing problem in social networks.\nAssume there is a social network and a set of coupons. We can offer coupons to\nsome users adaptively and those users who accept the offer will act as seeds\nand influence their friends in the social network. There are two constraints\nwhich are called the inner and outer constraints, respectively. The set of\ncoupons redeemed by users must satisfy inner constraints, and the set of all\nprobed users must satisfy outer constraints. One seeks to develop a coupon\nprobing policy that achieves the maximum influence while satisfying both inner\nand outer constraints. Our main result is a constant approximation policy for\nthe stochastic coupon probing problem for any monotone submodular utility\nfunction.\n",
        "method": "The set of coupons redeemed by users must satisfy inner constraints, and the set of all probed users must satisfy outer constraints."
    },
    {
        "abstract": "  Cellular networks have special characteristics including highly variable\nchannels, fast fluctuating capacities, deep per user buffers, self-inflicted\nqueuing delays, radio uplink/downlink scheduling delays, etc. These\ndistinguishing properties make the problem of achieving low latency and high\nthroughput in cellular networks more challenging than in wired networks. That's\nwhy in this environment, TCP and its flavors, which are generally designed for\nwired networks, perform poorly.\n  To cope with these challenges, we present C2TCP, a flexible end-to-end\nsolution targeting interactive applications requiring high throughput and low\ndelay in cellular networks. C2TCP stands on top of loss-based TCP and brings it\ndelay sensitivity without requiring any network state profiling, channel\nprediction, or complicated rate adjustment mechanisms. The key idea behind\nC2TCP is to absorb dynamics of unpredictable cellular channels by investigating\nlocal minimum delay of packets in a moving time window and react to the\ncellular network's capacity changes very fast.\n  Through extensive trace-based evaluations using traces from five commercial\nLTE and 3G networks, we have compared performance of C2TCP with various TCP\nvariants, and state-of-the-art schemes including BBR, Verus, and Sprout.\nResults show that on average, C2TCP outperforms these schemes and achieves\nlower average and 95th percentile delay for packets.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Through extensive trace-based evaluations using traces from five commercial LTE and 3G networks...\n\nNote: There is only one methodological sentence in this abstract, which describes the evaluation methodology used to compare the performance of C2TCP with other schemes."
    },
    {
        "abstract": "  Quantum Fisher information plays a central role in the field of quantum\nmetrology. In this paper we study the problem of quantum Fisher information of\nunitary processes. Associated to each parameter $\\theta_i$ of unitary process\n$U(\\boldsymbol{\\theta})$, there exists a unique Hermitian matrix\n$M_{\\theta_i}=i(U^\\dagger\\partial_{\\theta_i} U)$. Except for some simple cases,\nsuch as when the parameter under estimation is an overall multiplicative factor\nin the Hamiltonian, calculation of these matrices is not an easy task to treat\neven for estimating a single parameter of qubit systems. Using the Bloch vector\n$\\boldsymbol{m}_{\\theta_i}$, corresponding to each matrix $M_{\\theta_i}$, we\nfind a closed relation for the quantum Fisher information matrix of the $SU(2)$\nprocesses for an arbitrary number of estimation parameters and an arbitrary\ninitial state. We extend our results and present an explicit relation for each\nvector $\\boldsymbol{m}_{\\theta_i}$ for a general Hamiltonian with arbitrary\nparametrization. We illustrate our results by obtaining the quantum Fisher\ninformation matrix of the so-called angle-axis parameters of a general $SU(2)$\nprocess. Using a linear transformation between two different parameter spaces\nof a unitary process, we provide a way to move from quantum Fisher information\nof a unitary process in a given parametrization to the one of the other\nparametrization. Knowing this linear transformation enables one to calculate\nthe quantum Fisher information of a composite unitary process, i.e. a unitary\nprocess resulted from successive action of some simple unitary processes. We\napply this method for a spin-half system and obtain the quantum Fisher matrix\nof the coset parameters in terms of the one of the angle-axis parameters.\n",
        "method": "Using the Bloch vector $\\boldsymbol{m}_{\\theta_i}$, corresponding to each matrix $M_{\\theta_i}$, we find a closed relation for the quantum Fisher information matrix of the SU(2) processes for an arbitrary number of estimation parameters and an arbitrary initial state."
    },
    {
        "abstract": "  We searched for the $\\mu^+\\mu^-$ decay of a light vector gauge boson, also\nknown as dark photon, in the $e^+ e^- \\to \\mu^+ \\mu^- \\gamma_{\\rm ISR}$ process\nby means of the Initial State Radiation (ISR) method. We used 1.93~fb$^{-1}$ of\ndata collected by the KLOE experiment at the DA$\\Phi$NE $\\phi$-factory. No\nstructures have been observed over the irreducible $\\mu^+ \\mu^-$ background. A\n90\\% CL limit on the ratio $\\varepsilon^2=\\alpha^{\\prime}/\\alpha$ between the\ndark coupling constant and the fine structure constant of $ 3\\times\n10^{-6}-2\\times 10^{-7}$ has been set in the dark photon mass region between\n519 MeV and 973 MeV. This new limit has been combined with the published result\nobtained investigating the hypothesis of the dark photon decaying into hadrons\nin $e^+ e^- \\to \\pi^+ \\pi^- \\gamma_{\\rm ISR}$ events. The combined 90\\% CL\nlimit increases the sensitivity especially in the $\\rho-\\omega$ interference\nregion and excludes $\\varepsilon^2$ greater than $(13-2)\\times 10^{-7}$. For\ndark photon masses greater than 600 MeV the combined limit is lower than\n8~$\\times\\, 10^{-7}$ resulting more stringent than present constraints from\nother experiments.\n",
        "method": "We searched for the $\\mu^+\\mu^-$ decay of a light vector gauge boson, also known as dark photon, in the $e^+ e^- \\to \\mu^+ \\mu^- \\gamma_{\\rm ISR}$ process by means of the Initial State Radiation (ISR) method."
    },
    {
        "abstract": "  The article reports the domineering governing role played by the direction of\nelectric and magnetic fields on the internal advection pattern and strength\nwithin salt solution pendant droplets. Literature shows that solutal advection\ndrives circulation cells within salt based droplets. Flow visualization and\nvelocimetry reveals that the direction of the applied field governs the\nenhancement/reduction in circulation velocity and the directionality of\ncirculation inside the droplet. Further, it is noted that while magnetic fields\naugment the circulation velocity, the electric field leads to deterioration of\nthe same. The concepts of electro andmagnetohydrodynamics are appealed to and a\nStokesian stream function based mathematical model to deduce the field mediated\nvelocities has been proposed. The model is found to reveal the roles of and\ndegree of dependence on the governing Hartmann, Stuart, Reynolds and Masuda\nnumbers. The theoretical predictions are observed to be in good agreement with\nexperimental average spatio-temporal velocities. The present findings may have\nstrong implications in microscale electro and/or magnetohydrodynamics.\n",
        "method": "Flow visualization and velocimetry reveals that the direction of the applied field governs the enhancement/reduction in circulation velocity and the directionality of circulation inside the droplet."
    },
    {
        "abstract": "  The central building block of secure and privacy-preserving Vehicular\nCommunication (VC) systems is a Vehicular Public-Key Infrastructure (VPKI),\nwhich provides vehicles with multiple anonymized credentials, termed\npseudonyms. These pseudonyms are used to ensure message authenticity and\nintegrity while preserving vehicle (and thus passenger) privacy. In the light\nof emerging large-scale multi-domain VC environments, the efficiency of the\nVPKI and, more broadly, its scalability are paramount. In this extended\nabstract, we leverage the state-of-the-art VPKI system and enhance its\nfunctionality towards a highly-available and dynamically-scalable design, this\nensures that the system remains operational in the presence of benign failures\nor any resource depletion attack, and that it dynamically scales out, or\npossibly scales in, according to the requests' arrival rate. Our full-blown\nimplementation on the Google Cloud Platform shows that deploying a VPKI for a\nlarge-scale scenario can be cost-effective, while efficiently issuing\npseudonyms for the requesters.\n",
        "method": "We leverage the state-of-the-art VPKI system and enhance its functionality towards a highly-available and dynamically-scalable design."
    },
    {
        "abstract": "  Consider the following class of learning schemes: $$\\hat{\\boldsymbol{\\beta}}\n:= \\arg\\min_{\\boldsymbol{\\beta}}\\;\\sum_{j=1}^n\n\\ell(\\boldsymbol{x}_j^\\top\\boldsymbol{\\beta}; y_j) + \\lambda\nR(\\boldsymbol{\\beta}),\\qquad\\qquad (1) $$ where $\\boldsymbol{x}_i \\in\n\\mathbb{R}^p$ and $y_i \\in \\mathbb{R}$ denote the $i^{\\text{th}}$ feature and\nresponse variable respectively. Let $\\ell$ and $R$ be the loss function and\nregularizer, $\\boldsymbol{\\beta}$ denote the unknown weights, and $\\lambda$ be\na regularization parameter. Finding the optimal choice of $\\lambda$ is a\nchallenging problem in high-dimensional regimes where both $n$ and $p$ are\nlarge. We propose two frameworks to obtain a computationally efficient\napproximation ALO of the leave-one-out cross validation (LOOCV) risk for\nnonsmooth losses and regularizers. Our two frameworks are based on the primal\nand dual formulations of (1). We prove the equivalence of the two approaches\nunder smoothness conditions. This equivalence enables us to justify the\naccuracy of both methods under such conditions. We use our approaches to obtain\na risk estimate for several standard problems, including generalized LASSO,\nnuclear norm regularization, and support vector machines. We empirically\ndemonstrate the effectiveness of our results for non-differentiable cases.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Finding the optimal choice of $\\lambda$ is a challenging problem in high-dimensional regimes where both $n$ and $p$ are large.\n* We propose two frameworks to obtain a computationally efficient approximation ALO of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers.\n* Our two frameworks are based on the primal and dual formulations of (1).\n* We prove the equivalence of the two approaches under smoothness conditions.\n* This equivalence enables us to justify the accuracy of both methods under such conditions."
    },
    {
        "abstract": "  Domination game [SIAM J.\\ Discrete Math.\\ 24 (2010) 979--991] and total\ndomination game [Graphs Combin.\\ 31 (2015) 1453--1462] are by now well\nestablished games played on graphs by two players, named Dominator and Staller.\nIn this paper, Z-domination game, L-domination game, and LL-domination game are\nintroduced as natural companions of the standard domination games.\n  Versions of the Continuation Principle are proved for the new games. It is\nproved that in each of these games the outcome of the game, which is a\ncorresponding graph invariant, differs by at most one depending whether\nDominator or Staller starts the game. The hierarchy of the five domination\ngames is established. The invariants are also bounded with respect to the\n(total) domination number and to the order of a graph. Values of the three new\ninvariants are determined for paths up to a small constant independent from the\nlength of a path. Several open problems and a conjecture are listed. The latter\nasserts that the L-domination game number is not greater than $6/7$ of the\norder of a graph.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Versions of the Continuation Principle are proved for the new games.\""
    },
    {
        "abstract": "  This paper considers the problem of switching between two periodic motions,\nalso known as limit cycles, to create agile running motions. For each limit\ncycle, we use a control Lyapunov function to estimate the region of attraction\nat the apex of the flight phase. We switch controllers at the apex, only if the\ncurrent state of the robot is within the region of attraction of the subsequent\nlimit cycle. If the intersection between two limit cycles is the null set, then\nwe construct additional limit cycles till we are able to achieve sufficient\noverlap of the region of attraction between sequential limit cycles.\nAdditionally, we impose an exponential convergence condition on the control\nLyapunov function that allows us to rapidly transition between limit cycles.\nUsing the approach we demonstrate switching between 5 limit cycles in about 5\nsteps with the speed changing from 2 m/s to 5 m/s.\n",
        "method": "We use a control Lyapunov function to estimate the region of attraction at the apex of the flight phase, and switch controllers at the apex if the current state is within the region of attraction of the subsequent limit cycle."
    },
    {
        "abstract": "  We present a functional renormalization group (fRG) study of the two\ndimensional Hubbard model, performed with an algorithmic implementation which\nlifts some of the common approximations made in fRG calculations. In\nparticular, in our fRG flow; (i) we take explicitly into account the momentum\nand the frequency dependence of the vertex functions; (ii) we include the\nfeedback effect of the self-energy; (iii) we implement the recently introduced\nmultiloop extension which allows us to sum up {\\emph{all}} the diagrams of the\nparquet approximation with their exact weight. Due to its iterative structure\nbased on successive one-loop computations, the loop convergence of the fRG\nresults can be obtained with an affordable numerical effort. In particular,\nfocusing on the analysis of the physical response functions, we show that the\nresults become {\\emph{independent}} from the chosen cutoff scheme and from the\nway the fRG susceptibilities are computed, i.e., either through flowing\ncouplings to external fields, or through a \"post-processing\" contraction of the\ninteraction vertex at the end of the flow. The presented substantial refinement\nof fRG-based computation schemes paves a promising route towards future\nquantitative fRG analyses of more challenging systems and/or parameter regimes.\n",
        "method": "Here is the methodological sentence:\n\n(i) we take explicitly into account the momentum and the frequency dependence of the vertex functions; (ii) we include the feedback effect of the self-energy; (iii) we implement the recently introduced multiloop extension which allows us to sum up all the diagrams of the parquet approximation with their exact weight."
    },
    {
        "abstract": "  The present article experimentally explores the concept of large improving\nthe AC dielectric breakdown strength of insulating mineral oils by the addition\nof trace amounts of graphene or CNTs to form stable dispersions. The nano-oils\ninfused with these nanostructures of high electronic conductance indicate\nsuperior AC dielectric behaviour in terms of augmented breakdown strength\ncompared to the base oils. Experimental observations of two grades of\nsynthesized graphene and CNT nano-oils show that the nanomaterials not only\nimprove the average breakdown voltage but also significantly improve the\nreliability and survival probabilities of the oils under AC high voltage\nstressing. Improvement of the tune of ~ 70-80 % in the AC breakdown voltage of\nthe oils has been obtained via the present concept. The present study examines\nthe reliability of such nano-colloids with the help of two parameter Weibull\ndistribution and the oils show greatly augmented electric field bearing\ncapacity at both standard survival probability values of 5 % and 63.3 %. The\nfundamental mechanism responsible for such observed outcomes is reasoned to be\ndelayed streamer development and reduced streamer growth rates due to effective\nelectron scavenging by the nanostructures from the ionized liquid insulator. A\nmathematical model based on the principles of electron scavenging is proposed\nto quantify the amount of electrons scavenged by the nanostructures. The same\nis then employed to predict the enhanced AC breakdown voltage and the\nexperimental values are found to match well with the model predictions. The\npresent study can have strong implications in efficient, reliable and safer\noperation of real life AC power systems.\n",
        "method": "Experimental observations of two grades of synthesized graphene and CNT nano-oils show that the nanomaterials not only improve the average breakdown voltage but also significantly improve the reliability and survival probabilities of the oils under AC high voltage stressing."
    },
    {
        "abstract": "  Using the X-ray-selected active galactic nuclei (AGN) from the XMM-XXL north\nsurvey and the SDSS Baryon Oscillation Spectroscopic Survey (BOSS)\nspectroscopic follow-up of them, we compare the properties of X-ray unobscured\nand obscured broad-line AGN (BLAGN1 and BLAGN2; $N_\\textrm{H}$below and above\n$10^{21.5}$ cm$^{-2}$), including their X-ray luminosity $L_X$, black hole\nmass, Eddington ratio $\\lambda_{\\textrm{Edd}}$, optical continuum and line\nfeatures. We find that BLAGN2 have systematically larger broad line widths and\nhence apparently higher (lower) $M_{\\textrm{BH}}$ ($\\lambda_{\\textrm{Edd}}$)\nthan BLAGN1. We also find that the X-ray obscuration in BLAGN tends to coincide\nwith optical dust extinction, which is optically thinner than that in\nnarrow-line AGN (NLAGN) and likely partial-covering to the broad line region.\nAll the results can be explained in the framework of a multi-component, clumpy\ntorus model by interpreting BLAGN2 as an intermediate type between BLAGN1 and\nNLAGN in terms of an intermediate inclination angle.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We compare the properties of X-ray unobscured and obscured broad-line AGN (BLAGN1 and BLAGN2; $N_\\textrm{H}$ below and above $10^{21.5}$ cm$^{-2}$), including their X-ray luminosity $L_X$, black hole mass, Eddington ratio $\\lambda_{\\textrm{Edd}}$, optical continuum and line features.\""
    },
    {
        "abstract": "  Automatic multi-class object detection in remote sensing images in\nunconstrained scenarios is of high interest for several applications including\ntraffic monitoring and disaster management. The huge variation in object scale,\norientation, category, and complex backgrounds, as well as the different camera\nsensors pose great challenges for current algorithms. In this work, we propose\na new method consisting of a novel joint image cascade and feature pyramid\nnetwork with multi-size convolution kernels to extract multi-scale strong and\nweak semantic features. These features are fed into rotation-based region\nproposal and region of interest networks to produce object detections. Finally,\nrotational non-maximum suppression is applied to remove redundant detections.\nDuring training, we minimize joint horizontal and oriented bounding box loss\nfunctions, as well as a novel loss that enforces oriented boxes to be\nrectangular. Our method achieves 68.16% mAP on horizontal and 72.45% mAP on\noriented bounding box detection tasks on the challenging DOTA dataset,\noutperforming all published methods by a large margin (+6% and +12% absolute\nimprovement, respectively). Furthermore, it generalizes to two other datasets,\nNWPU VHR-10 and UCAS-AOD, and achieves competitive results with the baselines\neven when trained on DOTA. Our method can be deployed in multi-class object\ndetection applications, regardless of the image and object scales and\norientations, making it a great choice for unconstrained aerial and satellite\nimagery.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We propose a new method consisting of a novel joint image cascade and feature pyramid network with multi-size convolution kernels to extract multi-scale strong and weak semantic features.\""
    },
    {
        "abstract": "  Point source detection at low signal-to-noise is challenging for astronomical\nsurveys, particularly in radio interferometry images where the noise is\ncorrelated. Machine learning is a promising solution, allowing the development\nof algorithms tailored to specific telescope arrays and science cases. We\npresent DeepSource - a deep learning solution - that uses convolutional neural\nnetworks to achieve these goals. DeepSource enhances the Signal-to-Noise Ratio\n(SNR) of the original map and then uses dynamic blob detection to detect\nsources. Trained and tested on two sets of 500 simulated 1 deg x 1 deg MeerKAT\nimages with a total of 300,000 sources, DeepSource is essentially perfect in\nboth purity and completeness down to SNR = 4 and outperforms PyBDSF in all\nmetrics. For uniformly-weighted images it achieves a Purity x Completeness (PC)\nscore at SNR = 3 of 0.73, compared to 0.31 for the best PyBDSF model. For\nnatural-weighting we find a smaller improvement of ~40% in the PC score at SNR\n= 3. If instead we ask where either of the purity or completeness first drop to\n90%, we find that DeepSource reaches this value at SNR = 3.6 compared to the\n4.3 of PyBDSF (natural-weighting). A key advantage of DeepSource is that it can\nlearn to optimally trade off purity and completeness for any science case under\nconsideration. Our results show that deep learning is a promising approach to\npoint source detection in astronomical images.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Machine learning is a promising solution, allowing the development of algorithms tailored to specific telescope arrays and science cases.\n* DeepSource uses convolutional neural networks to achieve these goals.\n* Trained and tested on two sets of 500 simulated 1 deg x 1 deg MeerKAT images with a total of 300,000 sources."
    },
    {
        "abstract": "  We set up a new notion of local convergence for permutations and we prove a\ncharacterization in terms of proportions of \\emph{consecutive} pattern\noccurrences. We also characterize random limiting objects for this new topology\nintroducing a notion of \"shift-invariant\" property (corresponding to the notion\nof unimodularity for random graphs). We then study two models in the framework\nof random pattern-avoiding permutations. We compute the local limits of uniform\n$\\rho$-avoiding permutations, for $|\\rho|=3,$ when the size of the permutations\ntends to infinity. The core part of the argument is the description of the\nasymptotics of the number of consecutive occurrences of any given pattern. For\nthis result we use bijections between $\\rho$-avoiding permutations and rooted\nordered trees, local limit results for Galton--Watson trees, the Second moment\nmethod and singularity analysis.\n",
        "method": "Here is the methodological sentence:\n\nWe compute the local limits of uniform $\\rho$-avoiding permutations, for $|\\rho|=3,$ when the size of the permutations tends to infinity."
    },
    {
        "abstract": "  The Zero divisor Graph of a commutative ring $R$, denoted by $\\Gamma[R]$, is\na graph whose vertices are non-zero zero divisors of $R$ and two vertices are\nadjacent if their product is zero. In this paper we derive the Vertex and Edge\nConnectivity of the zero divisor graph $\\Gamma[\\mathbb{Z}_n]$, for any natural\nnumber $n$ . We also discuss the minimum degree of the zero divisor graph\n$\\Gamma[\\mathbb{Z}_n]$.\n",
        "method": "Here is the methodological sentence:\n\nWe derive the Vertex and Edge Connectivity of the zero divisor graph $\\Gamma[\\mathbb{Z}_n]$, for any natural number $n$."
    },
    {
        "abstract": "  It is generally assumed that oxygen potential in a thin oxide electrolyte\nfollows a linear distribution between electrodes. Jacobsen and Mogensen have\nshown, however, that this is not the case for thin zirconia membranes in solid\noxide electrochemical cells. Here we demonstrate that there is a ubiquitous\noxygen potential transition rooted in the p-type/n-type transition of\nelectronic conductivity inside mixed conducting oxides, and that the transition\nis extremely sensitive to electrode potential and current density. It is also\nremarkably sensitive to the conductivity ratio of electrons and holes, as well\nas their association with lattice oxygens and vacancies, which tends to\nincrease the oxygen flow. Direct evidence of a sharp oxygen potential\ntransition has been found in an equally sharp grain size transition in\nelectrically loaded zirconia. More broadly speaking, the oxygen potential\ntransition is akin to a first-order phase transition. Therefore, it will suffer\ninterface instability, especially in high-current-density devices. These\nfindings provide new opportunities to understand several disparate observations\nin the literature, from microstructural degradation and stress distribution in\nsolid oxide fuel/electrolyzer cells, to field-assisted sintering, to conducting\nfilaments in resistance memory, to dendrite formation in electrochemical cells.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* None (the abstract does not contain explicit methodological sentences)"
    },
    {
        "abstract": "  We verify the maximum conjecture on the rigidity of totally nondegenerate\nmodel CR manifolds in the following two cases: (i) for all models of CR\ndimension one (ii) for the so-called full-models, namely those in which their\nassociated symbol algebras are free CR. In particular, we discover that in each\narbitrary CR dimension and length >= 3, there exists at least one totally\nnondegenerate model, enjoying this conjecture. Our proofs rely upon some recent\nresults in the Tanaka theory of transitive prolongation of fundamental\nalgebras.\n",
        "method": "Here is the methodological sentence:\n\nOur proofs rely upon some recent results in the Tanaka theory of transitive prolongation of fundamental algebras."
    },
    {
        "abstract": "  In spite of progress in securing Vehicular Communication (VC) systems, there\nis no consensus on how to distribute Certificate Revocation Lists (CRLs). The\nmain challenges lie exactly in (i) crafting an efficient and timely\ndistribution of CRLs for numerous anonymous credentials, pseudonyms, (ii)\nmaintaining strong privacy for vehicles prior to revocation events, even with\nhonest-but-curious system entities, (iii) and catering to computation and\ncommunication constraints of on-board units with intermittent connectivity to\nthe infrastructure. Relying on peers to distribute the CRLs is a double-edged\nsword: abusive peers could \"pollute\" the process, thus degrading the timely\nCRLs distribution. In this paper, we propose a vehicle-centric solution that\naddresses all these challenges and thus closes a gap in the literature. Our\nscheme radically reduces CRL distribution overhead: each vehicle receives CRLs\ncorresponding only to its region of operation and its actual trip duration.\nMoreover, a \"fingerprint\" of CRL 'pieces' is attached to a subset of\n(verifiable) pseudonyms for fast CRL 'piece' validation (while mitigating\nresource depletion attacks abusing the CRL distribution). Our experimental\nevaluation shows that our scheme is efficient, scalable, dependable, and\npractical: with no more than 25 KB/s of traffic load, the latest CRL can be\ndelivered to 95% of the vehicles in a region (50x50 KM) within 15s, i.e., more\nthan 40 times faster than the state-of-the-art. Overall, our scheme is a\ncomprehensive solution that complements standards and can catalyze the\ndeployment of secure and privacy-protecting VC systems.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* ...each vehicle receives CRLs corresponding only to its region of operation and its actual trip duration.\n* A \"fingerprint\" of CRL 'pieces' is attached to a subset of (verifiable) pseudonyms for fast CRL 'piece' validation ...\n* Our experimental evaluation shows that our scheme is efficient, scalable, dependable, and practical: with no more than 25 KB/s of traffic load, the latest CRL can be delivered to 95% of the vehicles in a region (50x50 KM) within 15s ..."
    },
    {
        "abstract": "  Weak vector boson scattering (VBS) at the LHC provides an excellent source of\ninformation on the structure of quartic gauge couplings and possible effects of\nphysics beyond the SM in electroweak symmetry breaking. Parameterizing\ndeviations from the SM within an effective field theory at tree level, the\ndimension-8 operators, which are needed for sufficiently general modeling, lead\nto unphysical enhancements of cross sections within the accessible energy range\nof the LHC. Preservation of unitarity limits is needed for phenomenological\nstudies of the $VVjj$ events which signify VBS. Here we develop a numerical\nunitarization scheme for the full off-shell VBS processes and apply it to\nsame-sign $W$ scattering, i.e. processes like $qq\\to qqW^+W^+$. The scheme is\nimplemented within the Monte Carlo program VBFNLO, including leptonic decay of\nthe weak bosons and NLO QCD corrections. Distributions differentiating between\nhigher dimensional operators are discussed.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nParameterizing deviations from the SM within an effective field theory at tree level, ..."
    },
    {
        "abstract": "  We show that if a Riemannian manifold satisfies (3,3)-bipolar comparisons and\nhas an open flat subset then it is flat. The same holds for a version of MTW\nwhere the perpendicularity is dropped.\n  In particular we get that the (3,3)-bipolar comparison is strictly stronger\nthan the Alexandrov comparison.\n",
        "method": "Methodological sentences:\n\n* None (these abstracts do not contain methodological sentences)"
    },
    {
        "abstract": "  HMCF \"Hamiltonian Monte Carlo for Fields\" is a software add-on for the NIFTy\n\"Numerical Information Field Theory\" framework implementing Hamiltonian Monte\nCarlo (HMC) sampling in Python. HMCF as well as NIFTy are designed to address\ninference problems in high-dimensional spatially correlated setups such as\nimage reconstruction. HMCF adds an HMC sampler to NIFTy that automatically\nadjusts the many free parameters steering the HMC sampling machinery. A wide\nvariety of features ensure efficient full-posterior sampling for\nhigh-dimensional inference problems. These features include integration step\nsize adjustment, evaluation of the mass matrix, convergence diagnostics, higher\norder symplectic integration and simultaneous sampling of parameters and\nhyperparameters in Bayesian hierarchical models.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"HMCF adds an HMC sampler to NIFTy that automatically adjusts the many free parameters steering the HMC sampling machinery.\""
    },
    {
        "abstract": "  Music source separation with deep neural networks typically relies only on\namplitude features. In this paper we show that additional phase features can\nimprove the separation performance. Using the theoretical relationship between\nSTFT phase and amplitude, we conjecture that derivatives of the phase are a\ngood feature representation opposed to the raw phase. We verify this conjecture\nexperimentally and propose a new DNN architecture which combines amplitude and\nphase. This joint approach achieves a better signal-to distortion ratio on the\nDSD100 dataset for all instruments compared to a network that uses only\namplitude features. Especially, the bass instrument benefits from the phase\ninformation.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Using the theoretical relationship between STFT phase and amplitude, we conjecture that derivatives of the phase are a good feature representation opposed to the raw phase.\""
    },
    {
        "abstract": "  We construct a continuous time model for price-mediated contagion\nprecipitated by a common exogenous stress to the banking book of all firms in\nthe financial system. In this setting, firms are constrained so as to satisfy a\nrisk-weight based capital ratio requirement. We use this model to find\nanalytical bounds on the risk-weights for assets as a function of the market\nliquidity. Under these appropriate risk-weights, we find existence and\nuniqueness for the joint system of firm behavior and the asset prices. We\nfurther consider an analytical bound on the firm liquidations, which allows us\nto construct exact formulas for stress testing the financial system with\ndeterministic or random stresses. Numerical case studies are provided to\ndemonstrate various implications of this model and analytical bounds.\n",
        "method": "We use this model to find analytical bounds on the risk-weights for assets as a function of the market liquidity."
    },
    {
        "abstract": "  The two-dimensional ($2d$) fully frustrated Planar Rotator model on a square\nlattice has been the subject of a long controversy due to the simultaneous\n$Z_2$ and $O(2)$ symmetry existing in the model. The $O(2)$ symmetry being\nresponsible for the Berezinskii - Kosterlitz - Thouless transition ($BKT$)\nwhile the $Z_2$ drives an Ising-like transition. There are arguments supporting\ntwo possible scenarios, one advocating that the loss of $Ising$ and $BKT$ order\ntake place at the same temperature $T_{t}$ and the other that the $Z_2$\ntransition occurs at a higher temperature than the $BKT$ one. In the first case\nan immediate consequence is that this model is in a new universality class.\nMost of the studies take hand of some order parameter like the stiffness,\nBinder's cumulant or magnetization to obtain the transition temperature.\nConsidering that the transition temperatures are obtained, in general, as an\naverage over the estimates taken about several of those quantities, it is\ndifficult to decide if they are describing the same or slightly separate\ntransitions. In this paper we describe an iterative method based on the\nknowledge of the complex zeros of the energy probability distribution to study\nthe critical behavior of the system. The method is general with advantages over\nmost conventional techniques since it does not need to identify any order\nparameter \\emph{a priori}. The critical temperature and exponents can be\nobtained with good precision. We apply the method to study the Fully Frustrated\nPlanar Rotator ($PR$) and the Anisotropic Heisenberg ($XY$) models in two\ndimensions. We show that both models are in a new universality class with\n$T_{PR}=0.45286(32)$ and $T_{XY}=0.36916(16)$ and the transition exponent\n$\\nu=0.824(30)$ ($\\frac{1}{\\nu}=1.22(4)$).\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe describe an iterative method based on the knowledge of the complex zeros of the energy probability distribution to study the critical behavior of the system."
    },
    {
        "abstract": "  We study the space of orthogonally additive $n$-homogeneous polynomials on\n$C(K)$. There are two natural norms on this space. First, there is the usual\nsupremum norm of uniform convergence on the closed unit ball. As every\northogonally additive $n$-homogeneous polynomial is regular with respect to the\nBanach lattice structure, there is also the regular norm. These norms are\nequivalent, but have significantly different geometric properties. We\ncharacterise the extreme points of the unit ball for both norms, with different\nresults for even and odd degrees. As an application, we prove a Banach-Stone\ntheorem. We conclude with a classification of the exposed points.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\nAs every orthogonally additive $n$-homogeneous polynomial is regular with respect to the Banach lattice structure, there is also the regular norm.\n\nThese norms are equivalent, but have significantly different geometric properties."
    },
    {
        "abstract": "  In this work we demonstrate that a class of some one and two phase free\nboundary problems can be recast as nonlocal parabolic equations on a\nsubmanifold. The canonical examples would be one-phase Hele Shaw flow, as well\nas its two-phase analog. We also treat nonlinear versions of both one and two\nphase problems. In the special class of free boundaries that are graphs over\n$\\mathbb{R}^d$, we give a precise characterization that shows their motion is\nequivalent to that of a solution of a nonlocal (fractional), nonlinear\nparabolic equation for functions on $\\mathbb{R}^d$. Our main observation is\nthat the free boundary condition defines a nonlocal operator having what we\ncall the Global Comparison Property. A consequence of the connection with\nnonlocal parabolic equations is that for free boundary problems arising from\ntranslation invariant elliptic operators in the positive and negative phases,\none obtains, in a uniform treatment for all of the problems (one and two\nphase), a propagation of modulus of continuity for viscosity solutions of the\nfree boundary flow.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe give a precise characterization that shows their motion is equivalent to that of a solution of a nonlocal (fractional), nonlinear parabolic equation for functions on $\\mathbb{R}^d$."
    },
    {
        "abstract": "  We give effective versions of some results on Scott sentences. We show that\nif $\\mathcal{A}$ has a computable $\\Pi_\\alpha$ Scott sentence, then the orbits\nof all tuples are defined by formulas that are computable $\\Sigma_\\beta$ for\nsome $\\beta <\\alpha$. (This is an effective version of a result of\nMontalb\\'{a}n.) We show that if a countable structure $\\mathcal{A}$ has a\ncomputable $\\Sigma_\\alpha$ Scott sentence and one that is computable\n$\\Pi_\\alpha$, then it has one that is computable $d$-$\\Sigma_\\beta$ for some\n$\\beta < \\alpha$. (This is an effective version of a result of A. Miller.) We\nalso give an effective version of a result of D. Miller. Using the\nnon-effective results of Montalb\\'{a}n and A. Miller, we show that a finitely\ngenerated group has a $d$-$\\Sigma_2$ Scott sentence iff the orbit of some (or\nevery) generating tuple is defined by a $\\Pi_1$ formula. Using our effective\nresults, we show that for a computable finitely generated group, there is a\ncomputable $d$-$\\Sigma_2$ Scott sentence iff the orbit of some (every)\ngenerating tuple is defined by a computable $\\Pi_1$ formula.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We show that if $\\mathcal{A}$ has a computable $\\Pi_\\alpha$ Scott sentence, then ...\n* We show that if a countable structure $\\mathcal{A}$ has a computable $\\Sigma_\\alpha$ Scott sentence and one that is computable $\\Pi_\\alpha$, then ...\n* Using the non-effective results of Montalb\\'{a}n and A. Miller, we show that ..."
    },
    {
        "abstract": "  A new low-dimensional parameterization based on principal component analysis\n(PCA) and convolutional neural networks (CNN) is developed to represent complex\ngeological models. The CNN-PCA method is inspired by recent developments in\ncomputer vision using deep learning. CNN-PCA can be viewed as a generalization\nof an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA\nentail post-processing a PCA model to better honor complex geological features.\nIn CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new\nregularization involving a set of metrics for multipoint statistics is\nintroduced. The metrics are based on summary statistics of the nonlinear filter\nresponses of geological models to a pre-trained deep CNN. In addition, in the\nCNN-PCA formulation presented here, a convolutional neural network is trained\nas an explicit transform function that can post-process PCA models quickly.\nCNN-PCA is shown to provide both unconditional and conditional realizations\nthat honor the geological features present in reference SGeMS geostatistical\nrealizations for a binary channelized system. Flow statistics obtained through\nsimulation of random CNN-PCA models closely match results for random SGeMS\nmodels for a demanding case in which O-PCA models lead to significant\ndiscrepancies. Results for history matching are also presented. In this\nassessment CNN-PCA is applied with derivative-free optimization, and a subspace\nrandomized maximum likelihood method is used to provide multiple posterior\nmodels. Data assimilation and significant uncertainty reduction are achieved\nfor existing wells, and physically reasonable predictions are also obtained for\nnew wells. Finally, the CNN-PCA method is extended to a more complex\nnon-stationary bimodal deltaic fan system, and is shown to provide high-quality\nrealizations for this challenging example.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The CNN-PCA method is inspired by recent developments in computer vision using deep learning.\n* Both CNN-PCA and O-PCA entail post-processing a PCA model to better honor complex geological features.\n* In CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new regularization involving a set of metrics for multipoint statistics is introduced.\n* A convolutional neural network is trained as an explicit transform function that can post-process PCA models quickly."
    },
    {
        "abstract": "  Low-dimensional multiferroicity, though highly scarce in nature, has\nattracted great attention due to both fundamental and technological interests.\nUsing first-principles density functional theory, we show that ferromagnetism\nand ferroelectricity can coexist in monolayer transition metal phosphorus\nchalcogenides (TMPCs) - CuMP$_2$X$_6$ (M=Cr, V; X=S, Se). These van der Waals\nlayered materials represent a class of 2D multiferroic semiconductors that\nsimultaneously possess ferroelectric and ferromagnetic orders. In these\nmonolayer materials, Cu atoms spontaneously move away from the center atomic\nplane, giving rise to nontrivial electric dipole moment along the plane normal.\nIn addition, their ferromagnetism originates from indirect exchange interaction\nbetween Cr/V atoms, while their out-of-plane ferroelectricity suggests the\npossibility of controlling electric polarization by external vertical electric\nfield. Monolayer semiconducting TMPCs thus provide a solid-state 2D materials\nplatform for realizing 2D nanoscale switches and memory devices patterned with\ntop and bottom electrodes.\n",
        "method": "Using first-principles density functional theory, we show that ferromagnetism and ferroelectricity can coexist in monolayer transition metal phosphorus chalcogenides (TMPCs) - CuMP$_2$X$_6$ (M=Cr, V; X=S, Se)."
    },
    {
        "abstract": "  This paper proposes a frequency/time hybrid integral-equation method for the\ntime dependent wave equation in two and three-dimensional spatial domains.\nRelying on Fourier Transformation in time, the method utilizes a fixed\n(time-independent) number of frequency-domain integral-equation solutions to\nevaluate, with superalgebraically-small errors, time domain solutions for\narbitrarily long times. The approach relies on two main elements, namely, 1) A\nsmooth time-windowing methodology that enables accurate band-limited\nrepresentations for arbitrarily-long time signals, and 2) A novel Fourier\ntransform approach which, in a time-parallel manner and without causing\nspurious periodicity effects, delivers numerically dispersionless\nspectrally-accurate solutions. A similar hybrid technique can be obtained on\nthe basis of Laplace transforms instead of Fourier transforms, but we do not\nconsider the Laplace-based method in the present contribution. The algorithm\ncan handle dispersive media, it can tackle complex physical structures, it\nenables parallelization in time in a straightforward manner, and it allows for\ntime leaping---that is, solution sampling at any given time $T$ at\n$\\mathcal{O}(1)$-bounded sampling cost, for arbitrarily large values of $T$,\nand without requirement of evaluation of the solution at intermediate times.\nThe proposed frequency-time hybridization strategy, which generalizes to any\nlinear partial differential equation in the time domain for which\nfrequency-domain solutions can be obtained (including e.g. the time-domain\nMaxwell equations), and which is applicable in a wide range of scientific and\nengineering contexts, provides significant advantages over other available\nalternatives such as volumetric discretization, time-domain integral equations,\nand convolution-quadrature approaches.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Relying on Fourier Transformation in time, the method utilizes a fixed (time-independent) number of frequency-domain integral-equation solutions to evaluate, with superalgebraically-small errors, time domain solutions for arbitrarily long times.\n* The approach relies on two main elements, namely, 1) A smooth time-windowing methodology that enables accurate band-limited representations for arbitrarily-long time signals, and 2) A novel Fourier transform approach which, in a time-parallel manner and without causing spurious periodicity effects, delivers numerically dispersionless spectrally-accurate solutions.\n* The algorithm can handle dispersive media, it can tackle complex physical structures, it enables parallelization in time in a straightforward manner, and it allows for time leaping---that is, solution sampling at any given time $T$ at $\\mathcal{O}(1)$-bounded sampling cost, for arbitrarily large values of $T$, and without requirement of evaluation of the solution at intermediate times."
    },
    {
        "abstract": "  We evaluate the power of simple networks side-channels to violate user\nprivacy on Android devices. Specifically, we show that, using blackbox network\nmetadata alone (i.e., traffic statistics such as transmission time and size of\npackets) it is possible to infer several elements of a user's location and also\nidentify their web browsing history (i.e, which sites they visited). We do this\nwith relatively simple learning and classification methods and basic network\nstatistics. For most Android phones currently on the market, such process-level\ntraffic statistics are available for any running process, without any\npermissions control and at fine-grained details, although, as we demonstrate,\neven device-level statistics are sufficient for some of our attacks. In effect,\nit may be possible for any application running on these phones to identify\nprivacy-revealing elements of a user's location, for example, correlating\ntravel with places of worship, point-of-care medical establishments, or\npolitical activity.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We do this with relatively simple learning and classification methods and basic network statistics.\n* ...with any running process, without any permissions control and at fine-grained details..."
    },
    {
        "abstract": "  The present article reports the hitherto unreported phenomenon of arrested\nevaporation dynamics in pendent droplets in an electric field ambience. The\nevaporation kinetics of pendant droplets of electrically conducting saline\nsolutions in the presence of a transverse, alternating electric field is\ninvestigated experimentally. It has been observed that while increase of field\nstrength arrests the evaporation, increment in field frequency has the opposite\neffect. The same has been explained on the solvation kinetics of the ions in\nthe polar water. Theoretical analysis reveals that change in surface tension\nand diffusion driven evaporation model cannot predict the arrested or\ndecelerated evaporation. With the aid of Particle Image Velocimetry,\nsuppression of internal circulation velocity within the droplet is observed\nunder electric field stimulus, and this affects the evaporation rate directly.\nA mathematical scaling model is proposed to quantify the effects of\nelectrohydrodynamic circulation, electrothermal and electro-solutal advection\non the evaporation kinetics of the droplet. The analysis encompasses major\ngoverning parameters, viz. the thermal and solutal Marangoni numbers, the\nElectrohydrodynamic number, the electro Prandtl and electro Schmidt numbers and\ntheir respective contributions. It has been shown that the electrothermal\nMarangoni effect is supressed by the electric field, leading to deteriorated\nevaporation rates. Additionally, the electrosolutal Marangoni effect further\nsupresses the internal advection, which again arrests the evaporation rate by a\nlarger proportion. Stability analysis reveals that the electric body force\nretards the stable internal circulation within such droplets and arrests\nadvection.\n",
        "method": "The evaporation kinetics of pendant droplets of electrically conducting saline solutions in the presence of a transverse, alternating electric field is investigated experimentally."
    },
    {
        "abstract": "  We give an alternative proof of Faltings's theorem (Mordell's conjecture): a\ncurve of genus at least two over a number field has finitely many rational\npoints. Our argument utilizes the set-up of Faltings's original proof, but is\nin spirit closer to the methods of Chabauty and Kim: we replace the use of\nabelian varieties by a more detailed analysis of the variation of $p$-adic\nGalois representations in a family of algebraic varieties. The key inputs into\nthis analysis are the comparison theorems of $p$-adic Hodge theory, and\nexplicit topological computations of monodromy.\n  By the same methods we show that, in sufficiently large dimension and degree,\nthe set of hypersurfaces in projective space, with good reduction away from a\nfixed set of primes, is contained in a proper Zariski-closed subset of the\nmoduli space of all hypersurfaces. This uses in an essential way the\nAx--Schanuel property for period mappings, recently established by Bakker and\nTsimerman.\n",
        "method": "Here are the methodological sentences:\n\n* We replace the use of abelian varieties by a more detailed analysis of the variation of $p$-adic Galois representations in a family of algebraic varieties.\n* The key inputs into this analysis are the comparison theorems of $p$-adic Hodge theory, and explicit topological computations of monodromy."
    },
    {
        "abstract": "  We report soliton rains in a tuneable Tm-doped fiber laser mode locked by\ncarbon nanotubes. We also detect their second- and third-harmonic. We achieve a\ntuneability of over 56nm, from 1877 to 1933nm, by introducing a\npolarization-maintaining isolator and two in-line polarization controllers.\nThis makes our system promising as a tuneable filter for ultrafast\nspectroscopy.\n",
        "method": "We achieve a tuneability of over 56nm, from 1877 to 1933nm, by introducing a polarization-maintaining isolator and two in-line polarization controllers."
    },
    {
        "abstract": "  The sensitivity of millimeter wave (mmWave) signals to blockages is a\nfundamental challenge for mobile mmWave communication systems. The sudden\nblockage of the line-of-sight (LOS) link between the base station and the\nmobile user normally leads to disconnecting the communication session, which\nhighly impacts the system reliability. Further, reconnecting the user to\nanother LOS base station incurs high beam training overhead and critical\nlatency problems. In this paper, we leverage machine learning tools and propose\na novel solution for these reliability and latency challenges in mmWave MIMO\nsystems. In the developed solution, the base stations learn how to predict that\na certain link will experience blockage in the next few time frames using their\npast observations of adopted beamforming vectors. This allows the serving base\nstation to proactively hand-over the user to another base station with a highly\nprobable LOS link. Simulation results show that the developed deep learning\nbased strategy successfully predicts blockage/hand-off in close to 95% of the\ntimes. This reduces the probability of communication session disconnection,\nwhich ensures high reliability and low latency in mobile mmWave systems.\n",
        "method": "In this paper, we leverage machine learning tools and propose a novel solution for these reliability and latency challenges in mmWave MIMO systems."
    },
    {
        "abstract": "  We have started automatized photometric monitoring of active galactic nuclei\nusing the 46 cm telescope of the Wise Observatory in Israel. The telescope is\nspecially equipped with narrow-band filters to perform high-fidelity\nphotometric reverberation mapping of the accretion disk in V < 17 mag sources\nup to z ~ 0.1. Here, we describe the capability and accuracy of the experiment,\nand present the first science verification data obtained for the Seyfert 1\ngalaxy Mrk 279. With sub-diurnal sampling over more than two months, and\ntypical flux measurement uncertainties of $1\\%$, we are able to measure\ninter-band time-delays of up to ~2 days across the optical range.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"With sub-diurnal sampling over more than two months, and typical flux measurement uncertainties of $1\\%$,\""
    },
    {
        "abstract": "  In this paper, we derive a theoretical analysis of an interior penalty\ndiscontinuous Galerkin methods for solving the Cahn-Hilliard-Navier-Stokes\nmodel problem. We prove unconditional unique solvability of the discrete\nsystem, obtain unconditional discrete energy dissipation law, and derive\nstability bounds with a generalized chemical energy density. Convergence of the\nmethod is obtained by proving optimal a priori error estimates. Our analysis of\nthe unique solvability is valid for both symmetric and non-symmetric versions\nof the discontinuous Galerkin formulation.\n",
        "method": "We derive a theoretical analysis of an interior penalty discontinuous Galerkin methods for solving the Cahn-Hilliard-Navier-Stokes model problem."
    },
    {
        "abstract": "  In 1966, Arnold [1] showed that the Lagrangian flow of ideal incompressible\nfluids (described by Euler equations) coincide with the geodesic flow on the\nmanifold of volume preserving diffeomorphisms of the fluid domain. Arnold's\nproof and the subsequent work on this topic rely heavily on the properties of\nLie groups and Lie algebras which remain unfamiliar to most fluid dynamicists.\nIn this note, we provide a simple derivation of Arnold's result which only uses\nthe classical methods of calculus of variations. In particular, we show that\nthe Lagrangian flow maps generated by the solutions of the incompressible Euler\nequations coincide with the stationary curves of an appropriate energy\nfunctional when the extremization is carried out over the set of\nvolume-preserving diffeomorphisms.\n",
        "method": "In this note, we provide a simple derivation of Arnold's result which only uses the classical methods of calculus of variations."
    },
    {
        "abstract": "  A complete study on the fermion masses and flavor mixing is presented in a\nnon-minimal left-right symmetric model (NMLRMS) where the ${\\bf S}_{3}\\otimes\n{\\bf Z}_{2}\\otimes {\\bf Z}^{e}_{2}$ flavor symmetry drives the Yukawa\ncouplings. In the quark sector, the mass matrices possess a kind of the\ngeneralized Fritzsch textures that allow us to fit the CKM mixing matrix in\ngood agreement to the last experimental data. In the lepton sector, on the\nother hand, a soft breaking of the $\\mu\\leftrightarrow \\tau$ symmetry provides\na non zero and non maximal reactor and atmospheric angles, respectively. The\ninverted and degenerate hierarchy are favored in the model where a set of free\nparameters is found to be consistent with the current neutrino data.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The ${\\bf S}_{3}\\otimes {\\bf Z}_{2}\\otimes {\\bf Z}^{e}_{2}$ flavor symmetry drives the Yukawa couplings.\""
    },
    {
        "abstract": "  Motivated by the numerous healthcare applications of molecular communication\nwithin Internet of Bio-Nano Things (IoBNT), this work addresses the problem of\nabnormality detection in a blood vessel using multiple biological embedded\ncomputing devices called cooperative biological nanomachines (CNs), and a\ncommon receiver called the fusion center (FC). Due to blood flow inside a\nvessel, each CN and the FC are assumed to be mobile. In this work, each of the\nCNs perform abnormality detection with certain probabilities of detection and\nfalse alarm by counting the number of molecules received from a source, e.g.,\ninfected tissue. These CNs subsequently report their local decisions to a FC\nover a diffusion-advection blood flow channel using different types of\nmolecules in the presence of inter-symbol interference, multi-source\ninterference, and counting errors. Due to limited computational capability at\nthe FC, OR and AND logic based fusion rules are employed to make the final\ndecision after obtaining each local decision based on the optimal likelihood\nratio test. For the aforementioned system, probabilities of detection and false\nalarm at the FC are derived for OR and AND fusion rules. Finally, simulation\nresults are presented to validate the derived analytical results, which provide\nimportant insights.\n",
        "method": "Due to blood flow inside a vessel, each CN and the FC are assumed to be mobile."
    },
    {
        "abstract": "  [French] We develop new applications of Sklyanin's $K$-matrix formalism to\nthe study of periodic solutions of the sinh-Gordon equation.\n",
        "method": "The methodological sentence is:\n\nWe develop new applications of Sklyanin's $K$-matrix formalism."
    },
    {
        "abstract": "  We present a model-independent bound on $R(J/\\psi) \\! \\equiv \\! \\mathcal{BR}\n(B_c^+ \\rightarrow J/\\psi \\, \\tau^+\\nu_\\tau)/ \\mathcal{BR} (B_c^+ \\rightarrow\nJ/\\psi \\, \\mu^+\\nu_\\mu)$. This bound is constructed by constraining the form\nfactors through a combination of dispersive relations, heavy-quark relations at\nzero-recoil, and the limited existing determinations from lattice QCD. The\nresulting 95\\% confidence-level bound, $0.20\\leq R(J/\\psi)\\leq0.39$, agrees\nwith the recent LHCb result at $1.3 \\, \\sigma$, and rules out some previously\nsuggested model form factors.\n",
        "method": "This bound is constructed by constraining the form factors through a combination of dispersive relations, heavy-quark relations at zero-recoil, and the limited existing determinations from lattice QCD."
    },
    {
        "abstract": "  We study the Ricci flow for the Lorentzian Einstein-Hilbert action. We show\nthat Einstein gravity emerges as a fixed point of the Einstein-Ricci flow\nequations and derive a renormalization group flow in Euclidean signature. By\nconsidering linearizations near the fixed point, the dynamics of the metric\nreveal that curvature deformations flow according to a forward heat equation\nwith the stress-energy tensor acting as a source.\n",
        "method": "Here is the methodological sentence:\n\nWe show that Einstein gravity emerges as a fixed point of the Einstein-Ricci flow equations and derive a renormalization group flow in Euclidean signature."
    },
    {
        "abstract": "  Through first-principles calculations, the phonon-limited transport\nproperties of cubic boron-V compounds (BP, BAs and BSb) are studied. We find\nthat the high optical phonon frequency in these compounds leads to the\nsubstantial suppression of polar scattering and the reduction of inter-valley\ntransition mediated by large-wavevector optical phonons, both of which\nsignificantly facilitate charge transport. We also discover that BAs\nsimultaneously has a high hole mobility (2110 cm2/V-s) and electron mobility\n(1400 cm2/V-s) at room temperature, which is rare in semiconductors. Our\nfindings present a new insight in searching high mobility polar semiconductors,\nand point to BAs as a promising material for electronic and photovoltaic\ndevices in addition to its predicted high thermal conductivity.\n",
        "method": "Through first-principles calculations, the phonon-limited transport properties of cubic boron-V compounds (BP, BAs and BSb) are studied."
    },
    {
        "abstract": "  Energy transfer processes from a high-intensity ultrashort laser pulse to\nelectrons in simple dielectrics, silicon, diamond, and $\\alpha$-quartz are\ntheoretically investigated by first-principles calculations based on\ntime-dependent density functional theory (TDDFT). Dependences on frequency as\nwell as intensity of the laser pulse are examined in detail, making a\ncomparison with the Keldysh theory. Although the Keldysh theory reliably\nreproduces the main features of the TDDFT calculation, we find some deviations\nbetween results by the two theories. The origin of the differences is examined\nin detail.\n",
        "method": "Methodologically, the energy transfer processes were theoretically investigated by first-principles calculations based on time-dependent density functional theory (TDDFT)."
    },
    {
        "abstract": "  We classify multiply transitive homogeneous real (2,3,5) distributions up to\nlocal diffeomorphism equivalence.\n",
        "method": "We classify multiply transitive homogeneous real (2,3,5) distributions up to local diffeomorphism equivalence."
    },
    {
        "abstract": "  We propose a coalgebraic model for constructing and reasoning about\nstate-based protocols that implement efficient reductions among random\nprocesses. We provide basic tools that allow efficient protocols to be\nconstructed in a compositional way and analyzed in terms of the tradeoff\nbetween state and loss of entropy. We show how to use these tools to construct\nvarious entropy-conserving reductions between processes.\n",
        "method": "Here is the methodological sentence:\n\nWe provide basic tools that allow efficient protocols to be constructed in a compositional way and analyzed in terms of the tradeoff between state and loss of entropy."
    },
    {
        "abstract": "  We adapt a manifold sampling algorithm for the nonsmooth, nonconvex\nformulations of learning that arise when imposing robustness to outliers\npresent in the training data. We demonstrate the approach on objectives based\non trimmed loss. Empirical results show that the method has favorable scaling\nproperties. Although savings in time come at the expense of not certifying\noptimality, the algorithm consistently returns high-quality solutions on the\ntrimmed linear regression and multiclass classification problems tested.\n",
        "method": "We adapt a manifold sampling algorithm for the nonsmooth, nonconvex formulations of learning that arise when imposing robustness to outliers present in the training data."
    },
    {
        "abstract": "  The bootstrap, introduced by Efron (1982), has become a very popular method\nfor estimating variances and constructing confidence intervals. A key insight\nis that one can approximate the properties of estimators by using the empirical\ndistribution function of the sample as an approximation for the true\ndistribution function. This approach views the uncertainty in the estimator as\ncoming exclusively from sampling uncertainty. We argue that for causal\nestimands the uncertainty arises entirely, or partially, from a different\nsource, corresponding to the stochastic nature of the treatment received. We\ndevelop a bootstrap procedure that accounts for this uncertainty, and compare\nits properties to that of the classical bootstrap.\n",
        "method": "A key insight is that one can approximate the properties of estimators by using the empirical distribution function of the sample as an approximation for the true distribution function."
    },
    {
        "abstract": "  We prove necessary and sufficient conditions for the existence of homogeneous\nprime elements in normal N-graded rings of dimension two, in terms of rational\ncoefficient Weil divisors on projective curves.\n",
        "method": "The methodological sentence is:\n\nWe prove necessary and sufficient conditions..."
    },
    {
        "abstract": "  Synaptic connectivity detection is a critical task for neural reconstruction\nfrom Electron Microscopy (EM) data. Most of the existing algorithms for synapse\ndetection do not identify the cleft location and direction of connectivity\nsimultaneously. The few methods that computes direction along with contact\nlocation have only been demonstrated to work on either dyadic (most common in\nvertebrate brain) or polyadic (found in fruit fly brain) synapses, but not on\nboth types. In this paper, we present an algorithm to automatically predict the\nlocation as well as the direction of both dyadic and polyadic synapses. The\nproposed algorithm first generates candidate synaptic connections from\nvoxelwise predictions of signed proximity generated by a 3D U-net. A second 3D\nCNN then prunes the set of candidates to produce the final detection of cleft\nand connectivity orientation. Experimental results demonstrate that the\nproposed method outperforms the existing methods for determining synapses in\nboth rodent and fruit fly brain.\n",
        "method": "The few methods that compute direction along with contact location have only been demonstrated to work on either dyadic (found in vertebrate brain) or polyadic (found in fruit fly brain) synapses, but not on both types."
    },
    {
        "abstract": "  High quality upsampling of sparse 3D point clouds is critically useful for a\nwide range of geometric operations such as reconstruction, rendering, meshing,\nand analysis. In this paper, we propose a data-driven algorithm that enables an\nupsampling of 3D point clouds without the need for hard-coded rules. Our\napproach uses a deep network with Chamfer distance as the loss function,\ncapable of learning the latent features in point clouds belonging to different\nobject categories. We evaluate our algorithm across different amplification\nfactors, with upsampling learned and performed on objects belonging to the same\ncategory as well as different categories. We also explore the desirable\ncharacteristics of input point clouds as a function of the distribution of the\npoint samples. Finally, we demonstrate the performance of our algorithm in\nsingle-category training versus multi-category training scenarios. The final\nproposed model is compared against a baseline, optimization-based upsampling\nmethod. Results indicate that our algorithm is capable of generating more\nuniform and accurate upsamplings.\n",
        "method": "Our approach uses a deep network with Chamfer distance as the loss function, capable of learning the latent features in point clouds belonging to different object categories."
    },
    {
        "abstract": "  A convex code is a binary code generated by the pattern of intersections of a\ncollection of open convex sets in some Euclidean space. Convex codes are\nrelevant to neuroscience as they arise from the activity of neurons that have\nconvex receptive fields. In this paper, we use algebraic methods to determine\nif a code is convex. Specifically, we use the neural ideal of a code, which is\na generalization of the Stanley-Reisner ideal. Using the neural ideal together\nwith its standard generating set, the canonical form, we provide algebraic\nsignatures of certain families of codes that are non-convex. We connect these\nsignatures to the precise conditions on the arrangement of sets that prevent\nthe codes from being convex. Finally, we also provide algebraic signatures for\nsome families of codes that are convex, including the class of\nintersection-complete codes. These results allow us to detect convexity and\nnon-convexity in a variety of situations, and point to some interesting open\nquestions.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe use algebraic methods to determine if a code is convex, specifically using the neural ideal of a code, which is a generalization of the Stanley-Reisner ideal."
    },
    {
        "abstract": "  An irreducible, algebraic curve $\\mathcal X_g$ of genus $g\\geq 2$ defined\nover an algebraically closed field $k$ of characteristic $\\mbox{char } \\, k = p\n\\geq 0$, has finite automorphism group $\\mbox{Aut} (\\mathcal X_g)$. In this\npaper we describe methods of determining the list of groups $\\mbox{Aut}\n(\\mathcal X_g)$ for a fixed $g\\geq 2$. Moreover, equations of the corresponding\nfamilies of curves are given when possible.\n",
        "method": "Here is the methodological sentence:\n\nMethods of determining the list of groups Aut(Xg) for a fixed g\u22652."
    },
    {
        "abstract": "  We use Gemini Multi-Object Spectrograph (GMOS) Integral Field Unit (IFU)\nobservations of the inner 285$\\times$400 pc$^2$ region of the Seyfert 2 galaxy\nNGC 5643 to map the [SIII]$\\lambda9069$ emission-line flux distribution and\nkinematics, as well as the stellar kinematics, derived by fitting the\nCaII$\\lambda\\lambda\\lambda$8498,8542,8662 triplet, at a spatial resolution of\n45 pc. The stellar velocity field shows regular rotation, with a projected\nvelocity of 100 km/s and kinematic major axis along Position Angle\n$PA=-36^\\circ$. A ring of low stellar velocity dispersion values ($\\sim$70\nkm/s), attributed to young/intermediate age stellar populations, is seen\nsurrounding the nucleus with radius of 50 pc. We found that the [SIII] flux\ndistribution shows an elongated structure along the east-west direction and its\nkinematics is dominated by outflows within a bi-cone at an ionized gas outflow\nrate of 0.3 M$_\\odot$ yr$^{-1}$. In addition, velocity slices across the\n[SIII]$\\lambda9069$ emission-line reveal a kinematic component attributed to\nrotation of gas in the plane of the galaxy.\n",
        "method": "We use Gemini Multi-Object Spectrograph (GMOS) Integral Field Unit (IFU) observations of the inner 285\u00d7400 pc2 region of the Seyfert 2 galaxy NGC 5643 to map the [SIII]\u03bb9069 emission-line flux distribution and kinematics, as well as the stellar kinematics, derived by fitting the CaII\u03bb\u03bb8498,8542,8662 triplet, at a spatial resolution of 45 pc."
    },
    {
        "abstract": "  Eisenstein polynomials, which were defined by Oura, are analogues of the\nconcept of an Eisenstein series. Oura conjectured that there exist some\nanalogous properties between Eisenstein series and Eisenstein polynomials. In\nthis paper, we provide new analogous properties of Eisenstein polynomials and\nzeta polynomials. These properties are finite analogies of certain properties\nof Eisenstein series.\n",
        "method": "These results provide a finite analogue of the fact that Eisenstein series satisfy the functional equation."
    },
    {
        "abstract": "  What makes some types of languages more probable than others? For instance,\nwe know that almost all spoken languages contain the vowel phoneme /i/; why\nshould that be? The field of linguistic typology seeks to answer these\nquestions and, thereby, divine the mechanisms that underlie human language. In\nour work, we tackle the problem of vowel system typology, i.e., we propose a\ngenerative probability model of which vowels a language contains. In contrast\nto previous work, we work directly with the acoustic information -- the first\ntwo formant values -- rather than modeling discrete sets of phonemic symbols\n(IPA). We develop a novel generative probability model and report results based\non a corpus of 233 languages.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We work directly with the acoustic information -- the first two formant values -- rather than modeling discrete sets of phonemic symbols (IPA).\""
    },
    {
        "abstract": "  We present a new evaluation of the far-forward neutrino plus antineutrino\nflux and number of events from charm hadron decays in a 400 GeV proton beam\ndump experiment like the Search for Hidden Particles (SHiP). Using\nnext-to-leading order perturbative QCD and a model for intrinsic charm, we\ninclude intrinsic transverse momentum effects and other kinematic angular\ncorrections. We compare this flux to a far-forward flux evaluated with\nnext-to-leading order perturbative QCD, without intrinsic transverse momentum,\nthat used the angular distribution of charm quarks rather than the neutrinos\nfrom their decays. The tau neutrino plus antineutrino number of events in the\nperturbative QCD evaluation is reduced by a factor of about three when\nintrinsic transverse momentum and the full decay kinematics are included. We\nshow that intrinsic charm contributions can significantly enhance the number of\nevents from neutrinos from charm hadron decays. Measurements of the number of\nevents from tau neutrino plus antineutrino interactions and of the muon charge\nasymmetry as a function of energy can be used to constrain intrinsic charm\nmodels.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nUsing next-to-leading order perturbative QCD and a model for intrinsic charm, we include intrinsic transverse momentum effects and other kinematic angular corrections."
    },
    {
        "abstract": "  We quantify the linguistic complexity of different languages' morphological\nsystems. We verify that there is an empirical trade-off between paradigm size\nand irregularity: a language's inflectional paradigms may be either large in\nsize or highly irregular, but never both. Our methodology measures paradigm\nirregularity as the entropy of the surface realization of a paradigm -- how\nhard it is to jointly predict all the surface forms of a paradigm. We estimate\nthis by a variational approximation. Our measurements are taken on large\nmorphological paradigms from 31 typologically diverse languages.\n",
        "method": "Here is the methodological sentence:\n\nOur methodology measures paradigm irregularity as the entropy of the surface realization of a paradigm -- how hard it is to jointly predict all the surface forms of a paradigm."
    },
    {
        "abstract": "  Since the amount of information on the internet is growing rapidly, it is not\neasy for a user to find relevant information for his/her query. To tackle this\nissue, much attention has been paid to Automatic Document Summarization. The\nkey point in any successful document summarizer is a good document\nrepresentation. The traditional approaches based on word overlapping mostly\nfail to produce that kind of representation. Word embedding, distributed\nrepresentation of words, has shown an excellent performance that allows words\nto match on semantic level. Naively concatenating word embeddings makes the\ncommon word dominant which in turn diminish the representation quality. In this\npaper, we employ word embeddings to improve the weighting schemes for\ncalculating the input matrix of Latent Semantic Analysis method. Two\nembedding-based weighting schemes are proposed and then combined to calculate\nthe values of this matrix. The new weighting schemes are modified versions of\nthe augment weight and the entropy frequency. The new schemes combine the\nstrength of the traditional weighting schemes and word embedding. The proposed\napproach is experimentally evaluated on three well-known English datasets, DUC\n2002, DUC 2004 and Multilingual 2015 Single-document Summarization for English.\nThe proposed model performs comprehensively better compared to the\nstate-of-the-art methods, by at least 1% ROUGE points, leading to a conclusion\nthat it provides a better document representation and a better document summary\nas a result.\n",
        "method": "Two embedding-based weighting schemes are proposed and then combined to calculate the values of this matrix."
    },
    {
        "abstract": "  To characterize the meteoroid environment around Mercury and its contribution\nto the planet's exosphere, we combined four distinctive sources of meteoroids\nin the solar system: main-belt asteroids, Jupiter family comets, Halley-type\ncomets, and Oort Cloud comets. All meteoroid populations are described by\ncurrently available dynamical models. We used a recent calibration of the\nmeteoroid influx onto Earth as a constraint for the combined population model\non Mercury. We predict vastly different distributions of orbital elements,\nimpact velocities and directions of arrival for all four meteoroid populations\nat Mercury. We demonstrate that the most likely model of Mercury's meteoroid\nenvironment- in the sense of agreement with Earth -provides good agreement with\npreviously reported observations of Mercury's exosphere by the MESSENGER\nspacecraft and is not highly sensitive to variations of uncertain parameters\nsuch as the ratio of these populations at Earth, the size frequency\ndistribution, and the collisional lifetime of meteoroids. Finally, we provide a\nfully calibrated model consisting of high-resolution maps of mass influx and\nsurface vaporization rates for different values of Mercury's true anomaly\nangle.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We combined four distinctive sources of meteoroids in the solar system: main-belt asteroids, Jupiter family comets, Halley-type comets, and Oort Cloud comets.\""
    },
    {
        "abstract": "  We investigate a hybrid quantum system where an ensemble of nitrogen-vacancy\n(NV) centers in diamond is interfaced with a piezomagnetic superlattice that\nsupports surface phonon polaritons (SPhPs). We show that the strong magnetic\ncoupling between the collective spin waves in the NV spin ensemble and the\nquantized SPhPs can be realized, thanks to the subwavelength nature of the\nSPhPs and relatively long spin coherence times. The magnon-polariton coupling\nallows different modes of the SPhPs to be mapped and orthogonally stored in\ndifferent spatial modes of excitation in the solid medium. Because of its easy\nimplementation and high tunability, the proposed hybrid structure with NV spins\nand piezoactive superlattices could be used for quantum memory and quantum\ncomputation.\n",
        "method": "We show that the strong magnetic coupling between the collective spin waves in the NV spin ensemble and the quantized SPhPs can be realized, thanks to the subwavelength nature of the SPhPs and relatively long spin coherence times."
    },
    {
        "abstract": "  We employ simulation based approach for enhancing the efficiency of Cu2ZnSnS4\n(CZTS) based solar cells. Initial benchmarking of simulation with the\nexperimentally reported solar cell in literature is performed by incorporating\na suitable defect model. We then explore the effects of: (a) conduction band\noffset (CBO) at CZTS/CdS junction, (b) back surface field (BSF) due to an\nadditional layer with higher carrier density, and (c) high work function back\ncontact. Efficiency is observed to improve by about 70% upon optimizing the\nabove three parameters. We also observe that utilizing BSF in the configuration\ncan reduce the high work function requirement of the back contact. A work\nfunction of 5.2 eV (e.g., using Ni), a BSF layer (e.g., using SnS), and a CBO\nof 0.1 eV (e.g., using ZnS) constitute an optimal configuration.\n",
        "method": "We employ simulation based approach for enhancing the efficiency of Cu2ZnSnS4 (CZTS) based solar cells."
    },
    {
        "abstract": "  The detection of multiple curved lane markings on a non-flat road surface is\nstill a challenging task for automotive applications. To make an improvement,\nthe depth information can be used to greatly enhance the robustness of the lane\ndetection systems. The proposed system in this paper is developed from our\nprevious work where the dense vanishing point Vp is estimated globally to\nassist the detection of multiple curved lane markings. However, the outliers in\nthe optimal solution may severely affect the accuracy of the least squares\nfitting when estimating Vp. Therefore, in this paper we use Random Sample\nConsensus to update the inliers and outliers iteratively until the fraction of\nthe number of inliers versus the total number exceeds our pre-set threshold.\nThis significantly helps the system to overcome some suddenly changing\nconditions. Furthermore, we propose a novel lane position validation approach\nwhich provides a piecewise weight based on Vp and the gradient to reduce the\ngradient magnitude of the non-lane candidates. Then, we compute the energy of\neach possible solution and select all satisfying lane positions for\nvisualisation. The proposed system is implemented on a heterogeneous system\nwhich consists of an Intel Core i7-4720HQ CPU and a NVIDIA GTX 970M GPU. A\nprocessing speed of 143 fps has been achieved, which is over 38 times faster\nthan our previous work. Also, in order to evaluate the detection precision, we\ntested 2495 frames with 5361 lanes from the KITTI database (1637 lanes more\nthan our previous experiment). It is shown that the overall successful\ndetection rate is improved from 98.7% to 99.5%.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Therefore, in this paper we use Random Sample Consensus to update the inliers and outliers iteratively until the fraction of the number of inliers versus the total number exceeds our pre-set threshold.\n* Then, we propose a novel lane position validation approach which provides a piecewise weight based on Vp and the gradient to reduce the gradient magnitude of the non-lane candidates.\n* We compute the energy of each possible solution and select all satisfying lane positions for visualisation."
    },
    {
        "abstract": "  In this paper we study weighted versions of Fourier algebras of compact\nquantum groups. We focus on the spectral aspects of these Banach algebras in\ntwo different ways. We first investigate their Gelfand spectrum, which shows a\nconnection to the maximal classical closed subgroup and its complexification.\nSecondly, we study specific finite dimensional representations coming from the\ncomplexification of the underlying quantum group. We demonstrate that the\nweighted Fourier algebras can detect the complexification structure in the\nspecial case of $SU_q(2)$, whose complexification is the quantum Lorentz group\n$SL_q(2,\\mathbb{C})$.\n",
        "method": "We first investigate their Gelfand spectrum, which shows a connection to the maximal classical closed subgroup and its complexification."
    },
    {
        "abstract": "  The paper proposes a novel nature-inspired technique of optimization. It\nmimics the perching nature of eagles and uses mathematical formulations to\nintroduce a new addition to metaheuristic algorithms. The nature of the\nproposed algorithm is based on exploration and exploitation. The proposed\nalgorithm is developed into two versions with some modifications. In the first\nphase, it undergoes a rigorous analysis to find out their performance. In the\nsecond phase it is benchmarked using ten functions of two categories; uni-modal\nfunctions and multi-modal functions. In the third phase, we conducted a\ndetailed analysis of the algorithm by exploiting its controlling units or\nvariables. In the fourth and last phase, we consider real world optimization\nproblems with constraints. Both versions of the algorithm show an appreciable\nperformance, but analysis puts more weight to the modified version. The\ncompetitive analysis shows that the proposed algorithm outperforms the other\ntested metaheuristic algorithms. The proposed method has better robustness and\ncomputational efficiency.\n",
        "method": "In this phase, it is benchmarked using ten functions of two categories; uni-modal functions and multi-modal functions."
    },
    {
        "abstract": "  Background: The n-back and Paced Auditory Serial Addition Test (PASAT) are\ncommonly used verbal working memory tasks that have partially overlapping uses\nin clinical and experimental psychology. We performed three activation\nlikelihood estimation (ALE) meta-analyses, comparing two load levels of the\nn-back task (2-back, 3-back) to the PASAT and to each-other. These analyses\naimed to determine the involvement of cognitive and emotional brain regions for\nthese tasks. Results: We observed higher overall likelihood of activation the\nfrontal eye fields in the 3-back. The PASAT exhibited higher overall activation\nin the bilateral supplementary motor areas (SMA), left supramarginal gyrus, and\nleft superior parietal lobule. Furthermore, the 3-back exhibited higher\nactivation in the right SMA, and anterior mid-cingulate cortex versus the\n2-back, and the PASAT exhibited higher activation in a cluster near the right\npremotor area versus the 2-back. A laterality effect was observed in the\ndorsolateral prefrontal cortex between the PASAT (left) and 3-back(right).\nThese data suggest greater activation of regions traditionally associated with\nthe phonological loop during the PASAT, compared to the 2- and 3-back tasks.\nFurthermore, individual ALE analyses suggest involvement of emotional\nprocessing and salience network regions (insula, cingulate) in addition to the\nwell-established verbal working memory regions (Broca's region, bilateral SMA,\npremotor, posterior parietal cortices) in all 3 tasks. Conclusions: Here we\nidentify regions activated by the PASAT, which has not been meta-analytically\nreviewed prior to this study. Using ALE meta-analysis, we have also identified\nmeaningful differences in activation associated with specific cognitive and\nemotional aspects of verbal working memory during these tasks.\n",
        "method": "We performed three activation likelihood estimation (ALE) meta-analyses, comparing two load levels of the n-back task (2- back, 3-back) to the PASAT and to each-other."
    },
    {
        "abstract": "  In this paper, we are concerned with the asymptotic behavior of the\nNeumann-Poincare operator for Helmholtz system. By analyzing the asymptotic\nbehavior of spherical Bessel function near the origin and/or approach higher\norder, we prove the asymptotic behavior of spectral of Neumann-Poincare\noperator when frequency is small enough and/or the order is large enough. The\nresults show that spectral of Neumann-Poincare operator is continuous at the\norigin and converges to zero from the complex plane in general.\n",
        "method": "By analyzing the asymptotic behavior of spherical Bessel function near the origin and/or approaching higher order, we prove the asymptotic behavior of spectral of Neumann-Poincare operator when frequency is small enough and/or the order is large enough."
    },
    {
        "abstract": "  In many optical metrology techniques, fringe pattern analysis is the central\nalgorithm for recovering the underlying phase distribution from the recorded\nfringe patterns. Despite extensive research efforts for decades, how to extract\nthe desired phase information, with the highest possible accuracy, from the\nminimum number of fringe patterns remains one of the most challenging open\nproblems. Inspired by recent successes of deep learning techniques for computer\nvision and other applications, here, we demonstrate for the first time, to our\nknowledge, that the deep neural networks can be trained to perform fringe\nanalysis, which substantially enhances the accuracy of phase demodulation from\na single fringe pattern. The effectiveness of the proposed method is\nexperimentally verified using carrier fringe patterns under the scenario of\nfringe projection profilometry. Experimental results demonstrate its superior\nperformance in terms of high accuracy and edge-preserving over two\nrepresentative single-frame techniques: Fourier transform profilometry and\nWindowed Fourier profilometry.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We demonstrate for the first time, to our knowledge, that deep neural networks can be trained to perform fringe analysis...\""
    },
    {
        "abstract": "  Convolutional neural network (CNN) depth is of crucial importance for image\nsuper-resolution (SR). However, we observe that deeper networks for image SR\nare more difficult to train. The low-resolution inputs and features contain\nabundant low-frequency information, which is treated equally across channels,\nhence hindering the representational ability of CNNs. To solve these problems,\nwe propose the very deep residual channel attention networks (RCAN).\nSpecifically, we propose a residual in residual (RIR) structure to form very\ndeep network, which consists of several residual groups with long skip\nconnections. Each residual group contains some residual blocks with short skip\nconnections. Meanwhile, RIR allows abundant low-frequency information to be\nbypassed through multiple skip connections, making the main network focus on\nlearning high-frequency information. Furthermore, we propose a channel\nattention mechanism to adaptively rescale channel-wise features by considering\ninterdependencies among channels. Extensive experiments show that our RCAN\nachieves better accuracy and visual improvements against state-of-the-art\nmethods.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe propose a residual in residual (RIR) structure to form very deep network, which consists of several residual groups with long skip connections."
    },
    {
        "abstract": "  It has recently been proven that the invariance of observables with respect\nto angle dependent phase rotations of reaction amplitudes mixes multipoles\nchanging also their relative strength [1]. All contemporary partial wave\nanalyses (PWA) in $\\eta$ photoproduction on protons, either energy dependent\n(ED) [2-5] or single energy (SE) [6] do not take this effect into\nconsideration. It is commonly accepted that there exist quite some similarity\nin the $E0+$ multipole for all PWA, but notable differences in this, but also\nin remaining partial waves still remain. In this paper we demonstrate that once\nthis phase rotations are properly taken into account, all contemporary ED and\nSE partial wave analysis become almost identical for the dominant $E0+$\nmultipole, and the agreement among all other multipoles becomes much better. We\nalso show that the the measured observables are almost equally well reproduced\nfor all PWA, and the remaining differences among multipoles can be attributed\nsolely to the difference of predictions for unmeasured observables. So, new\nmeasurements are needed.\n",
        "method": "Here is the methodological sentence:\n\nWe demonstrate that once this phase rotations are properly taken into account..."
    },
    {
        "abstract": "  Engineering an array of precisely located cavity-coupled active media poses a\nmajor experimental challenge in the field of hybrid integrated photonics. We\ndeterministically position solution processed colloidal quantum dots (QDs) on\nhigh quality-factor silicon nitride nanobeam cavities and demonstrate\nlight-matter coupling. By lithographically defining a window on top of an\nencapsulated cavity that is cladded in a polymer resist, and spin coating QD\nsolution, we can precisely control the placement of the QDs, which subsequently\ncouple to the cavity. We show that the number of QDs coupled to the cavity can\nbe controlled by the size of the window. Furthermore, we demonstrate Purcell\nenhancement and saturable photoluminescence in this QD-cavity platform.\nFinally, we deterministically position QDs on a photonic molecule and observe\nQD-coupled cavity super-modes. Our results pave the way for controlling the\nnumber of QDs coupled to a cavity by engineering the window size, and the QD\ndimension, and will allow advanced studies in cavity enhanced single photon\nemission, ultralow power nonlinear optics, and quantum many-body simulations\nwith interacting photons.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* By lithographically defining a window on top of an encapsulated cavity that is cladded in a polymer resist, and spin coating QD solution, we can precisely control the placement of the QDs, which subsequently couple to the cavity.\n* We show that the number of QDs coupled to the cavity can be controlled by the size of the window."
    },
    {
        "abstract": "  The dynamics of supercritical fluids, a state of matter beyond the gas-liquid\ncritical point, changes from diffusive to oscillatory motions at high pressure.\nThis transition is believed to occur across a locus of thermodynamic states\ncalled the Frenkel line. The Frenkel line has been extensively investigated\nfrom the viewpoint of the dynamics, but its structural meaning is not still\nwell understood. This letter interprets the mesoscopic picture of the Frenkel\nline entirely based on a topological and geometrical framework. This discovery\nmakes it possible to understand the mechanism of rigid/non-rigid transition\nbased not on the dynamics of individual atoms, but on their instantaneous\nconfigurations. The topological classification method reveals that the\npercolation of solid-like structures occurs above the rigid-nonrigid crossover\ndensities.\n",
        "method": "The methodological sentences are:\n\nThis letter interprets the mesoscopic picture of the Frenkel line entirely based on a topological and geometrical framework.\nThe topological classification method reveals that the percolation of solid-like structures occurs above the rigid-nonrigid crossover densities."
    },
    {
        "abstract": "  Banaszek, W\\'odkiewicz and others\n(\\cite{Banaszek},\\cite{Chen},\\cite{Chen-Zhang}) made the surprising discovery\nthat Einstein-Bell locality inequalities can be violated by the two mode\nsqueezed vacuum by a factor $\\sqrt{2}$, in spite of the fact that the state has\na positive Wigner function. I use here the more general Gleason-Kochen-Specker\nassumption of non-contextuality \\cite{Gleason} to express classicality. I then\nderive non-contextuality Bell inequalities for correlations of $N$ pseudo spins\nembedded in an infinite dimensional continuous variable Hilbert space, and show\nthat their maximum possible quantum violation is by a factor $2^{(N-1)/2}$. I\nfind quantum states for which this maximum violation is reached. I also show\nthat the familiar displaced squeezed vacuum for a single optical mode, which\nhas a positive Wigner function, can violate the inequality by a factor $0.842\n(\\sqrt{2})^{N-1} $ for odd $N \\geq 3$ . The arbitrarily large non-classicality\nmeans that realizations of the pseudo-spin measurements even in a single mode\nphoton state might afford similar opportunities in quantum information tasks as\nentangled $N$ qubit systems with large $N$.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"I derive non-contextuality Bell inequalities for correlations of $N$ pseudo spins embedded in an infinite dimensional continuous variable Hilbert space...\""
    },
    {
        "abstract": "  Studying how diverse human populations are related is of historical and\nanthropological interest, in addition to providing a realistic null model for\ntesting for signatures of natural selection or disease associations.\nFurthermore, understanding the demographic histories of other species is\nplaying an increasingly important role in conservation genetics. A number of\nstatistical methods have been developed to infer population demographic\nhistories using whole-genome sequence data, with recent advances focusing on\nallowing for more flexible modeling choices, scaling to larger data sets, and\nincreasing statistical power. Here we review coalescent hidden Markov models, a\npowerful class of population genetic inference methods that can effectively\nutilize linkage disequilibrium information. We highlight recent advances, give\nadvice for practitioners, point out potential pitfalls, and present possible\nfuture research directions.\n",
        "method": "No methodological sentences were found in this abstract. The text primarily discusses the importance and applications of studying population demographic histories, as well as reviewing a specific class of methods (coalescent hidden Markov models) used for population genetic inference. There are no sentences describing experimental setup, data collection, analysis techniques, or procedures followed."
    },
    {
        "abstract": "  A distributed binary hypothesis testing (HT) problem involving two parties, a\nremote observer and a detector, is studied. The remote observer has access to a\ndiscrete memoryless source, and communicates its observations to the detector\nvia a rate-limited noiseless channel. The detector observes another discrete\nmemoryless source, and performs a binary hypothesis test on the joint\ndistribution of its own observations with those of the observer. While the goal\nof the observer is to maximize the type II error exponent of the test for a\ngiven type I error probability constraint, it also wants to keep a private part\nof its observations as oblivious to the detector as possible. Considering both\nequivocation and average distortion under a causal disclosure assumption as\npossible measures of privacy, the trade-off between the communication rate from\nthe observer to the detector, the type II error exponent, and privacy is\nstudied. For the general HT problem, we establish single-letter inner bounds on\nboth the rate-error exponent-equivocation and rate-error exponent-distortion\ntrade-offs. Subsequently, single-letter characterizations for both trade-offs\nare obtained (i) for testing against conditional independence of the observer's\nobservations from those of the detector, given some additional side-information\nat the detector; and (ii) when the communication rate constraint over the\nchannel is zero. Finally, we show by providing a counterexample that, the\nstrong converse which holds for distributed HT without a privacy constraint,\ndoes not hold when a privacy constraint is imposed. This implies that, in\ngeneral, the rate-error exponent-equivocation and rate-error\nexponent-distortion trade-offs are not independent of the type I error\nprobability constraint.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The remote observer has access to a discrete memoryless source, and communicates its observations to the detector via a rate-limited noiseless channel.\n* The detector observes another discrete memoryless source, and performs a binary hypothesis test on the joint distribution of its own observations with those of the observer."
    },
    {
        "abstract": "  We consider a two-state quantum walk on a line where after the first step an\nabsorbing sink is placed at the origin. The probability of finding the walker\nat position $j$, conditioned on that it has not returned to the origin, is\ninvestigated in the asymptotic limit. We prove a limit theorem for the\nconditional probability distribution and show that it is given by the Konno's\ndensity function modified by a pre-factor ensuring that the distribution\nvanishes at the origin. In addition, we discuss the relation to the problem of\nrecurrence of a quantum walk and determine the Polya number. Our approach is\nbased on path counting and stationary phase approximation.\n",
        "method": "Our approach is based on path counting and stationary phase approximation."
    },
    {
        "abstract": "  For $x\\in End(K^n)$ satisfying $x^2 = 0$ let $F_x$ be the variety of full\nflags stable under the action of $x$ (Springer fiber over $x$). The full\nclassification of the components of $F_x$ according to their smoothness was\nprovided in a paper of Fresse-Melnikov in terms of both Young tableaux and link\npatterns. Moreover in a paper of Fresse the purely combinatorial algorithm to\ncompute the singular locus of a singular components of $F_x$ is provided.\nHowever this algorithm involves the computation of the graph of the component,\nand the complexity of computations grows very quickly, so that in practice it\nis impossible to use it. In this paper, we construct another algorithm, derived\nfrom the algorithm of Fresse, providing all the components of the singular\nlocus of a singular component of $F_x$ in terms of link patterns constructed\nstraightforwardly from its link pattern.\n",
        "method": "Here is the methodological sentence:\n\nMoreover in a paper of Fresse the purely combinatorial algorithm to compute the singular locus of a singular components of $F_x$ is provided."
    },
    {
        "abstract": "  In this paper, first we present a new useful way of formulating probabilistic\nnormed spaces. Then by using this formulation and probabilistic normed space\nversion of the Baire category theorem, we prove four important results of\nfunctional analysis, i.e. the open mapping, closed graph, principle of uniform\nboundedness and Banach-Steinhaus theorem in PN-spaces.\n",
        "method": "First we present a new useful way of formulating probabilistic normed spaces."
    },
    {
        "abstract": "  Relying on rays, we search for submodules of a module V over a supertropical\nsemiring on which a given anisotropic quadratic form is quasilinear. Rays are\nclasses of a certain equivalence relation on V, that carry a notion of\nconvexity, which is consistent with quasilinearity. A criterion for\nquasilinearity is specified by a Cauchy-Schwartz ratio which paves the way to a\nconvex geometry on Ray(V), supported by a \"supertropical trigonometry\".\nEmploying a (partial) quasiordering on Ray(V), this approach allows for\nproducing convex quasilinear sets of rays, as well as paths, containing a given\nquasilinear set in a systematic way. Minimal paths are endowed with a\nsurprisingly rich combinatorial structure, delivered to the graph determined by\npairs of quasilinear rays -- apparently a fundamental object in the theory of\nsupertropical quadratic forms.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nEmploying a (partial) quasiordering on Ray(V), this approach allows for producing convex quasilinear sets of rays, as well as paths, containing a given quasilinear set in a systematic way."
    },
    {
        "abstract": "  Different routes towards the canonical formulation of a classical theory\nresult in different canonically equivalent Hamiltonians, while their quantum\ncounterparts are related through appropriate unitary transformation. However,\nfor higher-order theory of gravity, although two Hamiltonians emerging from the\nsame action differing by total derivative terms are related through canonical\ntransformation, the difference transpires while attempting canonical\nquantization, which is predominant in non-minimally coupled higher-order theory\nof gravity. We follow Dirac's constraint analysis to formulate phase-space\nstructures, in the presence (case-I) and absence (case-II) of total derivative\nterms. While the coupling parameter plays no significant role as such for\ncase-I, quantization depends on its form explicitly in case-II, and as a\nresult, unitary transformation relating the two is not unique. We find certain\nmathematical inconsistency in case-I, for modified Gauss-Bonnet-Dilatonic\ncoupled action, in particular. Thus, we conclude that total derivative terms\nindeed play a major role in the quantum domain and should be taken care of\na-priori, for consistency.\n",
        "method": "Methodological sentences:\n\n* We follow Dirac's constraint analysis to formulate phase-space structures...\n* While the coupling parameter plays no significant role as such..."
    },
    {
        "abstract": "  In 1895, Cantor showed that between every two countable dense real sets,\nthere is an order isomorphism. In fact, there is always such an order\nisomorphism, which is the restriction of a universal entire function.\n",
        "method": "No methodological sentences found in this abstract."
    },
    {
        "abstract": "  In permissive environments, E. coli can double its dry mass every 21 minutes.\nDuring this time, ribosomes, RNA polymerases, and the proteome are all doubled.\nYet, the question of how to relate bacterial doubling time to other\nbiologically relevant time scales in the growth process remains illusive, due\nto the complex temporal nesting pattern of these processes. In particular, the\nrelation between the cell's doubling time and the ribosome assembly time is not\nknown. Here we develop a model that connects growth rate to ribosome assembly\ntime and show that the existence of a self-assembly step increases the overall\ngrowth rate, because during ribosome self-assembly existing ribosomes can start\na new round of reproduction, by making a new batch of ribosomal proteins prior\nto the completion of the previous round. This overlapping of ribosome\nreproduction cycles increases growth rate beyond the serial-limit that is\ntypically assumed to hold. Using recent data from ribosome profiling and well\nknown measurements of the average translation rate, rigid bounds on the in-vivo\nribosome self-assembly time are set, which are robust to the assumptions\nregarding the biological noises involved. At 21 minutes doubling time, the\nribosome assembly time is found to be approximately 6 minutes --- three fold\nlarger than the common estimate. We further use our model to explain the\ndetrimental effect of a recently discovered ribosome assembly inhibitor drug,\nand predict the effect of limiting the expression of ribosome assembly\nchaperons on the overall growth rate.\n",
        "method": "During this time, ribosomes, RNA polymerases, and the proteome are all doubled."
    },
    {
        "abstract": "  We verify the critical case $p=p_0(n)$ of Strauss' conjecture (1981)\nconcerning the blow-up of solutions to semilinear wave equations with variable\ncoefficients in $\\mathbf{R}^n$, where $n\\geq 2$. The perturbations of Laplace\noperator are assumed to be smooth and decay exponentially fast at infinity. We\nalso obtain a sharp lifespan upper bound for solutions with compactly supported\ndata when $p=p_0(n)$. The unified approach to blow-up problems in all\ndimensions combines several classical ideas in order to generalize and simplify\nthe method of Zhou(2007) and Zhou and Han (2014): exponential \"eigenfunctions\"\nof the Laplacian are used to construct the test function $\\phi_q$ for linear\nwave equation with variable coefficients and John's method of iterations (1979)\nis augmented with the \"slicing method\" of Agemi, Kurokawa and Takamura (2000)\nfor lower bounds in the critical case.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The perturbations of Laplace operator are assumed to be smooth and decay exponentially fast at infinity.\n* We also obtain a sharp lifespan upper bound for solutions with compactly supported data when $p=p_0(n)$.\n* The unified approach to blow-up problems in all dimensions combines several classical ideas in order to generalize and simplify the method of Zhou(2007) and Zhou and Han (2014):\n\t+ Exponential \"eigenfunctions\" of the Laplacian are used to construct the test function $\\phi_q$ for linear wave equation with variable coefficients.\n\t+ John's method of iterations (1979) is augmented with the \"slicing method\" of Agemi, Kurokawa and Takamura (2000) for lower bounds in the critical case."
    },
    {
        "abstract": "  A watchman path is a path such that a direct line of sight exists between\neach point in some region and some point along the path. Here, we study the\nonline watchman path problem outside a convex polygon, i.e., in\n$\\mathbb{R}^2\\setminus \\Omega$, where $\\Omega$ is a convex polygon that is not\nknown in advance. We present an algorithm for the exploration of the region\noutside the polygon. We prove that the presented algorithms guarantees a\n$\\approx 22.77$ competitive ratio compared to the optimal offline watchman\npath.\n",
        "method": "Here is the methodological sentence:\n\nWe present an algorithm for the exploration of the region outside the polygon."
    },
    {
        "abstract": "  The spectral evolution and spatial distribution of backscattered Brillouin\nsignals is experimentally investigated in sub-wavelength silica microfibers.\nThe Brillouin spectrum evolution reveals the different dynamics of the various\npeaks, offering evidence of backscattering signals induced by acoustic waves\nwith phase velocity greater than that of the longitudinal wave. The spatial\ndistribution is found to have significant influence on the response of\nBrillouin scattering under tensile load, with hybrid acoustic modes providing a\nsmaller response under axial strain. This insight into interactions between\noptical and hybrid acoustic modes at sub-wavelength confinements could help\nunderstand ultrasonic waves in tapered waveguides, and have potential\napplications in optical sensing and detection.\n",
        "method": "Here is the methodological sentence:\n\nThe spectral evolution and spatial distribution of backscattered Brillouin signals is experimentally investigated in sub-wavelength silica microfibers."
    },
    {
        "abstract": "  We present a new method for the solution of PDEs on manifolds $\\mathbb{M}\n\\subset \\mathbb{R}^d$ of co-dimension one using stable scale-free radial basis\nfunction (RBF) interpolation. Our method involves augmenting polyharmonic\nspline (PHS) RBFs with polynomials to generate RBF-finite difference (RBF-FD)\nformulas. These polynomial basis elements are obtained using the\nrecently-developed \\emph{least orthogonal interpolation} technique (LOI) on\neach RBF-FD stencil to obtain \\emph{local} restrictions of polynomials in\n$\\mathbb{R}^3$ to stencils on $\\mathbb{M}$. The resulting RBF-LOI method uses\nCartesian coordinates, does not require any intrinsic coordinate systems or\nprojections of points onto tangent planes, and our tests illustrate robustness\nto stagnation errors. We show that our method produces high orders of\nconvergence for PDEs on the sphere and torus, and present some applications to\nreaction-diffusion PDEs motivated by biology.\n",
        "method": "We present a new method for the solution of PDEs on manifolds $\\mathbb{M} \\subset \\mathbb{R}^d$ of co-dimension one using stable scale-free radial basis function (RBF) interpolation."
    },
    {
        "abstract": "  Detecting bird sounds in audio recordings automatically, if accurate enough,\nis expected to be of great help to the research community working in bio- and\necoacoustics, interested in monitoring biodiversity based on audio field\nrecordings. To estimate how accurate the state-of-the-art machine learning\napproaches are, the Bird Audio Detection challenge involving large audio\ndatasets was recently organized. In this paper, experiments using several types\nof convolutional neural networks (i.e. standard CNNs, residual nets and densely\nconnected nets) are reported in the framework of this challenge. DenseNets were\nthe preferred solution since they were the best performing and most compact\nmodels, leading to a 88.22% area under the receiver operator curve score on the\ntest set of the challenge. Performance gains were obtained thank to data\naugmentation through time and frequency shifting, model parameter averaging\nduring training and ensemble methods using the geometric mean. On the contrary,\nthe attempts to enlarge the training dataset with samples of the test set with\nautomatic predictions used as pseudo-groundtruth labels consistently degraded\nperformance.\n",
        "method": "Here is the methodological sentence:\n\nExperiments using several types of convolutional neural networks (i.e. standard CNNs, residual nets and densely connected nets) are reported in the framework of this challenge."
    },
    {
        "abstract": "  In the past decades, beam-driven plasma wakefield acceleration (PWFA)\nexperiments have seen remarkable progress by using high-energy particle beams\nsuch as electron, positron and proton beams to drive wakes in neutral gas or\npre-ionized plasma. This review highlights a few recent experiments in the\nworld to compare experiment parameters and results.\n",
        "method": "No methodological sentences are present in this abstract. The text mainly provides an overview of the topic and mentions some recent experiments, but does not describe specific methods or approaches used in those experiments."
    },
    {
        "abstract": "  Atomically flat semiconducting materials such as monolayer WSe$_2$ hold great\npromise for novel optoelectronic devices. Recently, quantum light emission has\nbeen observed from bound excitons in exfoliated WSe$_2$. As part of developing\noptoelectronic devices, the control of the radiative properties of such\nemitters is an important step. Here we report the coupling of a bound exciton\nin WSe$_2$ to open microcavities. We use a range of radii of curvature in the\nplano-concave cavity geometry with mode volumes in the $\\lambda^3$ regime,\ngiving Purcell factors of up to 8 while increasing the photon flux five-fold.\nAdditionally we determine the quantum efficiency of the single photon emitter\nto be $\\eta = 0.46 \\pm 0.03$. Our findings pave the way to cavity-enhanced\nmonolayer based single photon sources for a wide range of applications in\nnanophotonics and quantum information technologies.\n",
        "method": "We use a range of radii of curvature in the plano-concave cavity geometry with mode volumes in the $\\lambda^3$ regime, giving Purcell factors of up to 8 while increasing the photon flux five-fold."
    },
    {
        "abstract": "  Several studies analyzed certain nonlinear dynamical systems by showing that\nthe cyclic number of sign variations in the vector of derivatives is an\ninteger-valued Lyapunov function. These results are based on direct analysis of\nthe dynamical equation satisfied by the vector of derivatives, i.e. the\nvariational system. However, it is natural to assume that they follow from the\nfact that the transition matrix in the variational system satisfies a variation\ndiminishing property (VDP) with respect to the cyclic number of sign variations\nin a vector. Motivated by this, we develop the theoretical framework of linear\ntime-varying systems whose solution satisfies a VDP with respect to the cyclic\nnumber of sign variations. This provides an analogue of the work of Schwarz on\ntotally positive differential systems, i.e. linear time-varying systems whose\nsolution satisfies a VDP with respect to the standard (non-cyclic) number of\nsign variations.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThese results are based on direct analysis of the dynamical equation satisfied by the vector of derivatives, i.e. the variational system."
    },
    {
        "abstract": "  We provide multicolored and infinite generalizations for a Ramsey-type\nproblem raised by Bollob\\'as, concerning colorings of $K_n$ where each color is\nwell-represented. Let $\\chi$ be a coloring of the edges of a complete graph on\n$n$ vertices into $r$ colors. We call $\\chi$ $\\varepsilon$-balanced if all\ncolor classes have $\\varepsilon$ fraction of the edges. Fix some graph $H$,\ntogether with an $r$-coloring of its edges. Consider the smallest natural\nnumber $R_\\varepsilon^r(H)$ such that for all $n\\geq R_\\varepsilon^r(H)$, all\n$\\varepsilon$-balanced colorings $\\chi$ of $K_n$ contain a subgraph isomorphic\nto $H$ in its coloring. Bollob\\'as conjectured a simple characterization of $H$\nfor which $R_\\varepsilon^2(H)$ is finite, which was later proved by Cutler and\nMont\\'agh. Here, we obtain a characterization for arbitrary values of $r$, as\nwell as asymptotically tight bounds. We also discuss generalizations to graphs\ndefined on perfect Polish spaces, where the corresponding notion of\nbalancedness is each color class being non-meagre.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe provide multicolored and infinite generalizations for a Ramsey-type problem raised by Bollob\\'as, concerning colorings of $K_n$ where each color is well-represented."
    },
    {
        "abstract": "  This is the first of two papers in which we investigate the properties of the\ndisplacement functions of automorphisms of free groups (more generally, free\nproducts) on Culler-Vogtmann Outer space and its simplicial bordification - the\nfree splitting complex - with respect to the Lipschitz metric. The theory for\nirreducible automorphisms being well-developed, we concentrate on the reducible\ncase. Since we deal with the bordification, we develop all the needed tools in\nthe more general setting of deformation spaces, and their associated free\nsplitting complexes.\n  In the present paper we study the local properties of the displacement\nfunction. In particular, we study its convexity properties and the behaviour at\nbordification points, by geometrically characterising its continuity-points. We\nprove that the global-simplex-displacement spectrum of $Aut(F_n)$ is a\nwell-ordered subset of $\\mathbb R$, this being helpful for algorithmic\npurposes. We introduce a weaker notion of train tracks, which we call {\\em\npartial train tracks} (which coincides with the usual one for irreducible\nautomorphisms) and we prove that, for any automorphism, points of minimal\ndisplacement - minpoints - coincide with the marked metric graphs that support\npartial train tracks. We show that any automorphism, reducible or not, has a\npartial train track (hence a minpoint) either in the outer space or its\nbordification. We show that, given an automorphism, any of its invariant free\nfactors is seen in a partial train track map. In a subsequent paper we will\nprove that level sets of the displacement functions are connected, and we will\napply that result to solve certain decision problems.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Since we deal with the bordification, we develop all the needed tools in the more general setting of deformation spaces, and their associated free splitting complexes.\n* We prove that the global-simplex-displacement spectrum of $Aut(F_n)$ is a well-ordered subset of $\\mathbb R$, this being helpful for algorithmic purposes."
    },
    {
        "abstract": "  This is the second of two papers in which we investigate the properties of\ndisplacement functions of automorphisms of free groups (more generally, free\nproducts) on the Culler-Vogtmann Outer space $CV_n$ and its simplicial\nbordification. We develop a theory for both reducible and irreducible\nautormorphisms. As we reach the bordification of $CV_n$ we have to deal with\ngeneral deformation spaces, for this reason we developed the theory in such\ngenerality. In first paper~\\cite{FMpartI} we studied general properties of the\ndisplacement functions, such as well-orderability of the spectrum and the\ntopological characterization of min-points via partial train tracks (possibly\nat infinity). This paper is devoted to proving that for any automorphism\n(reducible or not) any level set of the displacement function is connected. As\nan application, this result provides a stopping procedure for brute force\nsearch algorithms in $CV_n$. We use this to reprove two known algorithmic\nresults: the conjugacy problem for irreducible automorphisms and detecting\nirreducibility of automorphisms. Note: the two papers were originally packed\ntogether in the preprint arxiv:1703.09945. We decided to split that paper\nfollowing the recommendations of a referee.\n",
        "method": "As we reach the bordification of $CV_n$ we have to deal with general deformation spaces, for this reason we developed the theory in such generality."
    },
    {
        "abstract": "  In this paper we demonstrate that the selection of events with different\nmultiplicities of produced particles, leads to the violation of the azimuthal\nangular symmetry, $\\phi \\to \\pi - \\phi$. We find for LHC and lower energies,\nthat this violation can be so large for the events with multiplicities $n \\geq\n2 \\bar{n}$, where $\\bar{n}$ is the mean multiplicity, that it leads to almostno\nsuppression of $v_n$, with odd $n$. However, this can only occur if the typical\nsize of the dipole in DIS with a nuclear target is small, or $Q^2 \\,>\\,Q^2_s\\Lb\nA, Y_{\\rm min},b\\Rb$, where $Q_s$ is the saturation momentum of the nucleus at\n$Y = Y_{\\rm min}$. In the case of large sizes of dipoles, when $Q^2\n\\,<\\,Q^2_s\\Lb A, Y_{\\rm min},b\\Rb$, we show that $v_n =0$ for odd $n$.\nHadron-nucleus scattering is discussed.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We demonstrate that the selection of events with different multiplicities of produced particles, leads to the violation of the azimuthal angular symmetry...\n* We find for LHC and lower energies, that this violation can be so large for the events with multiplicities $n \\geq 2 \\bar{n}$..."
    },
    {
        "abstract": "  The study of Coulomb branches of 3-dimensional N=4 gauge theories via the\nassociated Hilbert series, the so-called monopole formula, has been proven\nuseful not only for 3-dimensional theories, but also for Higgs branches of 5\nand 6-dimensional gauge theories with 8 supercharges. Recently, a conjecture\nconnected different phases of 6-dimensional Higgs branches via gauging of a\ndiscrete global $S_n$ symmetry. On the corresponding 3-dimensional Coulomb\nbranch, this amounts to a geometric $S_n$-quotient. In this note, we prove the\nconjecture on Coulomb branches with unitary nodes and, moreover, extend it to\nCoulomb branches with other classical groups. The results promote discrete\n$S_n$-quotients to a versatile tool in the study of Coulomb branches.\n",
        "method": "The methodological sentence is:\n\nWe prove the conjecture on Coulomb branches with unitary nodes and, moreover, extend it to Coulomb branches with other classical groups."
    },
    {
        "abstract": "  We initiate a study of an infinite set of renormalization group flows with\naccidental supersymmetry enhancement. The ultraviolet fixed points are strongly\ninteracting four-dimensional $\\mathcal{N}=2$ superconformal field theories\n(SCFTs) with no known Lagrangian descriptions, and the infrared fixed points\nare SCFTs with thirty-two (Poincar\\'e plus special) supercharges.\n",
        "method": "Here is the methodological sentence:\n\nWe initiate a study of an infinite set of renormalization group flows..."
    },
    {
        "abstract": "  Gradually typed languages allow statically typed and dynamically typed code\nto interact while maintaining benefits of both styles. The key to reasoning\nabout these mixed programs is Siek-Vitousek-Cimini-Boyland's (dynamic) gradual\nguarantee, which says that giving components of a program more precise types\nonly adds runtime type checking, and does not otherwise change behavior. In\nthis paper, we give a semantic reformulation of the gradual guarantee called\ngraduality. We change the name to promote the analogy that graduality is to\ngradual typing what parametricity is to polymorphism. Each gives a\nlocal-to-global, syntactic-to-semantic reasoning principle that is formulated\nin terms of a kind of observational approximation.\n  Utilizing the analogy, we develop a novel logical relation for proving\ngraduality. We show that embedding-projection pairs (ep pairs) are to\ngraduality what relations are to parametricity. We argue that casts between two\ntypes where one is \"more dynamic\" (less precise) than the other necessarily\nform an ep pair, and we use this to cleanly prove the graduality cases for\ncasts from the ep-pair property. To construct ep pairs, we give an analysis of\nthe type dynamism relation (also known as type precision or naive subtyping)\nthat interprets the rules for type dynamism as compositional constructions on\nep pairs, analogous to the coercion interpretation of subtyping.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We give a semantic reformulation of the gradual guarantee called graduality.\n* We develop a novel logical relation for proving graduality.\n* We argue that casts between two types... necessarily form an ep pair, and we use this to cleanly prove the graduality cases for casts from the ep-pair property.\n* To construct ep pairs, we give an analysis of the type dynamism relation that interprets the rules for type dynamism as compositional constructions on ep pairs."
    },
    {
        "abstract": "  An automatic program that generates constant profit from the financial market\nis lucrative for every market practitioner. Recent advance in deep\nreinforcement learning provides a framework toward end-to-end training of such\ntrading agent. In this paper, we propose an Markov Decision Process (MDP) model\nsuitable for the financial trading task and solve it with the state-of-the-art\ndeep recurrent Q-network (DRQN) algorithm. We propose several modifications to\nthe existing learning algorithm to make it more suitable under the financial\ntrading setting, namely 1. We employ a substantially small replay memory (only\na few hundreds in size) compared to ones used in modern deep reinforcement\nlearning algorithms (often millions in size.) 2. We develop an action\naugmentation technique to mitigate the need for random exploration by providing\nextra feedback signals for all actions to the agent. This enables us to use\ngreedy policy over the course of learning and shows strong empirical\nperformance compared to more commonly used epsilon-greedy exploration. However,\nthis technique is specific to financial trading under a few market assumptions.\n3. We sample a longer sequence for recurrent neural network training. A side\nproduct of this mechanism is that we can now train the agent for every T steps.\nThis greatly reduces training time since the overall computation is down by a\nfactor of T. We combine all of the above into a complete online learning\nalgorithm and validate our approach on the spot foreign exchange market.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n1. In this paper, we propose an Markov Decision Process (MDP) model suitable for the financial trading task and solve it with the state-of-the-art deep recurrent Q-network (DRQN) algorithm.\n2. We employ a substantially small replay memory (only a few hundreds in size) compared to ones used in modern deep reinforcement learning algorithms (often millions in size.)\n3. We develop an action augmentation technique to mitigate the need for random exploration by providing extra feedback signals for all actions to the agent.\n4. We sample a longer sequence for recurrent neural network training."
    },
    {
        "abstract": "  Peripheral nerve injuries are difficult to treat due to limited axon\nregeneration; brief electrical stimulation of injured nerves is an emerging\ntherapy that can relieve pain and enhance regeneration. We report an original\nwireless stimulator based on a metal loop (diameter ~1 mm) that is powered by a\ntranscranial magnetic stimulator (TMS). The loop can be integrated in a\nchitosan scaffold that functions as a graft when applied onto transected nerves\n(graft-antenna). The graft-antenna was bonded to rat sciatic nerves by a laser\nwithout sutures; it did not migrate after implantation and was able to trigger\nsteady compound muscle action potentials for 12 weeks (CMAP ~1.3 mV). Eight\nweeks post-operatively, axon regeneration was facilitated in transected nerves\nthat were repaired with the graft-antenna and stimulated by the TMS for 1\nhour/week. The graft-antenna is an innovative and minimally-invasive device\nthat functions concurrently as a wireless stimulator and adhesive scaffold for\nnerve repair.\n",
        "method": "The loop can be integrated in a chitosan scaffold that functions as a graft when applied onto transected nerves (graft-antenna)."
    },
    {
        "abstract": "  Recently, a number of statistical problems have found an unexpected solution\nby inspecting them through a \"modal point of view\". These include classical\ntasks such as clustering or regression. This has led to a renewed interest in\nestimation and inference for the mode. This paper offers an extensive survey of\nthe traditional approaches to mode estimation and explores the consequences of\napplying this modern modal methodology to other, seemingly unrelated, fields.\n",
        "method": "None found."
    },
    {
        "abstract": "  In this paper, we consider the class of quasiconvex functions and its proper\nsubclass of conic functions. The integer minimization problem of these\nfunctions is considered in the paper, assuming that an optimized function is\ndefined by the comparison oracle. We will show that there is no a polynomial\nalgorithm on $\\log R$ to optimize quasiconvex functions in the ball of integer\nradius $R$ using only the comparison oracle. On the other hand, if an optimized\nfunction is conic, then we show that there is a polynomial on $\\log R$\nalgorithm. We also present an exponential on the dimension lower bound for the\noracle complexity of the conic function integer optimization problem.\nAdditionally, we give examples of known problems that can be polynomially\nreduced to the minimization problem of functions in our classes.\n",
        "method": "Here is the methodological sentence:\n\nWe will show that there is no a polynomial algorithm on $\\log R$ to optimize quasiconvex functions in the ball of integer radius $R$ using only the comparison oracle."
    },
    {
        "abstract": "  Breaking of ensemble equivalence between the microcanonical ensemble and the\ncanonical ensemble may occur for random graphs whose size tends to infinity,\nand is signaled by a non-zero specific relative entropy of the two ensembles.\nIn [3] and [4] it was shown that breaking occurs when the constraint is put on\nthe degree sequence (configuration model). It is not known what is the effect\non the relative entropy when the number of constraints is reduced, i.e., when\nonly part of the nodes are constrained in their degree (and the remaining nodes\nare left unconstrained). Intuitively, the relative entropy is expected to\ndecrease. However, this is not a trivial issue because when constraints are\nremoved both the microcanonical ensemble and the canonical ensemble change. In\nthis paper a formula for the relative entropy valid for generic discrete random\nstructures, recently formulated by Squartini and Garlaschelli, is used to prove\nthat the relative entropy is monotone in the number of constraints when the\nconstraint is on the degrees of the nodes. It is further shown that the\nexpression for the relative entropy corresponds, in the dense regime, to the\ndegrees in the microcanonical ensemble being asymptotically multivariate Dirac\nand in the canonical ensemble being asymptotically Gaussian.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* None (there is no explicit mention of methods or approaches used in the research)"
    },
    {
        "abstract": "  The pattern of branched electron flow revealed by scanning gate microscopy\nshows the distribution of ballistic electron trajectories. The details of the\npattern are determined by the correlated potential of remote dopants with an\namplitude far below the Fermi energy. We find that the pattern persists even if\nthe electron density is significantly reduced such that the change in Fermi\nenergy exceeds the background potential amplitude. The branch pattern is robust\nagainst changes in charge carrier density, but not against changes in the\nbackground potential caused by additional illumination of the sample.\n",
        "method": "The methodological sentence is:\n\nWe find that the pattern persists even if the electron density is significantly reduced such that the change in Fermi energy exceeds the background potential amplitude."
    },
    {
        "abstract": "  The impact of local reflection symmetry on wave localization and transport\nwithin finite disordered chains is investigated. Local symmetries thereby play\nthe role of a spatial correlation of variable range in the finite system. We\nfind that, on ensemble average, the chain eigenstates become more fragmented\nspatially for intermediate average symmetry domain sizes, depending on the\ndegree of disorder. This is caused by the partial formation of states with\napproximate local parity confined within fictitious, disorder-induced double\nwells and perturbed by the coupling to adjacent domains. The dynamical\nevolution of wave-packets shows that the average site-resolved transfer\nefficiency is enhanced between regions connected by local symmetry. The\ntransfer may further be drastically amplified in the presence of spatial\noverlap between the symmetry domains, and in particular when global and local\nsymmetry coexist. Applicable to generic discrete models for matter and light\nwaves, our work provides a perspective to understand and exploit the impact of\nlocal order at multiple scales in complex systems.\n",
        "method": "Here is the methodological sentence:\n\nWe find that..."
    },
    {
        "abstract": "  From the luminosity, effective temperature, and age of the Hyades brown dwarf\n2MASSJ04183483+2131275 (2M0418), sub-stellar evolutionary models predict a mass\nin the range 39-55 Jupiter masses (M_Jup) which is insufficient to produce any\nsubstantial lithium burning except for the very upper range >53 M_Jup. Our goal\nis to measure the abundance of lithium in this object, test the consistency\nbetween models and observations and refine constraints on the mass and age of\nthe object.\n  We used the 10.4-m Gran Telescopio Canarias (GTC) with its low-dispersion\noptical spectrograph to obtain ten spectra of 2277s each covering the range\n6300-10300 Angstroms with a resolving power of R~500.\n  In the individual spectra, which span several months, we detect persistent\nunresolved H_alpha in emission with pseudo equivalent widths (pEW) in the range\n45-150 Angstroms and absorption lines of various alkalis with the typical\nstrengths found in objects of L5 spectral type. The lithium resonance line at\n6707.8 Angstroms is detected with pEW of 18+/-4 Angstroms in 2M0418 (L5).\n  We determine a lithium abundance of log N(Li) = 3.0+/-0.4 dex consistent with\na minimum preservation of 90% of this element which confirms 2M0418 as a brown\ndwarf with a maximum mass of 52 M_Jup. We infer a maximum age for the Hyades of\n775 Myr from a comparison with the BHAC15 models. Combining recent results from\nthe literature with our study, we constrain the mass of 2M0418 to 45-52 M_Jup\nand the age of the cluster to 580-775 Myr (1 sigma) based on the lithium\ndepletion boundary method.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We used the 10.4-m Gran Telescopio Canarias (GTC) with its low-dispersion optical spectrograph to obtain ten spectra of 2277s each covering the range 6300-10300 Angstroms with a resolving power of R~500.\n* In the individual spectra, which span several months..."
    },
    {
        "abstract": "  A Bayesian approach termed BAyesian Least Squares Optimization with\nNonnegative L1-norm constraint (BALSON) is proposed. The error distribution of\ndata fitting is described by Gaussian likelihood. The parameter distribution is\nassumed to be a Dirichlet distribution. With the Bayes rule, searching for the\noptimal parameters is equivalent to finding the mode of the posterior\ndistribution. In order to explicitly characterize the nonnegative L1-norm\nconstraint of the parameters, we further approximate the true posterior\ndistribution by a Dirichlet distribution. We estimate the statistics of the\napproximating Dirichlet posterior distribution by sampling methods. Four\nsampling methods have been introduced. With the estimated posterior\ndistributions, the original parameters can be effectively reconstructed in\npolynomial fitting problems, and the BALSON framework is found to perform\nbetter than conventional methods.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The error distribution of data fitting is described by Gaussian likelihood.\n* The parameter distribution is assumed to be a Dirichlet distribution.\n* We further approximate the true posterior distribution by a Dirichlet distribution.\n* We estimate the statistics of the approximating Dirichlet posterior distribution by sampling methods."
    },
    {
        "abstract": "  We analyze the breakdown of causality for the perfect fluid limit in a medium\nwith polarizeability. We show that to restore causality a relaxation term\nlinking vorticity and polarization, analogous to the Israel-Stewart term\nlinking viscous forces and gradients,is required. This term provides a minimum\namount of dissipation a locally thermalized relativistic medium with\npolarizeability must have, independently of its underlying degrees of freedom.\nFor ferromagnetic materials an infrared acausal mode remains, which we\ninterpret as a Banks-Casher mode signaling spontaneous magnetization. With\nthese ingredients, we propose a candidate for a fully causal Lagrangian of a\nrelativistic polarizeable system near the perfect fluid limit.\n",
        "method": "We show that to restore causality a relaxation term linking vorticity and polarization, analogous to the Israel-Stewart term linking viscous forces and gradients,is required."
    },
    {
        "abstract": "  An extremal curve germ is the analytic germ of a threefold with terminal\nsingularities along a reduced complete curve admitting a contraction whose\nfibers have dimension at most one. The aim of the present paper is to review\nthe results concerning those contractions whose central fiber is irreducible\nand contains only one non-Gorenstein point.\n",
        "method": "Methodological sentences are not explicitly mentioned in this abstract, as it appears to be a summary or introduction rather than a description of methods used in the research."
    },
    {
        "abstract": "  A software architecture is the result of multiple decisions made by a\nsoftware architect. These decisions are called architectural decisions, as they\nbring solutions to architectural problems. Relations between decisions can be\ncaptured in architectural decision models. Such models are then a form of\nreusable knowledge for software architects. Several models have been described\nin the literature, introducing necessary concepts and relations. These concepts\nand relations were usually explained using natural language. Not much work has\nbeen done so far on their formal definitions. Specifically, such a definition\nof an architectural decision model is still missing. The purpose of this paper\nis filling this gap by providing the formal definition of an architectural\ndecision model at both syntax and semantics levels. At the syntax level,\ndifferent concepts and relations that are elements of a model have been\nmathematically defined. At the semantics level, the meaning of a model has been\ndefined in a form of denotational semantics. The formalization not only allows\nfor better understanding of architectural decision models but opens the\npossibility to reason on such models, e.g., checking their consistency -\nsomething that is very limited for the models proposed so far. A practical\nexample of the semantics of an architectural decision model is also presented.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Relations between decisions can be captured in architectural decision models.\n* Several models have been described in the literature, introducing necessary concepts and relations.\n* These concepts and relations were usually explained using natural language.\n* The formalization not only allows for better understanding of architectural decision models but opens the possibility to reason on such models, e.g., checking their consistency."
    },
    {
        "abstract": "  The ability to learn from incrementally arriving data is essential for any\nlife-long learning system. However, standard deep neural networks forget the\nknowledge about the old tasks, a phenomenon called catastrophic forgetting,\nwhen trained on incrementally arriving data. We discuss the biases in current\nGenerative Adversarial Networks (GAN) based approaches that learn the\nclassifier by knowledge distillation from previously trained classifiers. These\nbiases cause the trained classifier to perform poorly. We propose an approach\nto remove these biases by distilling knowledge from the classifier of AC-GAN.\nExperiments on MNIST and CIFAR10 show that this method is comparable to current\nstate of the art rehearsal based approaches. The code for this paper is\navailable at https://bit.ly/incremental-learning\n",
        "method": "Here is the extracted methodological sentence:\n\nWe discuss the biases in current GAN-based approaches that learn the classifier by knowledge distillation from previously trained classifiers."
    },
    {
        "abstract": "  The goal of this work is spatio-temporal action localization in videos, using\nonly the supervision from video-level class labels. The state-of-the-art casts\nthis weakly-supervised action localization regime as a Multiple Instance\nLearning problem, where instances are a priori computed spatio-temporal\nproposals. Rather than disconnecting the spatio-temporal learning from the\ntraining, we propose Spatio-Temporal Instance Learning, which enables action\nlocalization directly from box proposals in video frames. We outline the\nassumptions of our model and propose a max-margin objective and optimization\nwith latent variables that enable spatio-temporal learning of actions from\nvideo labels. We also provide an efficient linking algorithm and two reranking\nstrategies to facilitate and further improve the action localization.\nExperimental evaluation on four action datasets demonstrate the effectiveness\nof our approach for localization from weak supervision. Moreover, we show how\nto incorporate other supervision levels and mixtures, as a step towards\ndetermining optimal supervision strategies for action localization.\n",
        "method": "Rather than disconnecting the spatio-temporal learning from the training, we propose Spatio-Temporal Instance Learning, which enables action localization directly from box proposals in video frames."
    },
    {
        "abstract": "  We study smooth, global-in-time solutions of the relativistic Vlasov-Maxwell\nsystem that possess arbitrarily large charge densities and electric fields. In\nparticular, we construct spherically symmetric solutions that describe a thin\nshell of equally charged particles concentrating arbitrarily close to the\norigin and which give rise to charge densities and electric fields as large as\none desires at some finite time. We show that these solutions exist even for\narbitrarily small initial data or any desired mass. In the latter case, the\ntime at which solutions concentrate can also be made arbitrarily large.\n",
        "method": "We construct spherically symmetric solutions that describe a thin shell of equally charged particles concentrating arbitrarily close to the origin and which give rise to charge densities and electric fields as large as one desires at some finite time."
    },
    {
        "abstract": "  One of the key differences between the learning mechanism of humans and\nArtificial Neural Networks (ANNs) is the ability of humans to learn one task at\na time. ANNs, on the other hand, can only learn multiple tasks simultaneously.\nAny attempts at learning new tasks incrementally cause them to completely\nforget about previous tasks. This lack of ability to learn incrementally,\ncalled Catastrophic Forgetting, is considered a major hurdle in building a true\nAI system. In this paper, our goal is to isolate the truly effective existing\nideas for incremental learning from those that only work under certain\nconditions. To this end, we first thoroughly analyze the current state of the\nart (iCaRL) method for incremental learning and demonstrate that the good\nperformance of the system is not because of the reasons presented in the\nexisting literature. We conclude that the success of iCaRL is primarily due to\nknowledge distillation and recognize a key limitation of knowledge\ndistillation, i.e, it often leads to bias in classifiers. Finally, we propose a\ndynamic threshold moving algorithm that is able to successfully remove this\nbias. We demonstrate the effectiveness of our algorithm on CIFAR100 and MNIST\ndatasets showing near-optimal results. Our implementation is available at\nhttps://github.com/Khurramjaved96/incremental-learning.\n",
        "method": "To isolate the truly effective existing ideas for incremental learning from those that only work under certain conditions, we thoroughly analyze the current state of the art (iCaRL) method and demonstrate that its good performance is not due to reasons presented in the existing literature."
    },
    {
        "abstract": "  From the gambling logs of an online lottery game we extract the probability\ndistribution of various quantities (e.g., bet value, total pool size, waiting\ntime between successive gambles) as well as related correlation coefficients.\nWe view the net change of income of each player as a random walk. The mean\nsquared displacement of these net income random walks exhibits a transition\nbetween a super-diffusive and a normal diffusive regime. We discuss different\nrandom walk models with truncated power-law step lengths distributions that\nallow to reproduce some of the properties extracted from the gambling logs.\nAnalyzing the mean squared displacement and the first-passage time distribution\nfor these models allows to identify the key features needed for observing this\ncrossover from super-diffusion to normal diffusion.\n",
        "method": "Here is the methodological sentence:\n\nWe extract the probability distribution of various quantities (e.g., bet value, total pool size, waiting time between successive gambles) as well as related correlation coefficients."
    },
    {
        "abstract": "  Automatic lesion segmentation in dermoscopy images is an essential step for\ncomputer-aided diagnosis of melanoma. The dermoscopy images exhibits rotational\nand reflectional symmetry, however, this geometric property has not been\nencoded in the state-of-the-art convolutional neural networks based skin lesion\nsegmentation methods. In this paper, we present a deeply supervised rotation\nequivariant network for skin lesion segmentation by extending the recent group\nrotation equivariant network~\\cite{cohen2016group}. Specifically, we propose\nthe G-upsampling and G-projection operations to adapt the rotation equivariant\nclassification network for our skin lesion segmentation problem. To further\nincrease the performance, we integrate the deep supervision scheme into our\nproposed rotation equivariant segmentation architecture. The whole framework is\nequivariant to input transformations, including rotation and reflection, which\nimproves the network efficiency and thus contributes to the segmentation\nperformance. We extensively evaluate our method on the ISIC 2017 skin lesion\nchallenge dataset. The experimental results show that our rotation equivariant\nnetworks consistently excel the regular counterparts with the same model\ncomplexity under different experimental settings. Our best model achieves\n77.23\\%(JA) on the test dataset, outperforming the state-of-the-art challenging\nmethods and further demonstrating the effectiveness of our proposed deeply\nsupervised rotation equivariant segmentation network. Our best model also\noutperforms the state-of-the-art challenging methods, which further demonstrate\nthe effectiveness of our proposed deeply supervised rotation equivariant\nsegmentation network.\n",
        "method": "Here is the methodological sentence:\n\nWe propose the G-upsampling and G-projection operations to adapt the rotation equivariant classification network for our skin lesion segmentation problem."
    },
    {
        "abstract": "  In 1983, Conway and Gordon proved that for every spatial complete graph on\nsix vertices, the sum of the linking numbers over all of the constituent\ntwo-component links is odd, and that for every spatial complete graph on seven\nvertices, the sum of the Arf invariants over all of the Hamiltonian knots is\nodd. In 2009, the second author gave integral lifts of the Conway-Gordon\ntheorems in terms of the square of the linking number and the second\ncoefficient of the Conway polynomial. In this paper, we generalize the integral\nConway-Gordon theorems to complete graphs with arbitrary number of vertices\ngreater than or equal to six. As an application, we show that for every\nrectilinear spatial complete graph whose number of vertices is greater than or\nequal to six, the sum of the second coefficients of the Conway polynomials over\nall of the Hamiltonian knots is determined explicitly in terms of the number of\ntriangle-triangle Hopf links.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nAs an application, we show that for every rectilinear spatial complete graph whose number of vertices is greater than or equal to six..."
    },
    {
        "abstract": "  We show that the $\\g$-vector of the interval subdivision of a simplicial\ncomplex with a nonnegative and symmetric $h$-vector is nonnegative. In\nparticular, we prove that such $\\g$-vector is the $f$-vector of some balanced\nsimplicial complex. Moreover, we show that the local $\\g$-vector of the\ninterval subdivision of a simplex is nonnegative; answering a question by\nJuhnke-Kubitzke et al.\n",
        "method": "Methodological sentences: None provided in this abstract."
    },
    {
        "abstract": "  Two-dimensional (2D) organic-inorganic perovskites have recently attracted\nincreasing attention due to their great environmental stability, remarkable\nquantum confinement effect and layered characteristic. Heterostructures\nconsisting of 2D layered perovskites are expected to exhibit new physical\nphenomena inaccessible to the single 2D perovskites and can greatly extend\ntheir functionalities for novel electronic and optoelectronic applications.\nHerein, we develop a novel solution method to synthesize 2D perovskite\nsingle-crystals with the centimeter size, high phase purity, controllable\njunction depth, high crystalline quality and great stability for highly narrow\ndual-band photodetectors. On the basis of the different lattice constant,\nsolubility and growth rate between different n number, the newly designed\nsynthesis method allows to first grow n=1 perovskite guided by the\nself-assembled layer of the organic cations at the water-air interface and\nsubsequently n=2 layer is formed via diffusion process. Such growth process\nprovides an efficient away for us to readily obtain 2D perovskite\nheterostructural single-crystals with various thickness and junction depth by\ncontrolling the concentration, reaction temperature and time. Photodetectors\nbased on such heterostructural single crystal plates exhibit extremely low dark\ncurrent, high on-off current ratio, and highly narrow dual-band spectral\nresponse with a full-width at half-maximum of 20 nm at 540 nm and 34 nm at 610\nnm. In particular, the synthetic strategy is general for other 2D perovskites\nand the narrow dual-band spectral response with all full-width at half-maximum\nbelow 40 nm can be continuously tuned from red to blue by properly changing the\nhalide compositions.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* On the basis of the different lattice constant, solubility and growth rate between different n number, the newly designed synthesis method allows to first grow n=1 perovskite guided by the self-assembled layer of the organic cations at the water-air interface and subsequently n=2 layer is formed via diffusion process.\n* Such growth process provides an efficient away for us to readily obtain 2D perovskite heterostructural single-crystals with various thickness and junction depth by controlling the concentration, reaction temperature and time."
    },
    {
        "abstract": "  One scheme is presented to construct the robust multi-qubit arbitrary-phase\ncontrolled-phase gate (CPG) with one control and multiple target qubits in\nRydberg atoms using the Lewis-Riesenfeld (LR) invariant method. The scheme is\nnot limited by adiabatic condition while preserves the robustness against\ncontrol parameter variations of adiabatic evolution. Comparing with the\nadiabatic case, our scheme does not require very strong Rydberg interaction\nstrength. Taking the construction of two-qubit $\\pi$ CPG as an example, our\nscheme is more robust against control parameter variations than non-adiabatic\nscheme and faster than adiabatic scheme.\n",
        "method": "Here is the methodological sentence:\n\nThe scheme is presented to construct the robust multi-qubit arbitrary-phase controlled-phase gate (CPG) with one control and multiple target qubits in Rydberg atoms using the Lewis-Riesenfeld (LR) invariant method."
    },
    {
        "abstract": "  We present a complete reasoning principle for contextual equivalence in an\nuntyped probabilistic language. The language includes continuous (real-valued)\nrandom variables, conditionals, and scoring. It also includes recursion, since\nthe standard call-by-value fixpoint combinator is expressible. We demonstrate\nthe usability of our characterization by proving several equivalence schemas,\nincluding familiar facts from lambda calculus as well as results specific to\nprobabilistic programming. In particular, we use it to prove that reordering\nthe random draws in a probabilistic program preserves contextual equivalence.\nThis allows us to show, for example, that (let x = $e_1$ in let y = $e_2$ in\n$e_0$) is equivalent to (let y = $e_2$ in let x = $e_1$ in $e_0$) (provided $x$\ndoes not occur free in $e_2$ and $y$ does not occur free in $e_1$) despite the\nfact that $e_1$ and $e_2$ may have sampling and scoring effects.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We present a complete reasoning principle for contextual equivalence in an untyped probabilistic language.\n* We demonstrate the usability of our characterization by proving several equivalence schemas...\n* We use it to prove that reordering the random draws in a probabilistic program preserves contextual equivalence."
    },
    {
        "abstract": "  Measurements of normalized differential cross sections as functions of the\nmultiplicity and kinematic variables of charged-particle tracks from the\nunderlying event in top quark and antiquark pair production are presented. The\nmeasurements are performed in proton-proton collisions at a center-of-mass\nenergy of 13 TeV, and are based on data collected by the CMS experiment at the\nLHC in 2016 corresponding to an integrated luminosity of 35.9 fb$^{-1}$. Events\ncontaining one electron, one muon, and two jets from the hadronization and\nfragmentation of b quarks are used. These measurements characterize, for the\nfirst time, properties of the underlying event in top quark pair production and\nshow no deviation from the universality hypothesis at energy scales typically\nabove twice the top quark mass.\n",
        "method": "Events containing one electron, one muon, and two jets from the hadronization and fragmentation of b quarks are used."
    },
    {
        "abstract": "  Bayesian optimization is an approach to optimizing objective functions that\ntake a long time (minutes or hours) to evaluate. It is best-suited for\noptimization over continuous domains of less than 20 dimensions, and tolerates\nstochastic noise in function evaluations. It builds a surrogate for the\nobjective and quantifies the uncertainty in that surrogate using a Bayesian\nmachine learning technique, Gaussian process regression, and then uses an\nacquisition function defined from this surrogate to decide where to sample. In\nthis tutorial, we describe how Bayesian optimization works, including Gaussian\nprocess regression and three common acquisition functions: expected\nimprovement, entropy search, and knowledge gradient. We then discuss more\nadvanced techniques, including running multiple function evaluations in\nparallel, multi-fidelity and multi-information source optimization,\nexpensive-to-evaluate constraints, random environmental conditions, multi-task\nBayesian optimization, and the inclusion of derivative information. We conclude\nwith a discussion of Bayesian optimization software and future research\ndirections in the field. Within our tutorial material we provide a\ngeneralization of expected improvement to noisy evaluations, beyond the\nnoise-free setting where it is more commonly applied. This generalization is\njustified by a formal decision-theoretic argument, standing in contrast to\nprevious ad hoc modifications.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression...\n* We then discuss more advanced techniques, including running multiple function evaluations in parallel...\n* Within our tutorial material we provide a generalization of expected improvement to noisy evaluations..."
    },
    {
        "abstract": "  The column-and-constraint generation (CCG) method was introduced by\n\\citet{Zeng2013} for solving two-stage adaptive optimization. We found that the\nCCG method is quite scalable, but sometimes, and in some applications often,\nproduces infeasible first-stage solutions, even though the problem is feasible.\nIn this research, we extend the CCG method in a way that (a) maintains\nscalability and (b) always produces feasible first-stage decisions if they\nexist. We compare our method to several recently proposed methods and find that\nit reaches high accuracies faster and solves significantly larger problems.\n",
        "method": "We extend the CCG method in a way that (a) maintains scalability and (b) always produces feasible first-stage decisions if they exist."
    },
    {
        "abstract": "  Within the framework of the coalescence model based on the phase-space\ndistributions of protons and neutrons generated from the {{\\tt iEBE-VISHNU}}\nhybrid model with {{\\tt AMPT}} initial conditions, we study the spectra and\nelliptic flow of deuterons and helium-3 in relativistic heavy ion collisions at\nthe Relativistic Heavy Ion Collider (RHIC) and the Larger Hadron Collider\n(LHC). Results from our model calculations for Au + Au collisions at\n$\\sqrt{s_{NN}}=200$ GeV at RHIC and Pb+Pb collisions at $\\sqrt{s_{NN}}=2.76$\nTeV at the LHC are compared with available experimental data. Good agreements\nare generally seen between theoretical results and experimental data, except\nthat the calculated yield of helium-3 in Pb + Pb collisions at\n$\\sqrt{s_{NN}}=2.76$ TeV underestimates the data by about a factor of two.\nPossible reasons for these discrepancies are discussed. We also make\npredictions on the spectra and elliptic flow of deuterons and helium-3 in Pb +\nPb collisions at $\\sqrt{s_{NN}}=5.02$ TeV that are being studied at LHC.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We study the spectra and elliptic flow of deuterons and helium-3 within the framework of the coalescence model based on the phase-space distributions of protons and neutrons generated from the iEBE-VISHNU hybrid model with AMPT initial conditions.\""
    },
    {
        "abstract": "  Errors-in-variables is a long-standing, difficult issue in linear regression;\nand progress depends in part on new identifying assumptions. I characterize\nmeasurement error as bad-leverage points and assume that fewer than half the\nsample observations are heavily contaminated, in which case a high-breakdown\nrobust estimator may be able to isolate and down weight or discard the\nproblematic data. In simulations of simple and multiple regression where eiv\naffects 25% of the data and R-squared is mediocre, certain high-breakdown\nestimators have small bias and reliable confidence intervals.\n",
        "method": "Methodological sentence: I characterize measurement error as bad-leverage points and assume that fewer than half the sample observations are heavily contaminated, in which case a high-breakdown robust estimator may be able to isolate and down weight or discard the problematic data."
    },
    {
        "abstract": "  The Palatini $f(|\\hat{\\Omega}|)$ gravity is a generalized theory of the\nEddington-inspired Born-Infeld gravity, where\n$\\Omega_{~N}^{K}\\equiv\\delta_{~N}^{K}+bg^{KL}R_{LN}(\\Gamma)$ is an auxiliary\ntensor constructed with the spacetime metric $g$ and independent connection\n$\\Gamma$. In this paper, we study $f(|\\hat{\\Omega}|)$ theory with\n$f(|\\hat{\\Omega}|)=|\\hat{\\Omega}|^{\\frac{1}{2}+n}$ in the thick brane scenario\nand give some constraints on the brane model. We finally found an analytic\nsolution of the thick brane generated by a single scalar field. The behavior of\nthe negative energy density denotes the localization of the thick brane at the\norigin of the extra dimension. In our braneworld, the warp factor is divergent\nat the boundary of the extra dimension while the brane system is asymptotically\nanti$-$de Sitter. It is shown that the tensor perturbation of the brane is\nstable and the massless graviton is localized on the thick brane. Therefore,\nthe effective Einstein-Hilbert action on the brane can be rebuilt in the\nlow-energy approximation. According to the recent test of the gravitational\ninverse-square law, we give some constraints on the $f(|\\hat{\\Omega}|)$ brane.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We study $f(|\\hat{\\Omega}|)$ theory with $f(|\\hat{\\Omega}|)=|\\hat{\\Omega}|^{\\frac{1}{2}+n}$ in the thick brane scenario...\n* We finally found an analytic solution of the thick brane generated by a single scalar field."
    },
    {
        "abstract": "  Deep learning and deep architectures are emerging as the best machine\nlearning methods so far in many practical applications such as reducing the\ndimensionality of data, image classification, speech recognition or object\nsegmentation. In fact, many leading technology companies such as Google,\nMicrosoft or IBM are researching and using deep architectures in their systems\nto replace other traditional models. Therefore, improving the performance of\nthese models could make a strong impact in the area of machine learning.\nHowever, deep learning is a very fast-growing research domain with many core\nmethodologies and paradigms just discovered over the last few years. This\nthesis will first serve as a short summary of deep learning, which tries to\ninclude all of the most important ideas in this research area. Based on this\nknowledge, we suggested, and conducted some experiments to investigate the\npossibility of improving the deep learning based on automatic programming\n(ADATE). Although our experiments did produce good results, there are still\nmany more possibilities that we could not try due to limited time as well as\nsome limitations of the current ADATE version. I hope that this thesis can\npromote future work on this topic, especially when the next version of ADATE\ncomes out. This thesis also includes a short analysis of the power of ADATE\nsystem, which could be useful for other researchers who want to know what it is\ncapable of.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* \"Based on this knowledge, we suggested, and conducted some experiments\"\n* \"Although our experiments did produce good results...\""
    },
    {
        "abstract": "  Multiple data sources are becoming increasingly available for statistical\nanalyses in the era of big data. As an important example in finite-population\ninference, we consider an imputation approach to combining a probability sample\nwith big observational data. Unlike the usual imputation for missing data\nanalysis, we create imputed values for the whole elements in the probability\nsample. Such mass imputation is attractive in the context of survey data\nintegration (Kim and Rao, 2012). We extend mass imputation as a tool for data\nintegration of survey data and big non-survey data. The mass imputation methods\nand their statistical properties are presented. The matching estimator of\nRivers (2007) is also covered as a special case. Variance estimation with\nmass-imputed data is discussed. The simulation results demonstrate the proposed\nestimators outperform existing competitors in terms of robustness and\nefficiency.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We consider an imputation approach to combining a probability sample with big observational data.\n* Unlike the usual imputation for missing data analysis, we create imputed values for the whole elements in the probability sample.\n* We extend mass imputation as a tool for data integration of survey data and big non-survey data.\n* The mass imputation methods and their statistical properties are presented.\n* Variance estimation with mass-imputed data is discussed."
    },
    {
        "abstract": "  The interesting properties of Kagome bands, consisting of Dirac bands and a\nflat band, have attracted extensive attention. However, the materials with only\none Kagome band around the Fermi level cannot possess physical properties of\nDirac fermions and strong correlated fermions simultaneously. Here, we propose\na new type of band structure --- double Kagome bands, which can realize\ncoexistence of the two kinds of fermions. Moreover, the new band structure is\nfound to exist in a new two-dimensional material, phosphorus carbide P2C3. The\ncarbide material shows good stability and unusual electronic properties. Strong\nmagnetism appears in the structure by hole doping of the flat band, which\nresults in spin splitting of the Dirac bands. The edge states induced by Dirac\nand flat bands coexist on the Fermi level, indicating outstanding transport\ncharacteristics. In addition, a possible route to experimentally grow P2C3 on\nsome suitable substrates such as the Ag (111) surface is also discussed.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* ...which can realize coexistence of the two kinds of fermions.\n* The new band structure is found to exist in a new two-dimensional material, phosphorus carbide P2C3.\n* A possible route to experimentally grow P2C3 on some suitable substrates such as the Ag (111) surface is also discussed."
    },
    {
        "abstract": "  In this paper we propose a model for open Markov chains that can be\ninterpreted as a system of non-interacting particles evolving according to the\nrules of a Markov chain. The number of particles in the system is not constant,\nbecause we allow the particles to arrive or leave the state space according to\nprescribed protocols. We describe this system by looking at the population of\nparticles on every state by establishing the rules of time-evolution of the\ndistribution of particles. We show that it is possible to describe the\ndistribution of particles over the state space through the corresponding moment\ngenerating function. Such a description is given through the dynamics ruling\nthe behavior of such a moment generating function and we prove that the system\nis able to attain the stationarity under some conditions. We also show that it\nis possible to describe the dynamics of the two first cumulants of the\ndistribution of particles, which in some way is a simpler technique to obtain\nuseful information of the open Markov chain for practical purposes. Finally we\nalso study the behavior of the time-dependent correlation functions of the\nnumber of particles present in the system. We give some simple examples of open\nchains that either, can be fully described through the moment generating\nfunction or partially described through the exact solution of the cumulant\ndynamics.\n",
        "method": "Here are the methodological sentences:\n\n* We describe this system by looking at the population of particles on every state by establishing the rules of time-evolution of the distribution of particles.\n* Such a description is given through the dynamics ruling the behavior of such a moment generating function.\n* We prove that the system is able to attain the stationarity under some conditions."
    },
    {
        "abstract": "  The appeal of lasers can be attributed to both their ubiquitous applications\nand their role as model systems for elucidating nonequilibrium and cooperative\nphenomena. Introducing novel concepts in lasers thus has a potential for both\napplied and fundamental implications. Here we experimentally demonstrate that\nthe coupling between carrier spin and light polarization in common\nsemiconductor lasers can enable room-temperature modulation frequencies above\n200 GHz, exceeding by nearly an order of magnitude the best conventional\nsemiconductor lasers. Surprisingly, this ultrafast operation relies on a short\ncarrier spin relaxation time and a large anisotropy of the refractive index,\nboth commonly viewed as detrimental in spintronics and conventional lasers. Our\nresults overcome the key speed limitations of conventional directly modulated\nlasers and offer a prospect for the next generation of low-energy ultrafast\noptical communication.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Here we experimentally demonstrate...\""
    },
    {
        "abstract": "  Persistence length of dsDNA is known to decrease with increase in ionic\nconcentration of the solution. In contrast to this, here we show that\npersistence length of dsDNA increases dramatically as a function of ionic\nliquid (IL) concentration. Using all atomic explicit solvent molecular dynamics\nsimulations and theoretical models we present, for the first time, a systematic\nstudy to determine the mechanical properties of dsDNA in various hydrated ionic\nliquids at different concentrations. We find that dsDNA in 50 wt% ILs have\nlower persistence length and stretch modulus in comparison to 80 wt% ILs. We\nfurther observe that both persistence length and stretch modulus of dsDNA\nincrease as we increase the ILs concentration. Present trend of stretch modulus\nand persistence length of dsDNA with ILs concentration supports the predictions\nof the macroscopic elastic theory, in contrast to the behavior exhibited by\ndsDNA in monovalent salt. Our study further suggests the preferable ILs that\ncan be used for maintaining DNA stability during long-term storage.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Using all atomic explicit solvent molecular dynamics simulations and theoretical models we present, for the first time, a systematic study to determine the mechanical properties of dsDNA in various hydrated ionic liquids at different concentrations."
    },
    {
        "abstract": "  We consider the Cauchy problem defined for a general class of nonlocal wave\nequations modeling bidirectional wave propagation in a nonlocally and\nnonlinearly elastic medium whose constitutive equation is given by a\nconvolution integral. We prove a long-time existence result for the nonlocal\nwave equations with a power-type nonlinearity and a small parameter. As the\nenergy estimates involve a loss of derivatives, we follow the Nash-Moser\napproach proposed by Alvarez-Samaniego and Lannes. As an application to the\nlong-time existence theorem, we consider the limiting case in which the kernel\nfunction is the Dirac measure and the nonlocal equation reduces to the\ngoverning equation of one-dimensional classical elasticity theory. The present\nstudy also extends our earlier result concerning local well-posedness for\nsmooth kernels to nonsmooth kernels.\n",
        "method": "We follow the Nash-Moser approach proposed by Alvarez-Samaniego and Lannes."
    },
    {
        "abstract": "  Let $C$ be a hyperelliptic curve defined over $\\mathbb{Q}$, whose Weierstrass\npoints are defined over extensions of $\\mathbb{Q}$ of degree at most three, and\nat least one of them is rational. Generalizing a result of R. Soleng (in the\ncase of elliptic curves), we prove that any line bundle of degree $0$ on $C$\nwhich is not torsion can be specialised into ideal classes of imaginary\nquadratic fields whose order can be made arbitrarily large. This gives a\npositive answer, for such curves, to a question by Agboola and Pappas.\n",
        "method": "Methodological sentences:\n\nNone."
    },
    {
        "abstract": "  In this paper, we investigate exact tail asymptotics for the stationary\ndistribution of a fluid model driven by the $M/M/c$ queue, which is a\ntwo-dimensional queueing system with a discrete phase and a continuous level.\nWe extend the kernel method to study tail asymptotics of its stationary\ndistribution, and a total of three types of exact tail asymptotics is\nidentified from our study and reported in the paper.\n",
        "method": "Methodological sentences:\n\n* We extend the kernel method to study tail asymptotics of its stationary distribution..."
    },
    {
        "abstract": "  In this paper, we apply a Lyapunov functional approach to Lotka-Volterra\nsystems with infinite delays and feedback controls and establish that the\nfeedback controls have no influence on the attractivity properties of a\nsaturated equilibrium. This improves previous results by the authors and\nothers, where, while feedback controls were used mostly to change the position\nof a unique saturated equilibrium, additional conditions involving the controls\nhad to be assumed in order to preserve its global attractivity. The situation\nof partial extinction is further analysed, for which the original system is\nreduced to a lower dimensional one which maintains its global dynamics\nfeatures.\n",
        "method": "Methodological sentence:\n\nWe apply a Lyapunov functional approach to Lotka-Volterra systems with infinite delays and feedback controls..."
    },
    {
        "abstract": "  A search for heavy resonances, decaying into the standard model vector bosons\nand the standard model Higgs boson, is presented. The final states considered\ncontain a b quark-antiquark pair from the decay of the Higgs boson, along with\nelectrons and muons and missing transverse momentum, due to undetected\nneutrinos, from the decay of the vector bosons. The mass spectra are used to\nsearch for a localized excess consistent with a resonant particle. The data\nsample corresponds to an integrated luminosity of 35.9 fb$^{-1}$ collected in\n2016 by the CMS experiment at the CERN LHC from proton-proton collisions at a\ncenter-of-mass energy of 13 TeV. The data are found to be consistent with\nbackground expectations. Exclusion limits are set in the context of spin-0 two\nHiggs doublet models, some of which include the presence of dark matter. In the\nspin-1 heavy vector triplet framework, mass-degenerate W' and Z' resonances\nwith dominant couplings to the standard model gauge bosons are excluded below a\nmass of 2.9 TeV at 95% confidence level.\n",
        "method": "The data sample corresponds to an integrated luminosity of 35.9 fb$^{-1}$ collected in 2016 by the CMS experiment at the CERN LHC from proton-proton collisions at a center-of-mass energy of 13 TeV."
    },
    {
        "abstract": "  Rate control at the MAC-layer is one of the fundamental building blocks in\nmany wireless networks. Over the past two decades around thirty mechanisms have\nbeen proposed in the literature. Among them, there are mechanisms that make\nrate selection decisions based on sophisticated measurements of wireless link\nquality, and others that are based on straight-forward heuristics. Minstrel,\nfor example, is an elegant mechanism that has been adopted by hundreds of\nmillions of computers, yet, not much was known about its performance until\nrecently. The purpose of this paper is to provide a comprehensive survey and\nanalysis of existing solutions from the two fundamental aspects of rate control\n- metrics and algorithms. We also review how these solutions were evaluated and\ncompared against each other. Based on our detailed studies and observations, we\nshare important insights on future development of rate control mechanisms at\nthe MAC-layer. This discussion also takes into account the recent developments\nin wireless technologies and emerging applications, such as Internet-of-Things,\nand shows issues that need to be addressed in the design of new rate control\nmechanisms suitable for these technologies and applications.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe also review how these solutions were evaluated and compared against each other."
    },
    {
        "abstract": "  In this note, for the multiplier ideal sheaves with weights\n$\\log\\sum_{i}|z_{i}|^{a_{i}}$, we present the sufficient and necessary\ncondition of the existence of decreasing equisingular approximations with\nanalytic singularities.\n",
        "method": "Here is the methodological sentence:\n\nWe present the sufficient and necessary condition of the existence..."
    },
    {
        "abstract": "  In this paper we explore the potential of stoichiometry determination for\nchalcogenide superlattices, promising candidates for next-generation\nphase-change memory, via X-ray diffraction. To this end, a set of epitaxial\nGeTe/Sb2Te3 superlattice samples with varying layer thicknesses is\nsputter-deposited. Kinematical scattering theory is employed to link the\naverage composition with the diffraction features. The observed lattice\nconstants of the superlattice reference unit cell follow Vegard's law, enabling\na straight-forward and non-destructive stoichiometry determination.\n",
        "method": "A set of epitaxial GeTe/Sb2Te3 superlattice samples with varying layer thicknesses is sputter-deposited."
    },
    {
        "abstract": "  The prevalence of different kinds of electronic devices and the volume of\ncontent on the Web have increased the amount of plagiarism, which is considered\nan unethical act. If we want to be efficient in the detection and prevention of\nthese acts, we have to improve today's methods of discovering plagiarism. The\npaper presents a research study where a framework for the improved detection of\nplagiarism is proposed. The framework focuses on the integration of social\nnetwork information, information from the Web, and an advanced semantically\nenriched visualization of information about authors and documents that enables\nthe exploration of obtained data by seeking of advanced patterns of plagiarism.\nTo support the proposed framework, a special software tool was also developed.\nThe statistical evaluation confirmed that the employment of social network\nanalysis and advanced visualization techniques led to improvements in the\nconfirmation and investigation stages of the plagiarism detection process,\nthereby enhancing the overall efficiency of the plagiarism detection process.\n",
        "method": "To support the proposed framework, a special software tool was also developed."
    },
    {
        "abstract": "  We study a nonlinear Robin problem driven by the $p$-Laplacian and with a\nreaction term depending on the gradient (the convection term). Using the theory\nof nonlinear operators of monotone-type and the asymptotic analysis of a\nsuitable perturbation of the original equation, we show the existence of a\npositive smooth solution.\n",
        "method": "Using the theory of nonlinear operators of monotone-type and the asymptotic analysis of a suitable perturbation of the original equation, we show the existence of a positive smooth solution."
    },
    {
        "abstract": "  In this note, we give an alternative proof of the generating function of\n$p$-Bernoulli numbers. Our argument is based on the Euler's integral\nrepresentation.\n",
        "method": "Our argument is based on the Euler's integral representation."
    },
    {
        "abstract": "  The complex elliptic Ginibre ensemble with coupling $\\tau$ is a complex\nGaussian matrix interpolating between the Gaussian Unitary Ensemble (GUE) and\nthe Ginibre ensemble. It has been known for some time that its eigenvalues form\na determinantal point process in the complex plane. A recent result of Kanazawa\nand Kieburg (arXiv:1804.03985) shows that the singular values form a Pfaffian\npoint process. In this paper we turn to consider an extended elliptic Ginibre\nensemble, which connects the GUE and the spiked Wishart matrix, and prove that\nthe singular values still build a Pfaffian point process with correlation\nkernels expressed by contour integral representations. As $\\tau$ tends to 1 at\na certain critical rate, we prove that the limiting distribution of the largest\nsingular value is described as a new Fredholm Pfaffian series, which connects\ntwo distributions $F_{\\mathrm{GUE}}$ and $F^{2}_{\\mathrm{GUE}}$ where\n$F_{\\mathrm{GUE}}$ is the GUE Tracy-Widom distribution. For fixed $\\tau$, we\nprove the Baik-Ben Arous-P\\'ech\\'e transition of the largest singular value and\nthe sine kernel in the bulk. We also observe a crossover phenomenon at the\norigin when $\\tau$ tends to 1 at another critical rate.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* A recent result of Kanazawa and Kieburg (arXiv:1804.03985) shows that the singular values form a Pfaffian point process.\n* As $\\tau$ tends to 1 at a certain critical rate, we prove that the limiting distribution of the largest singular value is described as a new Fredholm Pfaffian series...\n* For fixed $\\tau$, we prove the Baik-Ben Arous-Pech\u00e9 transition of the largest singular value and the sine kernel in the bulk."
    },
    {
        "abstract": "  Let $S = K[x_1, \\ldots, x_n]$ denote the polynomial ring in $n$ variables\nover a field $K$ with each $\\mathrm{deg}\\ x_i = 1$ and $I \\subset S$ a\nhomogeneous ideal of $S$ with $\\dim S/I = d$. The Hilbert series of $S/I$ is of\nthe form $h_{S/I}(\\lambda)/(1 - \\lambda)^d$, where $h_{S/I}(\\lambda) = h_0 +\nh_1\\lambda + h_2\\lambda^2 + \\cdots + h_s\\lambda^s$ with $h_s \\neq 0$ is the\n$h$-polynomial of $S/I$. Given arbitrary integers $r \\geq 1$ and $s \\geq 1$, a\nlexsegment ideal $I$ of $S = K[x_1, \\ldots, x_n]$, where $n \\leq \\max\\{r, s\\} +\n2$, satisfying $\\mathrm{reg}(S/I) = r$ and $ \\mathrm{deg}\\ h_{S/I}(\\lambda) =\ns$ will be constructed.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe Hilbert series of $S/I$ is of the form $h_{S/I}(\\lambda)/(1 - \\lambda)^d$, where $h_{S/I}(\\lambda) = h_0 + h_1\\lambda + h_2\\lambda^2 + \\cdots + h_s\\lambda^s$ with $h_s \\neq 0$ is the $h$-polynomial of $S/I$."
    },
    {
        "abstract": "  We describe the computation of polytope volumes by descent in the face\nlattice, its implementation in Normaliz, and the connection to\nreverse-lexicographic triangulations. The efficiency of the algorithm is\ndemonstrated by several high dimensional polytopes of different\ncharacteristics. Finally, we present an application to voting theory where\npolytope volumes appear as probabilities of certain paradoxa.\n",
        "method": "We describe the computation of polytope volumes by descent in the face lattice, its implementation in Normaliz, and the connection to reverse-lexicographic triangulations."
    },
    {
        "abstract": "  We report on strongly temperature-dependent kinetics of negatively charged\ncarrier complexes in asymmetric InAs/AlGaInAs/InP quantum dots (dashes)\nemitting at telecom wavelengths. The structures are highly elongated and of\nlarge volume, which results in atypical carrier confinement characteristics\nwith $s$-$p$ shell energy splittings far below the optical phonon energy, which\nstrongly affects the phonon-assisted relaxation. Probing the emission kinetics\nwith time-resolved microphotoluminescence from a single dot, we observe a\nstrongly non-monotonic temperature dependence of the charged exciton lifetime.\nUsing a kinetic rate-equation model, we find that a relaxation side-path\nthrough the excited charged exciton triplet states may lead to such behavior.\nThis, however, involves efficient singlet-triplet relaxation via the electron\nspin-flip. Thus, we interpret the results as an indirect observation of\nstrongly enhanced electron spin relaxation without magnetic field, possibly\nresulting from atypical confinement characteristics.\n",
        "method": "Probing the emission kinetics with time-resolved microphotoluminescence from a single dot, we observe a strongly non-monotonic temperature dependence of the charged exciton lifetime."
    },
    {
        "abstract": "  We consider a critical superprocess $\\{X;\\mathbf P_\\mu\\}$ with general\nspatial motion and spatially dependent stable branching mechanism with lowest\nstable index $\\gamma_0 > 1$. We first show that, under some conditions,\n$\\mathbf P_{\\mu}(\\|X_t\\|\\neq 0)$ converges to $0$ as $t\\to \\infty$ and is\nregularly varying with index $(\\gamma_0-1)^{-1}$. Then we show that, for a\nlarge class of non-negative testing functions $f$, the distribution of\n$\\{X_t(f);\\mathbf P_\\mu(\\cdot|\\|X_t\\|\\neq 0)\\}$, after appropriate rescaling,\nconverges weakly to a positive random variable $\\mathbf z^{(\\gamma_0-1)}$ with\nLaplace transform $E[e^{-u\\mathbf\nz^{(\\gamma_0-1)}}]=1-(1+u^{-(\\gamma_0-1)})^{-1/(\\gamma_0-1)}.$\n",
        "method": "We first show that, under some conditions, $\\mathbf P_{\\mu}(\\|X_t\\|\\neq 0)$ converges to $0$ as $t\\to \\infty$ and is regularly varying with index $(\\gamma_0-1)^{-1}$."
    },
    {
        "abstract": "  The relativistic quantum dynamics of a spinless charged particle interacting\nwith both Aharonov-Bohm and Coulomb-type potentials in the G\\\"odel-type\nspacetime is considered. The dynamics of the system is governed by the\nKlein-Gordon equation with interactions. We verify that it is possible to\nestablish a quantum condition between the energy of the particle and the\nparameter that characterizes the vorticist of the spacetime. We rigorously\nanalyze the ground state of the system and determine the corresponding wave\nfunctions to it.\n",
        "method": "The dynamics of the system is governed by the Klein-Gordon equation with interactions."
    },
    {
        "abstract": "  Despite being very successful within the pattern recognition and machine\nlearning community, graph-based methods are often unusable because of the lack\nof mathematical operations defined in graph domain. Graph embedding, which maps\ngraphs to a vectorial space, has been proposed as a way to tackle these\ndifficulties enabling the use of standard machine learning techniques. However,\nit is well known that graph embedding functions usually suffer from the loss of\nstructural information. In this paper, we consider the hierarchical structure\nof a graph as a way to mitigate this loss of information. The hierarchical\nstructure is constructed by topologically clustering the graph nodes, and\nconsidering each cluster as a node in the upper hierarchical level. Once this\nhierarchical structure is constructed, we consider several configurations to\ndefine the mapping into a vector space given a classical graph embedding, in\nparticular, we propose to make use of the Stochastic Graphlet Embedding (SGE).\nBroadly speaking, SGE produces a distribution of uniformly sampled low to high\norder graphlets as a way to embed graphs into the vector space. In what\nfollows, the coarse-to-fine structure of a graph hierarchy and the statistics\nfetched by the SGE complements each other and includes important structural\ninformation with varied contexts. Altogether, these two techniques\nsubstantially cope with the usual information loss involved in graph embedding\ntechniques, obtaining a more robust graph representation. This fact has been\ncorroborated through a detailed experimental evaluation on various benchmark\ngraph datasets, where we outperform the state-of-the-art methods.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nOnce this hierarchical structure is constructed, we consider several configurations to define the mapping into a vector space given a classical graph embedding, in particular, we propose to make use of the Stochastic Graphlet Embedding (SGE)."
    },
    {
        "abstract": "  Neutron stars can provide new insight into dark matter properties, as these\ndense objects capture dark matter particles very efficiently. It has recently\nbeen shown that the energy transfer in the dark matter capture process can lead\nto appreciable heating of neutron stars, which may be observable with\nforthcoming infra-red telescopes. We examine this heating in the context of\ninelastic dark matter, for which signals in conventional nuclear-recoil based\ndirect detection experiments are highly suppressed when the momentum transfer\nis small compared to the mass splitting between dark matter states. Neutron\nstars permit inelastic scattering for much greater mass splittings, because\ndark matter particles are accelerated to velocities close to the speed of light\nduring infall. Using an effective operator approach for fermionic DM that\nscatters inelastically, we show that the observation of a very cold neutron\nstar would lead to very stringent limits on the interaction strengths that, in\nmost cases, much stronger than any present, or future, direct detection\nexperiment on Earth. This holds both for elastic scattering and for inelastic\nscattering with mass splittings up to $\\sim 300 MeV$.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We examine this heating in the context of inelastic dark matter, ...\n* Using an effective operator approach for fermionic DM that scatters inelastically, ..."
    },
    {
        "abstract": "  Consider a germ $C$ of reduced curve on a smooth germ $S$ of complex analytic\nsurface. Assume that $C$ contains a smooth branch $L$. Using the Newton-Puiseux\nseries of $C$ relative to any coordinate system $(x,y)$ on $S$ such that $L$ is\nthe $y$-axis, one may define the {\\em Eggers-Wall tree} $\\Theta_L(C)$ of $C$\nrelative to $L$. Its ends are labeled by the branches of $C$ and it is endowed\nwith three natural functions measuring the characteristic exponents of the\nprevious Newton-Puiseux series, their denominators and contact orders. The main\nobjective of this paper is to embed canonically $\\Theta_L(C)$ into Favre and\nJonsson's valuative tree $\\mathbb{P}(\\mathcal{V})$ of real-valued\nsemivaluations of $S$ up to scalar multiplication, and to show that this\nembedding identifies the three natural functions on $\\Theta_L(C)$ as pullbacks\nof other naturally defined functions on $\\mathbb{P}(\\mathcal{V})$. As a\nconsequence, we prove an inversion theorem generalizing the well-known\nAbhyankar-Zariski inversion theorem concerning one branch: if $L'$ is a second\nsmooth branch of $C$, then the valuative embeddings of the Eggers-Wall trees\n$\\Theta_{L'}(C)$ and $\\Theta_L(C)$ identify them canonically, their associated\ntriples of functions being easily expressible in terms of each other. We prove\nalso that the space $\\mathbb{P}(\\mathcal{V})$ is the projective limit of\nEggers-Wall trees over all choices of curves $C$. As a supplementary result, we\nexplain how to pass from $\\Theta_L(C)$ to an associated splice diagram.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\nNone found. The abstract appears to be a summary of the paper's main objectives and results, with no explicit mention of methods or approaches used in the research."
    },
    {
        "abstract": "  Region-based convolutional neural networks\n(R-CNN)~\\cite{fast_rcnn,faster_rcnn,mask_rcnn} have largely dominated object\ndetection. Operators defined on RoIs (Region of Interests) play an important\nrole in R-CNNs such as RoIPooling~\\cite{fast_rcnn} and\nRoIAlign~\\cite{mask_rcnn}. They all only utilize information inside RoIs for\nRoI prediction, even with their recent deformable\nextensions~\\cite{deformable_cnn}. Although surrounding context is well-known\nfor its importance in object detection, it has yet been integrated in R-CNNs in\na flexible and effective way. Inspired by the auto-context\nwork~\\cite{auto_context} and the multi-class object layout\nwork~\\cite{nms_context}, this paper presents a generic context-mining RoI\noperator (i.e., \\textit{RoICtxMining}) seamlessly integrated in R-CNNs, and the\nresulting object detection system is termed \\textbf{Auto-Context R-CNN} which\nis trained end-to-end. The proposed RoICtxMining operator is a simple yet\neffective two-layer extension of the RoIPooling or RoIAlign operator. Centered\nat an object-RoI, it creates a $3\\times 3$ layout to mine contextual\ninformation adaptively in the $8$ surrounding context regions on-the-fly.\nWithin each of the $8$ context regions, a context-RoI is mined in term of\ndiscriminative power and its RoIPooling / RoIAlign features are concatenated\nwith the object-RoI for final prediction. \\textit{The proposed Auto-Context\nR-CNN is robust to occlusion and small objects, and shows promising\nvulnerability for adversarial attacks without being adversarially-trained.} In\nexperiments, it is evaluated using RoIPooling as the backbone and shows\ncompetitive results on Pascal VOC, Microsoft COCO, and KITTI datasets\n(including $6.9\\%$ mAP improvements over the R-FCN~\\cite{rfcn} method on COCO\n\\textit{test-dev} dataset and the first place on both KITTI pedestrian and\ncyclist detection as of this submission).\n",
        "method": "Here is the methodological sentence:\n\nThe proposed RoICtxMining operator is a simple yet effective two-layer extension of the RoIPooling or RoIAlign operator."
    },
    {
        "abstract": "  For a locally compact, totally disconnected group $G$, a subgroup $H$ and a\ncharacter $\\chi:H \\to \\mathbb{C}^{\\times}$ we define a Hecke algebra\n$\\mathcal{H}_\\chi$ and explore the connection between commutativity of\n$\\mathcal{H}_\\chi$ and the $\\chi$-Gelfand property of $(G,H)$, i.e. the\nproperty $\\mathrm{dim}_\\mathbb{C}(\\rho^*)^{(H,\\chi^{-1})} \\leq 1$ for every\n$\\rho \\in \\mathrm{Irr}(G)$, the irreducible representations of $G$.\n  We show that the conditions of the Gelfand-Kazhdan criterion imply\ncommutativity of $\\mathcal{H}_\\chi$, and verify in several simple cases that\ncommutativity of $\\mathcal{H}_\\chi$ is equivalent to the $\\chi$-Gelfand\nproperty of $(G,H)$.\n  We then show that if $G$ is a connected reductive group over a $p$-adic field\n$F$, and $G/H$ is $F$-spherical, then the cuspidal part of $\\mathcal{H}_\\chi$\nis commutative if and only if $(G,H)$ satisfies the $\\chi$-Gelfand property\nwith respect to all cuspidal representations ${\\rho \\in \\mathrm{Irr}(G)}$.\n  We conclude by showing that if $(G,H)$ satisfies the $\\chi$-Gelfand property\nwith respect to all irreducible $(H,\\chi^{-1})$-tempered representations of $G$\nthen $\\mathcal{H}_\\chi$ is commutative.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We show that...\n* We then show that...\n* We conclude by showing that..."
    },
    {
        "abstract": "  We investigate the discrete Fuglede's conjecture and Pompeiu problem on\nfinite abelian groups and develop a strong connection between the two problems.\nWe give a geometric condition under which a multiset of a finite abelian group\nhas the discrete Pompeiu property. Using this description and the revealed\nconnection we prove that Fuglede's conjecture holds for $\\mathbb{Z}_{p^n q^2}$,\nwhere $p$ and $q$ are different primes. In particular, we show that every\nspectral subset of $\\mathbb{Z}_{p^n q^2}$ tiles the group. Further, using our\ncombinatorial methods we give a simple proof for the statement that Fuglede's\nconjecture holds for $\\mathbb{Z}_p^2$.\n",
        "method": "Here are the methodological sentences:\n\n* We develop a strong connection between the two problems.\n* Using this description and the revealed connection we prove that Fuglede's conjecture holds for $\\mathbb{Z}_{p^n q^2}$, where $p$ and $q$ are different primes."
    },
    {
        "abstract": "  Aims: In this work, we aim to provide a reliable list of gravitational lens\n(GL) candidates based on a search performed over the entire Gaia Data Release 2\n(Gaia DR2). We also show that the sole astrometric and photometric informations\ncoming from the Gaia satellite yield sufficient insights for supervised\nlearning methods to automatically identify GL candidates with an efficiency\nthat is comparable to methods based on image processing. Methods: We simulated\n106,623,188 lens systems composed of more than two images, based on a regular\ngrid of parameters characterizing a non-singular isothermal ellipsoid lens\nmodel in the presence of an external shear. These simulations are used as an\ninput for training and testing our supervised learning models consisting of\nExtremely Randomized Trees. The latter are finally used to assign to each of\nthe 2,129,659 clusters of celestial objects a discriminant value that reflects\nthe ability of our simulations to match the observed relative positions and\nfluxes from each cluster. Once complemented with additional constraints, these\ndiscriminant values allowed us to identify GL candidates out of the list of\nclusters. Results: We report the discovery of 15 new quadruply-imaged lens\ncandidates with angular separations less than 6\" and assess the performance of\nour approach by recovering 12 out of the 13 known quadruply-imaged systems with\nall their components detected in Gaia DR2 with a misclassification rate of\nfortuitous clusters of stars as lens systems that is below one percent.\nSimilarly, the identification capability of our method regarding\nquadruply-imaged systems where three images are detected in Gaia DR2 is\nassessed by recovering 10 out of the 13 known quadruply-imaged systems having\none of their constituting images discarded. The associated misclassification\nrate varying then between 5.8% and 20%, depending on the image we decided to\nremove.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We simulated 106,623,188 lens systems composed of more than two images, based on a regular grid of parameters characterizing a non-singular isothermal ellipsoid lens model in the presence of an external shear.\""
    },
    {
        "abstract": "  Internet of Things (IoT) data are increasingly viewed as a new form of\nmassively distributed and large scale digital assets, which are continuously\ngenerated by millions of connected devices. The real value of such assets can\nonly be realized by allowing IoT data trading to occur on a marketplace that\nrewards every single producer and consumer, at a very granular level.\nCrucially, we believe that such a marketplace should not be owned by anybody,\nand should instead fairly and transparently self-enforce a well defined set of\ngovernance rules. In this paper we address some of the technical challenges\ninvolved in realizing such a marketplace. We leverage emerging blockchain\ntechnologies to build a decentralized, trusted, transparent and open\narchitecture for IoT traffic metering and contract compliance, on top of the\nlargely adopted IoT brokered data infrastructure. We discuss an Ethereum-based\nprototype implementation and experimentally evaluate the overhead cost\nassociated with Smart Contract transactions, concluding that a viable business\nmodel can indeed be associated with our technical approach.\n",
        "method": "We leverage emerging blockchain technologies to build a decentralized, trusted, transparent and open architecture for IoT traffic metering and contract compliance, on top of the largely adopted IoT brokered data infrastructure."
    },
    {
        "abstract": "  Due to atomically thin structure, graphene/hexagonal boron nitride (G/hBN)\nheterostructures are intensively sensitive to the external mechanical forces\nand deformations being applied to their lattice structure. In particular,\nstrain can lead to the modification of the electronic properties of G/hBN.\nFurthermore, moir\\'e structures driven by misalignment of graphene and hBN\nlayers introduce new features to the electronic behavior of G/hBN. Utilizing\n{\\it ab initio} calculation, we study the strain-induced modification of the\nelectronic properties of diverse stacking faults of G/hBN when applying\nin-plane strain on both layers, simultaneously. We observe that the interplay\nof few percent magnitude in-plane strain and moir\\'e pattern in the\nexperimentally applicable systems leads to considerable valley drifts, band gap\nmodulation and enhancement of the substrate-induced Fermi velocity\nrenormalization. Furthermore, we find that regardless of the strain alignment,\nthe zigzag direction becomes more efficient for electronic transport, when\napplying in-plane non-equibiaxial strains.\n",
        "method": "Utilizing {\\it ab initio} calculation, we study the strain-induced modification of the electronic properties of diverse stacking faults of G/hBN when applying in-plane strain on both layers, simultaneously."
    },
    {
        "abstract": "  We present a complete calculation of nucleon-deuteron scattering as well as\nground and low-lying excited states of light nuclei in the mass range A=3-16 up\nthrough next-to-next-to-leading order in chiral effective field theory using\nsemilocal coordinate-space regularized two- and three-nucleon forces. It is\nshown that both of the low-energy constants entering the three-nucleon force at\nthis order can be reliably determined from the triton binding energy and the\ndifferential cross section minimum in elastic nucleon-deuteron scattering. The\ninclusion of the three-nucleon force is found to improve the agreement with the\ndata for most of the considered observables.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe present a complete calculation of nucleon-deuteron scattering as well as ground and low-lying excited states of light nuclei in the mass range A=3-16 up through next-to-next-leading order in chiral effective field theory using semilocal coordinate-space regularized two- and three-nucleon forces."
    },
    {
        "abstract": "  In this paper, we determine the spectrum, the point spectrum, the continuous\nspectrum and the residual spectrum of the generalized difference operator\n$\\Delta_{a,b}$ on the sequence space $\\ell_p \\ (1< p < \\infty)$ where the real\nsequences $a=\\{a_k\\}$ and $b=\\{b_k\\}$ are not necessarily convergent. Hence our\nresults generalize the work given by Akhmedov and El-Shabrawy [Math. Slovaca\n65~(5) (2015) 1137--1152] for the sequence space $\\ell_p (1< p <\\infty)$.\n",
        "method": "The methodological sentence is:\n\nWe determine the spectrum, the point spectrum, the continuous spectrum and the residual spectrum of the generalized difference operator $\\Delta_{a,b}$ on the sequence space $\\ell_p (1 < p < \\infty)$."
    },
    {
        "abstract": "  A new forecasting method based on the concept of the profile predictive the\nlikelihood function is proposed for discrete-valued processes. In particular,\ngeneralized autoregressive and moving average (GARMA) models for Poisson\ndistributed data are explored in details. Highest density regions are used to\nconstruct forecasting regions. The proposed forecast estimates and regions are\ncoherent. Large sample results are derived for the forecasting distribution.\nNumerical studies using simulations and a real data set are used to establish\nthe performance of the proposed forecasting method. Robustness of the proposed\nmethod to possible misspecification in the model is also studied.\n",
        "method": "Methodological sentences:\n\n* A new forecasting method based on the concept of the profile predictive likelihood function is proposed for discrete-valued processes.\n* Highest density regions are used to construct forecasting regions.\n* Large sample results are derived for the forecasting distribution.\n* Numerical studies using simulations and a real data set are used to establish the performance of the proposed forecasting method."
    },
    {
        "abstract": "  Clustering is crucial for many computer vision applications such as robust\ntracking, object detection and segmentation. This work presents a real-time\nclustering technique that takes advantage of the unique properties of\nevent-based vision sensors. Since event-based sensors trigger events only when\nthe intensity changes, the data is sparse, with low redundancy. Thus, our\napproach redefines the well-known mean-shift clustering method using\nasynchronous events instead of conventional frames. The potential of our\napproach is demonstrated in a multi-target tracking application using Kalman\nfilters to smooth the trajectories. We evaluated our method on an existing\ndataset with patterns of different shapes and speeds, and a new dataset that we\ncollected. The sensor was attached to the Baxter robot in an eye-in-hand setup\nmonitoring real-world objects in an action manipulation task. Clustering\naccuracy achieved an F-measure of 0.95, reducing the computational cost by 88%\ncompared to the frame-based method. The average error for tracking was 2.5\npixels and the clustering achieved a consistent number of clusters along time.\n",
        "method": "This work presents a real-time clustering technique that redefines the well-known mean-shift clustering method using asynchronous events instead of conventional frames."
    },
    {
        "abstract": "  Quantum non-locality is normally defined via violations of Bell's\ninequalities that exclude certain classical hidden variable theories from\nexplaining quantum correlations. Another definition of non-locality refers to\nthe wave-function collapse thereby one can prepare a quantum state from\narbitrary far away. In both cases one can debate on whether non-locality is a\nreal physical phenomenon, e.g. one can employ formulations of quantum mechanics\nthat does not use collapse, or one can simply refrain from explaining quantum\ncorrelations via classical hidden variables. Here we point out that there is a\nnon-local effect within quantum mechanics, i.e. without involving hidden\nvariables or collapse. This effect is seen via imprecise (i.e. interval-valued)\njoint probability of two observables, which replaces the ill-defined notion of\nthe precise joint probability for non-commuting observables. It is consistent\nwith all requirements for the joint probability, e.g. those for commuting\nobservales. The non-locality amounts to a fact that (in a two-particle system)\nthe joint imprecise probability of non-commuting two-particle observables (i.e.\ntensor product of single-particle observables) does not factorize into\nsingle-particle contributions, even for uncorrelated states of the two-particle\nsystem. The factorization is recovered for a less precise (i.e. the one\ninvolving a wider interval) joint probability. This approach to non-locality\nreconciles it with locality, since the latter emerges as a less precise\ndescription.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In both cases one can debate on whether non-locality is a real physical phenomenon, e.g. one can employ formulations of quantum mechanics that does not use collapse, or one can simply refrain from explaining quantum correlations via classical hidden variables.\n* It is consistent with all requirements for the joint probability, e.g. those for commuting observables.\n\nNote: These sentences describe the methods or approaches used in the research, including the formulation of quantum mechanics and the consistency with requirements for joint probabilities."
    },
    {
        "abstract": "  The simultaneous detection of gravitational and electromagnetic waves from a\nbinary neutron star merger has both solidified the link between neutron star\nmergers and short-duration gamma-ray bursts (GRBs) and demonstrated the ability\nof astronomers to follow-up the gravitational wave detection to place\nconstraints on the ejecta from these mergers as well as the nature of the GRB\nengine and its surroundings. As the sensitivity of aLIGO and VIRGO increases,\nit is likely that a growing number of such detections will occur in the next\nfew years, leading to a sufficiently-large number of events to constrain the\npopulations of these GRB events. While long-duration GRBs originate from\nmassive stars and thus are located near their stellar nurseries, binary neutron\nstars may merge on much longer timescales, and thus may have had time to\nmigrate appreciably. The strength and character of the electromagnetic\nafterglow emission of binary neutron star mergers is a sensitive function of\nthe circum-merger environment. Though the explosion sites of short GRBs have\nbeen explored in the literature, the question has yet to be fully addressed in\nits cosmological context. We present cosmological simulations following the\nevolution of a galaxy cluster including star formation combined with binary\npopulation synthesis models to self-consistently track the locations and\nenvironmental gas densities of compact binary merger sites throughout the\ncosmic web. We present probability distributions for densities as a function of\nredshift and discuss model sensitivity to population synthesis model\nassumptions.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We present cosmological simulations following the evolution of a galaxy cluster including star formation combined with binary population synthesis models to self-consistently track the locations and environmental gas densities of compact binary merger sites throughout the cosmic web.\""
    },
    {
        "abstract": "  The goal of our industrial ticketing system is to retrieve a relevant\nsolution for an input query, by matching with historical tickets stored in\nknowledge base. A query is comprised of subject and description, while a\nhistorical ticket consists of subject, description and solution. To retrieve a\nrelevant solution, we use textual similarity paradigm to learn similarity in\nthe query and historical tickets. The task is challenging due to significant\nterm mismatch in the query and ticket pairs of asymmetric lengths, where\nsubject is a short text but description and solution are multi-sentence texts.\nWe present a novel Replicated Siamese LSTM model to learn similarity in\nasymmetric text pairs, that gives 22% and 7% gain (Accuracy@10) for retrieval\ntask, respectively over unsupervised and supervised baselines. We also show\nthat the topic and distributed semantic features for short and long texts\nimproved both similarity learning and retrieval.\n",
        "method": "To retrieve a relevant solution, we use textual similarity paradigm to learn similarity in the query and historical tickets."
    },
    {
        "abstract": "  This paper introduces a semi-parametric approach to image inpainting for\nirregular holes. The nonparametric part consists of an external image database.\nDuring test time database is used to retrieve a supplementary image, similar to\nthe input masked picture, and utilize it as auxiliary information for the deep\nneural network. Further, we propose a novel method of generating masks with\nirregular holes and present public dataset with such masks. Experiments on\nCelebA-HQ dataset show that our semi-parametric method yields more realistic\nresults than previous approaches, which is confirmed by the user study.\n",
        "method": "Here is the methodological sentence:\n\n\"During test time database is used to retrieve a supplementary image, similar to the input masked picture, and utilize it as auxiliary information for the deep neural network.\""
    },
    {
        "abstract": "  In this paper, we first address adverse effects of cyber-physical attacks on\ndistributed synchronization of multi-agent systems, by providing conditions\nunder which an attacker can destabilize the underlying network, as well as\nanother set of conditions under which local neighborhood tracking errors of\nintact agents converge to zero. Based on this analysis, we propose a\nKullback-Liebler divergence based criterion in view of which each agent detects\nits neighbors' misbehavior and, consequently, forms a self-belief about the\ntrustworthiness of the information it receives. Agents continuously update\ntheir self-beliefs and communicate them with their neighbors to inform them of\nthe significance of their outgoing information. Moreover, if the self-belief of\nan agent is low, it forms trust on its neighbors. Agents incorporate their\nneighbors' self-beliefs and their own trust values on their control protocols\nto slow down and mitigate attacks. We show that using the proposed resilient\napproach, an agent discards the information it receives from a neighbor only if\nits neighbor is compromised, and not solely based on the discrepancy among\nneighbors' information, which might be caused by legitimate changes, and not\nattacks. The proposed approach is guaranteed to work under mild connectivity\nassumptions.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Based on this analysis, we propose a Kullback-Liebler divergence based criterion...\n* Agents continuously update their self-beliefs and communicate them with their neighbors...\n* Moreover, if the self-belief of an agent is low, it forms trust on its neighbors."
    },
    {
        "abstract": "  Recurrent Networks are one of the most powerful and promising artificial\nneural network algorithms to processing the sequential data such as natural\nlanguages, sound, time series data. Unlike traditional feed-forward network,\nRecurrent Network has a inherent feed back loop that allows to store the\ntemporal context information and pass the state of information to the entire\nsequences of the events. This helps to achieve the state of art performance in\nmany important tasks such as language modeling, stock market prediction, image\ncaptioning, speech recognition, machine translation and object tracking etc.,\nHowever, training the fully connected RNN and managing the gradient flow are\nthe complicated process. Many studies are carried out to address the mentioned\nlimitation. This article is intent to provide the brief details about recurrent\nneurons, its variances and trips & tricks to train the fully recurrent neural\nnetwork. This review work is carried out as a part of our IPO studio software\nmodule 'Multiple Object Tracking'.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Recurrent Network has a inherent feed back loop that allows to store the temporal context information and pass the state of information to the entire sequences of the events.\n* Training the fully connected RNN and managing the gradient flow are the complicated process."
    },
    {
        "abstract": "  The Hamiltonian of a linearly driven two-level system, or qubit, in the\nstandard rotating frame contains non-commuting terms that oscillate at twice\nthe drive frequency, $\\omega$, rendering the task of analytically finding the\nqubit's time evolution nontrivial. The application of the rotating wave\napproximation (RWA), which is suitable only for drives whose amplitude, or\nenvelope, $H_1(t)$, is small compared to $\\omega$ and varies slowly on the time\nscale of $1/\\omega$, yields a simple Hamiltonian that can be integrated\nrelatively easily. We present a series of corrections to the RWA Hamiltonian in\n$1/\\omega$, resulting in an effective Hamiltonian whose time evolution is\naccurate also for time-dependent drive envelopes in the regime of strong\ndriving, i.e., for $|H_1(t)| \\lesssim \\omega$. By extending the Magnus\nexpansion with the use of a Taylor series we introduce a method that we call\nthe Magnus-Taylor expansion, which we use to derive a recurrence relation for\ncomputing the effective Hamiltonian. We then employ the same method to derive\nkick operators, which complete our theory for non-smooth drives. The time\nevolution generated by our kick operators and effective Hamiltonian, both of\nwhich depend explicitly on the envelope and its time derivatives, agrees with\nthe exact time evolution at periodic points in time. For the leading\nHamiltonian correction we obtain a term proportional to the first derivative of\nthe envelope, which competes with the Bloch-Siegert shift.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The application of the rotating wave approximation (RWA), which is suitable only for drives whose amplitude, or envelope, $H_1(t)$, is small compared to $\\omega$ and varies slowly on the time scale of $1/\\omega$, yields a simple Hamiltonian that can be integrated relatively easily.\n* By extending the Magnus expansion with the use of a Taylor series we introduce a method that we call the Magnus-Taylor expansion, which we use to derive a recurrence relation for computing the effective Hamiltonian.\n* We then employ the same method to derive kick operators, which complete our theory for non-smooth drives."
    },
    {
        "abstract": "  The merger rate of stellar-mass black hole binaries (sBHBs) inferred by the\nAdvanced Laser Interferometer Gravitational-Wave Observatory (LIGO) suggests\nthe need for an efficient source of sBHB formation. Active galactic nucleus\n(AGN) disks are a promising location for the formation of these sBHBs, as well\nas binaries of other compact objects, because of powerful torques exerted by\nthe gas disk. These gas torques cause orbiting compact objects to migrate\ntowards regions in the disk where inward and outward torques cancel, known as\nmigration traps. We simulate the migration of stellar mass black holes in an\nexample of a model AGN disk, using an augmented N-body code that includes\nanalytic approximations to migration torques, stochastic gravitational forces\nexerted by turbulent density fluctuations in the disk, and inclination and\neccentricity dampening produced by passages through the gas disk, in addition\nto the standard gravitational forces between objects. We find that sBHBs form\nrapidly in our model disk as stellar-mass black holes migrate towards the\nmigration trap. These sBHBs are likely to subsequently merge on short\ntime-scales. The process continues, leading to the build-up of a population of\nover-massive stellar-mass black holes. The formation of sBHBs in AGN disks\ncould contribute significantly to the sBHB merger rate inferred by LIGO.\n",
        "method": "Here is the methodological sentence:\n\nWe simulate the migration of stellar mass black holes in an example of a model AGN disk, using an augmented N-body code that includes analytic approximations to migration torques, stochastic gravitational forces exerted by turbulent density fluctuations in the disk, and inclination and eccentricity dampening produced by passages through the gas disk, in addition to the standard gravitational forces between objects."
    },
    {
        "abstract": "  We obtained constraints on a 12 parameter extended cosmological scenario\nincluding non-phantom dynamical dark energy (NPDDE) with CPL parametrization.\nWe also include the six $\\Lambda$CDM parameters, number of relativistic\nneutrino species ($N_{\\textrm{eff}}$) and sum over active neutrino masses\n($\\sum m_{\\nu}$), tensor-to-scalar ratio ($r_{0.05}$), and running of the\nspectral index ($n_{run}$). We use CMB Data from Planck 2015; BAO Measurements\nfrom SDSS BOSS DR12, MGS, and 6dFS; SNe Ia Luminosity Distance measurements\nfrom the Pantheon Sample; CMB B-mode polarization data from BICEP2/Keck\ncollaboration (BK14); Planck lensing data; and a prior on Hubble constant\n($73.24\\pm1.74$ km/sec/Mpc) from local measurements (HST). We have found strong\nbounds on the sum of the active neutrino masses. For instance, a strong bound\nof $\\sum m_{\\nu} <$ 0.123 eV (95\\% C.L.) comes from Planck+BK14+BAO. Although\nwe are in such an extended parameter space, this bound is stronger than a bound\nof $\\sum m_{\\nu} <$ 0.158 eV (95\\% C.L.) obtained in $\\Lambda \\textrm{CDM}+\\sum\nm_{\\nu}$ with Planck+BAO. Varying $A_{\\textrm{lens}}$ instead of $r_{0.05}$\nhowever leads to weaker bounds on $\\sum m_{\\nu}$. Inclusion of the HST leads to\nthe standard value of $N_{\\textrm{eff}} = 3.045$ being discarded at more than\n68\\% C.L., which increases to 95\\% C.L. when we vary $A_{\\textrm{lens}}$\ninstead of $r_{0.05}$, implying a small preference for dark radiation, driven\nby the $H_0$ tension.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We use CMB Data from Planck 2015; BAO Measurements from SDSS BOSS DR12, MGS, and 6dFS; SNe Ia Luminosity Distance measurements from the Pantheon Sample; CMB B-mode polarization data from BICEP2/Keck collaboration (BK14); Planck lensing data; and a prior on Hubble constant ($73.24\\pm1.74$ km/sec/Mpc) from local measurements (HST).\""
    },
    {
        "abstract": "  Resource allocation is of great importance in the next generation wireless\ncommunication systems, especially for cognitive radio networks (CRNs). Many\nresource allocation strategies have been proposed to optimize the performance\nof CRNs. However, it is challenging to implement these strategies and achieve\nreal-time performance in wireless systems since most of them need accurate and\ntimely channel state information and/or other network statistics. In this paper\na resource allocation strategy based on deep neural networks (DNN) is proposed\nand the training method is presented to train the neural networks. Simulation\nresults show that our proposed strategy based on DNN is efficient in terms of\nthe computation time compared with the conventional resource allocation\nschemes.\n",
        "method": "However, it is challenging to implement these strategies and achieve real-time performance in wireless systems since most of them need accurate and timely channel state information and/or other network statistics."
    },
    {
        "abstract": "  Consider $L$ groups of point sources or spike trains, with the\n$l^{\\text{th}}$ group represented by $x_l(t)$. For a function $g:\\mathbb{R}\n\\rightarrow \\mathbb{R}$, let $g_l(t) = g(t/\\mu_l)$ denote a point spread\nfunction with scale $\\mu_l > 0$, and with $\\mu_1 < \\cdots < \\mu_L$. With $y(t)\n= \\sum_{l=1}^{L} (g_l \\star x_l)(t)$, our goal is to recover the source\nparameters given samples of $y$, or given the Fourier samples of $y$. This\nproblem is a generalization of the usual super-resolution setup wherein $L =\n1$; we call this the multi-kernel unmixing super-resolution problem. Assuming\naccess to Fourier samples of $y$, we derive an algorithm for this problem for\nestimating the source parameters of each group, along with precise\nnon-asymptotic guarantees. Our approach involves estimating the group\nparameters sequentially in the order of increasing scale parameters, i.e., from\ngroup $1$ to $L$. In particular, the estimation process at stage $1 \\leq l \\leq\nL$ involves (i) carefully sampling the tail of the Fourier transform of $y$,\n(ii) a \\emph{deflation} step wherein we subtract the contribution of the groups\nprocessed thus far from the obtained Fourier samples, and (iii) applying\nMoitra's modified Matrix Pencil method on a deconvolved version of the samples\nin (ii).\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Assuming access to Fourier samples of $y$, we derive an algorithm for this problem for estimating the source parameters of each group, along with precise non-asymptotic guarantees.\n* Our approach involves estimating the group parameters sequentially in the order of increasing scale parameters, i.e., from group 1 to L.\n* In particular, the estimation process at stage 1 \u2264 l \u2264 L involves (i) carefully sampling the tail of the Fourier transform of y, (ii) a deflation step wherein we subtract the contribution of the groups processed thus far from the obtained Fourier samples, and (iii) applying Moitra's modified Matrix Pencil method on a deconvolved version of the samples in (ii)."
    },
    {
        "abstract": "  This study combines a one-dimensional (1D) model with micro-CT imaging and\nhemodynamic data to quantify uncertainty of flow and pressure predictions in\nthe pulmonary arteries in a control and hypoxia induced hypertensive mouse. We\nuse local and global sensitivity and correlation analysis to determine\nparameters that can be inferred from the model and available data. Least\nsquares optimization is used to estimate mouse specific parameters, and\nBayesian as well as asymptotic uncertainty quantification techniques are\nemployed to determine confidence, credible, and prediction intervals for the\nmodel parameters and response. These techniques are used to examine the effects\nof network size and to understand how parameters change with disease\n(hypertension). Results showed that the peripheral vascular resistance is the\nmost sensitive, and as the network size increases the parameter behavior\nchanges. Correlation analysis revealed that in hypertension large vessel\nstiffness is correlated with proximal resistance in the boundary. We were able\nto estimate identifiable parameters using both deterministic and Bayesian\ntechniques (the maxima of the parameter distributions determined using Bayesian\nanalysis aligned with local optima). From these estimates we determined\nconfidence and prediction intervals, which all were within physiological\nexpectation. Analysis of estimated parameter values for the representative mice\nstudied here showed that the hypertensive mouse has stiffer (but larger)\nvessels and that compliance is decreased both in the proximal and peripheral\nvasculature.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We use local and global sensitivity and correlation analysis to determine parameters that can be inferred from the model and available data.\n* Least squares optimization is used to estimate mouse specific parameters, and Bayesian as well as asymptotic uncertainty quantification techniques are employed to determine confidence, credible, and prediction intervals for the model parameters and response."
    },
    {
        "abstract": "  We discuss expectation values of the twist operator $U$ appearing in the\nLieb-Schultz-Mattis theorem (or the polarization operator for periodic systems)\nin excited states of the one-dimensional correlated systems\n$z_L^{(q,\\pm)}\\equiv\\braket{\\Psi_{q/2}^{\\pm}|U^q|\\Psi_{q/2}^{\\pm}}$, where\n$\\ket{\\Psi_{p}^{\\pm}}$ denotes the excited states given by linear combinations\nof momentum $2pk_{\\rm F}$ with parity $\\pm 1$. We found that $z_L^{(q,\\pm)}$\ngives universal values $\\pm 1/2$ on the Tomonaga-Luttinger (TL) fixed point,\nand its signs identify the topology of the dominant phases. Therefore, this\nexpectation value changes between $\\pm 1/2$ discontinuously at a phase\ntransition point with the U(1) or SU(2) symmetric Gaussian universality class.\nThis means that $z_L^{(q,\\pm)}$ extracts the topological information of TL\nliquids. We explain these results based on the free-fermion picture and the\nbosonization theory, and also demonstrate them in several physical systems.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We found that $z_L^{(q,\\pm)}$ gives universal values $\\pm 1/2$ on the Tomonaga-Luttinger (TL) fixed point...\""
    },
    {
        "abstract": "  The $P$-partition generating function of a (naturally labeled) poset $P$ is a\nquasisymmetric function enumerating order-preserving maps from $P$ to\n$\\mathbb{Z}^+$. Using the Hopf algebra of posets, we give necessary conditions\nfor two posets to have the same generating function. In particular, we show\nthat they must have the same number of antichains of each size, as well as the\nsame shape (as defined by Greene). We also discuss which shapes guarantee\nuniqueness of the $P$-partition generating function and give a method of\nconstructing pairs of non-isomorphic posets with the same generating function.\n",
        "method": "Here is the methodological sentence:\n\nUsing the Hopf algebra of posets, we give necessary conditions for two posets to have the same generating function."
    },
    {
        "abstract": "  In light of the joint multimessenger detection of a binary neutron star\nmerger as the gamma-ray burst GRB 170817A and in gravitational waves as\nGW170817, we reanalyze the Fermi Gamma-ray Burst Monitor data of one of the\nclosest short gamma-ray bursts: GRB 150101B. We find this burst is composed of\na short hard spike followed by a comparatively long soft tail. This apparent\ntwo-component nature is phenomenologically similar to that of GRB 170817A.\nWhile GRB 170817A was distinct from the previously known population of short\ngamma-ray bursts in terms of its prompt intrinsic energetics, GRB 150101B is\nnot. Despite these differences, GRB 150101B can be modeled as a more on-axis\nversion of GRB 170817A. Identifying a similar signature in two of the closest\nshort gamma-ray bursts suggests the soft tail is common, but generally\nundetectable in more distant events. If so, it will be possible to identify\nnearby short gamma-ray bursts from the prompt gamma-ray emission alone, aiding\nthe search for kilonovae.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We reanalyze the Fermi Gamma-ry Burst Monitor data of one of the closest short gamma-ry bursts: GRB 150101B."
    },
    {
        "abstract": "  Let $\\mathcal{A}$ be a smooth proper C-linear triangulated category\nCalabi-Yau of dimension 3 endowed with a (non-trivial) rank function. Using the\nhomological unit of $\\mathcal{A}$ with respect to the given rank function, we\ndefine Hodge numbers for $\\mathcal{A}$. If the classes of unitary objects\ngenerate the complexified numerical K-theory of $\\mathcal{A}$ (hypothesis\nsatisfied for many examples of smooth proper Calabi-Yau categories of dimension\n3), it is proved that these numbers are independent of the chosen rank function\n: they are intrinsic invariants of the triangulated category $\\mathcal{A}$. In\nthe special case where $\\mathcal{A}$ is a semi-orthogonal component of the\nderived category of a smooth complex projective variety and the homological\nunit of $\\mathcal{A}$ is $\\mathbb{C} \\oplus \\mathbb{C}[3]$ (that is\n$\\mathcal{A}$ is strict Calabi-Yau with respect to the rank function), we\ndefine a Hodge structure on the Hochschild homology of $\\mathcal{A}$. The\ndimensions of the Hodge spaces of this structure are the Hodge numbers\naforementioned. Finally, we give some numerical applications toward the\nHomological Mirror Symmetry conjecture for cubic sevenfolds and double quartic\nfivefolds.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nUsing the homological unit of $\\mathcal{A}$ with respect to the given rank function, we define Hodge numbers for $\\mathcal{A}$."
    },
    {
        "abstract": "  We study the modal logic of the closure algebra $P_2$, generated by the set\nof all polygons in the Euclidean plane $\\mathbb{R}^2$. We show that this logic\nis finitely axiomatizable, is complete with respect to the class of frames we\ncall \"crown\" frames, is not first order definable, does not have the Craig\ninterpolation property, and its validity problem is PSPACE-complete.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe show that this logic is finitely axiomatizable, is complete with respect to the class of frames we call \"crown\" frames..."
    },
    {
        "abstract": "  We report on broadband observations of CTA 102 ($z=1.037$) during the active\nstates in 2016-2017. In the $\\gamma$-ray band, Fermi LAT observed several\nprominent flares which followed a harder-when-brighter behavior: the hardest\nphoton index $\\Gamma=1.61\\pm 0.10$ being unusual for FSRQs. The peak\n$\\gamma$-ray flux above 100 MeV $(3.55\\pm0.55)\\times10^{-5}\\:{\\rm\nphoton\\:cm^{-2}\\:s^{-1}}$ observed on MJD 57738.47 within 4.31 minutes,\ncorresponds to an isotropic $\\gamma$-ray luminosity of\n$L_{\\gamma}=3.25\\times10^{50}\\:{\\rm erg\\:s^{-1}}$, comparable with the highest\nvalues observed from blazars so far. The analyses of the Swift UVOT/XRT data\nshow an increase in the UV/optical and X-ray bands which is contemporaneous\nwith the bright $\\gamma$-ray periods. The X-ray spectrum observed by Swift XRT\nand NuSTAR during the $\\gamma$-ray flaring period is characterized by a hard\nphoton index of $\\sim1.30$. The shortest e-folding time was $4.08\\pm1.44$\nhours, suggesting a very compact emission region\n$R\\leq\\delta\\times2.16\\times10^{14}$ cm. We modeled the spectral energy\ndistribution of CTA 102 in several periods (having different properties in\nUV/optical, X-ray and $\\gamma$-ray bands) assuming a compact blob inside and\noutside the BLR. We found that the high-energy data are better described when\nthe infrared thermal radiation of the dusty torus is considered. In the flaring\nperiods when the correlation between the $\\gamma$-ray and UV/optical/X-ray\nbands is lacking, the $\\gamma$-ray emission can be produced from the\ninteraction of fresh electrons in a different blob, which does not make a\ndominant contribution at lower energies.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The analyses of the Swift UVOT/XRT data show an increase in the UV/optical and X-ray bands which is contemporaneous with the bright \u03b3-ray periods.\n* We modeled the spectral energy distribution of CTA 102 in several periods (having different properties in UV/optical, X-ray and \u03b3-ray bands) assuming a compact blob inside and outside the BLR."
    },
    {
        "abstract": "  In this paper a novel Quantum Double Delta Swarm (QDDS) algorithm modeled\nafter the mechanism of convergence to the center of attractive potential field\ngenerated within a single well in a double Dirac delta well setup has been put\nforward and the preliminaries discussed. Theoretical foundations and\nexperimental illustrations have been incorporated to provide a first basis for\nfurther development, specifically in refinement of solutions and applicability\nto problems in high dimensional spaces. Simulations are carried out over\nvarying dimensionality on four benchmark functions, viz. Rosenbrock,\nRastrigrin, Griewank and Sphere as well as the multidimensional Finite Impulse\nResponse (FIR) Filter design problem with different population sizes. Test\nresults illustrate the algorithm yields superior results to some related\nreports in the literature while reinforcing the need of substantial future work\nto deliver near-optimal results consistently, especially if dimensionality\nscales up.\n",
        "method": "Here is the methodological sentence:\n\nSimulations are carried out over varying dimensionality on four benchmark functions, viz. Rosenbrock, Rastrigrin, Griewank and Sphere as well as the multidimensional Finite Impulse Response (FIR) Filter design problem with different population sizes."
    },
    {
        "abstract": "  Quantum simulations with ultracold atoms typically create atomic\nwavefunctions with structures at optical length scales, where direct imaging\nsuffers from the diffraction limit. In analogy to advances in optical\nmicroscopy for biological applications, we use a non-linear atomic response to\nsurpass the diffraction limit. Exploiting quantum interference, we demonstrate\nimaging with super-resolution of $\\lambda/100$ and excellent temporal\nresolution of 500 ns. We characterize our microscope's performance by measuring\nthe ensemble averaged wavefunction of atoms within the unit cells of an optical\nlattice, and observe the dynamics of atoms excited into periodic motion. This\napproach can be readily applied to image any atomic or molecular system, as\nlong as it hosts a three-level system.\n",
        "method": "Here is the methodological sentence:\n\nWe use a non-linear atomic response to surpass the diffraction limit."
    },
    {
        "abstract": "  The key issue of few-shot learning is learning to generalize. This paper\nproposes a large margin principle to improve the generalization capacity of\nmetric based methods for few-shot learning. To realize it, we develop a unified\nframework to learn a more discriminative metric space by augmenting the\nclassification loss function with a large margin distance loss function for\ntraining. Extensive experiments on two state-of-the-art few-shot learning\nmethods, graph neural networks and prototypical networks, show that our method\ncan improve the performance of existing models substantially with very little\ncomputational overhead, demonstrating the effectiveness of the large margin\nprinciple and the potential of our method.\n",
        "method": "To realize it, we develop a unified framework to learn a more discriminative metric space by augmenting the classification loss function with a large margin distance loss function for training."
    },
    {
        "abstract": "  Neural networks use their hidden layers to transform input data into linearly\nseparable data clusters, with a linear or a perceptron type output layer making\nthe final projection on the line perpendicular to the discriminating\nhyperplane. For complex data with multimodal distributions this transformation\nis difficult to learn. Projection on $k\\geq 2$ line segments is the simplest\nextension of linear separability, defining much easier goal for the learning\nprocess. Simple problems are 2-separable, but problems with inherent complex\nlogic may be solved in a simple way by $k$-separable projections. The\ndifficulty of learning non-linear data distributions is shifted to separation\nof line intervals, simplifying the transformation of data by hidden network\nlayers. For classification of difficult Boolean problems, such as the parity\nproblem, linear projection combined with \\ksep is sufficient and provides a\npowerful new target for learning. More complex targets may also be defined,\nchanging the goal of learning from linear discrimination to creation of data\ndistributions that can easily be handled by specialized models selected to\nanalyze output distributions. This approach can replace many layers of\ntransformation required by deep learning models.\n",
        "method": "Methodological sentences:\n\n* Neural networks use their hidden layers to transform input data into linearly separable data clusters, with a linear or a perceptron type output layer making the final projection on the line perpendicular to the discriminating hyperplane.\n* Projection on $k\\geq 2$ line segments is the simplest extension of linear separability, defining much easier goal for the learning process."
    },
    {
        "abstract": "  In this paper, we study codes correcting $t$ duplications of $\\ell$\nconsecutive symbols. These errors are known as tandem duplication errors, where\na sequence of symbols is repeated and inserted directly after its original\noccurrence. Using sphere packing arguments, we derive non-asymptotic upper\nbounds on the cardinality of codes that correct such errors for any choice of\nparameters. Based on the fact that a code correcting insertions of $t$\nzero-blocks can be used to correct $t$ tandem duplications, we construct codes\nfor tandem duplication errors. We compare the cardinalities of these codes with\ntheir sphere packing upper bounds. Finally, we discuss the asymptotic behavior\nof the derived codes and bounds, which yields insights about the tandem\nduplication channel.\n",
        "method": "Using sphere packing arguments, we derive non-asymptotic upper bounds on the cardinality of codes that correct such errors for any choice of parameters."
    },
    {
        "abstract": "  The rapid evolution of technology and the parallel increasing complexity of\nalgorithmic analysis in HEP requires developers to acquire a much larger\nportfolio of programming skills. Young researchers graduating from universities\nworldwide currently do not receive adequate preparation in the very diverse\nfields of modern computing to respond to growing needs of the most advanced\nexperimental challenges. There is a growing consensus in the HEP community on\nthe need for training programmes to bring researchers up to date with new\nsoftware technologies, in particular in the domains of concurrent programming\nand artificial intelligence. We review some of the initiatives under way for\nintroducing new training programmes and highlight some of the issues that need\nto be taken into account for these to be successful.\n",
        "method": "Here is the methodological sentence:\n\nWe review some of the initiatives under way for introducing new training programmes..."
    },
    {
        "abstract": "  Machine learning has been applied to several problems in particle physics\nresearch, beginning with applications to high-level physics analysis in the\n1990s and 2000s, followed by an explosion of applications in particle and event\nidentification and reconstruction in the 2010s. In this document we discuss\npromising future research and development areas for machine learning in\nparticle physics. We detail a roadmap for their implementation, software and\nhardware resource requirements, collaborative initiatives with the data science\ncommunity, academia and industry, and training the particle physics community\nin data science. The main objective of the document is to connect and motivate\nthese areas of research and development with the physics drivers of the\nHigh-Luminosity Large Hadron Collider and future neutrino experiments and\nidentify the resource needs for their implementation. Additionally we identify\nareas where collaboration with external communities will be of great benefit.\n",
        "method": "None. There are no methodological sentences in this abstract."
    },
    {
        "abstract": "  Pairwise network models such as the Gaussian Graphical Model (GGM) are a\npowerful and intuitive way to analyze dependencies in multivariate data. A key\nassumption of the GGM is that each pairwise interaction is independent of the\nvalues of all other variables. However, in psychological research this is often\nimplausible. In this paper, we extend the GGM by allowing each pairwise\ninteraction between two variables to be moderated by (a subset of) all other\nvariables in the model, and thereby introduce a Moderated Network Model (MNM).\nWe show how to construct the MNM and propose an L1-regularized nodewise\nregression approach to estimate it. We provide performance results in a\nsimulation study and show that MNMs outperform the split-sample based methods\nNetwork Comparison Test (NCT) and Fused Graphical Lasso (FGL) in detecting\nmoderation effects. Finally, we provide a fully reproducible tutorial on how to\nestimate MNMs with the R-package mgm and discuss possible issues with model\nmisspecification.\n",
        "method": "A key assumption of the GGM is that each pairwise interaction is independent of the values of all other variables."
    },
    {
        "abstract": "  Recently, heuristics based on the Douglas-Rachford splitting algorithm and\nthe alternating direction method of multipliers (ADMM) have found empirical\nsuccess in minimizing convex functions over nonconvex sets, but not much has\nbeen done to improve the theoretical understanding of them. In this paper, we\ninvestigate convergence of these heuristics. First, we characterize optimal\nsolutions of minimization problems involving convex cost functions over\nnonconvex constraint sets. We show that these optimal solutions are related to\nthe fixed point set of the underlying nonconvex Douglas-Rachford operator.\nNext, we establish sufficient conditions under which the Douglas-Rachford\nsplitting heuristic either converges to a point or its cluster points form a\nnonempty compact connected set. In the case where the heuristic converges to a\npoint, we establish sufficient conditions for that point to be an optimal\nsolution. Then, we discuss how the ADMM heuristic can be constructed from the\nDouglas-Rachford splitting algorithm. We show that, unlike in the convex case,\nthe algorithms in our nonconvex setup are not equivalent to each other and have\na rather involved relationship between them. Finally, we comment on convergence\nof the ADMM heuristic and compare it with the Douglas-Rachford splitting\nheuristic.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe characterize optimal solutions of minimization problems involving convex cost functions over nonconvex constraint sets.\n\nNote that there may be additional methodological details mentioned throughout the abstract, but this is the most explicit one."
    },
    {
        "abstract": "  Reasoning about exceptions in ontologies is nowadays one of the challenges\nthe description logics community is facing. The paper describes a preferential\napproach for dealing with exceptions in Description Logics, based on the\nrational closure. The rational closure has the merit of providing a simple and\nefficient approach for reasoning with exceptions, but it does not allow\nindependent handling of the inheritance of different defeasible properties of\nconcepts. In this work we outline a possible solution to this problem by\nintroducing a variant of the lexicographical closure, that we call skeptical\nclosure, which requires to construct a single base. We develop a bi-preference\nsemantics semantics for defining a characterization of the skeptical closure.\n",
        "method": "The paper describes a preferential approach for dealing with exceptions in Description Logics, based on the rational closure."
    },
    {
        "abstract": "  We prove $C^{1,\\nu}$ regularity for local minimizers of the \\oh{multi-phase}\nenergy: \\begin{flalign*} w \\mapsto\n\\int_{\\Omega}\\snr{Dw}^{p}+a(x)\\snr{Dw}^{q}+b(x)\\snr{Dw}^{s} \\ dx,\n\\end{flalign*} under sharp assumptions relating the couples $(p,q)$ and $(p,s)$\nto the H\\\"older exponents of the modulating coefficients $a(\\cdot)$ and\n$b(\\cdot)$, respectively.\n",
        "method": "No methodological sentences were found in this abstract."
    },
    {
        "abstract": "  T. Harima and J. Watanabe studied the Lefschetz properties of free extension\nArtinian algebras $C$ over a base $A$ with fibre $B$. The free extensions are\ndeformations of the usual tensor product, when $C$ is also Gorenstein, so are\n$A$ and $B$, and it is natural to ask for the relation among the Macaulay dual\ngenerators for the algebras. Writing a dual generator $F$ for $C$ as a\nhomogeneous \"polynomial\" in $T$ and the dual variables for $B$, and given the\ndual generator for $B$, we give sufficient conditions on $F$ that ensure that\n$C$ is a free extension of $A={\\sf k}[t]/(t^n)$ with fiber $B$. We give\nexamples that explore the sharpness of the statements. We also consider a\nspecial set of coinvariant algebras $C$ which are free extensions of $A$, but\nwhich do not satisfy the sufficient conditions of our main result.\n",
        "method": "The methodological sentence is:\n\nWe give sufficient conditions on $F$ that ensure that $C$ is a free extension of $A={\\sf k}[t]/(t^n)$ with fiber $B$."
    },
    {
        "abstract": "  We consider the following distributed service model: jobs with unit mean,\ngeneral distribution, and independent processing times arrive as a renewal\nprocess of rate $\\lambda n$, with $0<\\lambda<1$, and are immediately dispatched\nto one of several queues associated with $n$ identical servers with unit\nprocessing rate. We assume that the dispatching decisions are made by a central\ndispatcher endowed with a finite memory, and with the ability to exchange\nmessages with the servers.\n  We study the fundamental resource requirements (memory bits and message\nexchange rate), in order to drive the expected queueing delay in steady-state\nof a typical job to zero, as $n$ increases. We develop a novel approach to show\nthat, within a certain broad class of \"symmetric\" policies, every dispatching\npolicy with a message rate of the order of $n$, and with a memory of the order\nof $\\log n$ bits, results in an expected queueing delay which is bounded away\nfrom zero, uniformly as $n\\to\\infty$.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n1. We consider the following distributed service model: jobs with unit mean, general distribution, and independent processing times arrive as a renewal process of rate $\\lambda n$, with $0<\\lambda<1$, and are immediately dispatched to one of several queues associated with $n$ identical servers with unit processing rate.\n2. We develop a novel approach to show that, within a certain broad class of \"symmetric\" policies..."
    },
    {
        "abstract": "  Quantum error detection has always been a fundamental challenge in a\nfault-tolerant quantum computer. Hence, it is of immense importance to detect\nand deal with arbitrary errors to efficiently perform quantum computation.\nSeveral error detection codes have been proposed and realized for lower number\nof qubit systems. Here we present an error detection code for a (2n+1)-qubit\nentangled state using two syndrome qubits and simulate it on IBM's 16-qubit\nquantum computer for a 13-qubit entangled system. The code is able to detect an\narbitrary quantum error in any one of the first 2n qubits of the (2n+1)-qubit\nentangled state and detects any bit-flip error on the last qubit of the\n(2n+1)-qubit entangled state via measurements on a pair of ancillary error\nsyndrome qubits. The protocol presented here paves the way for designing error\ndetection codes for the general higher number of entangled qubit systems.\n",
        "method": "Here is the methodological sentence:\n\nHere we present an error detection code for a (2n+1)-qubit entangled state using two syndrome qubits and simulate it on IBM's 16-qubit quantum computer for a 13-qubit entangled system."
    },
    {
        "abstract": "  We study the problem of community detection in a random hypergraph model\nwhich we call the stochastic block model for $k$-uniform hypergraphs ($k$-SBM).\nWe investigate the exact recovery problem in $k$-SBM and show that a sharp\nphase transition occurs around a threshold: below the threshold it is\nimpossible to recover the communities with non-vanishing probability, yet above\nthe threshold there is an estimator which recovers the communities almost\nasymptotically surely. We also consider a simple, efficient algorithm for the\nexact recovery problem which is based on a semidefinite relaxation technique.\n",
        "method": "We investigate the exact recovery problem in $k$-SBM and show that a sharp phase transition occurs around a threshold: below the threshold it is impossible to recover the communities with non-vanishing probability, yet above the threshold there is an estimator which recovers the communities almost asymptotically surely."
    },
    {
        "abstract": "  The permutation test is known as the exact test procedure in statistics.\nHowever, often it is not exact in practice and only an approximate method since\nonly a small fraction of every possible permutation is generated. Even for a\nsmall sample size, it often requires to generate tens of thousands\npermutations, which can be a serious computational bottleneck. In this paper,\nwe propose a novel combinatorial inference procedure that enumerates all\npossible permutations combinatorially without any resampling. The proposed\nmethod is validated against the standard permutation test in simulation studies\nwith the ground truth. The method is further applied in twin DTI study in\ndetermining the genetic contribution of the minimum spanning tree of the\nstructural brain connectivity.\n",
        "method": "Here is the methodological sentence:\n\nThe proposed method enumerates all possible permutations combinatorially without any resampling."
    },
    {
        "abstract": "  Model-based compression is an effective, facilitating, and expanded model of\nneural network models with limited computing and low power. However,\nconventional models of compression techniques utilize crafted features [2,3,12]\nand explore specialized areas for exploration and design of large spaces in\nterms of size, speed, and accuracy, which usually have returns Less and time is\nup. This paper will effectively analyze deep auto compression (ADC) and\nreinforcement learning strength in an effective sample and space design, and\nimprove the compression quality of the model. The results of compression of the\nadvanced model are obtained without any human effort and in a completely\nautomated way. With a 4- fold reduction in FLOP, the accuracy of 2.8% is higher\nthan the manual compression model for VGG-16 in ImageNet.\n",
        "method": "This paper will effectively analyze deep auto compression (ADC) and reinforcement learning strength in an effective sample and space design, and improve the compression quality of the model."
    },
    {
        "abstract": "  Balloon-borne astronomy is a unique tool that allows for a level of image\nstability and significantly reduced atmospheric interference without the often\nprohibitive cost and long development time-scale that are characteristic of\nspace-borne facility-class instruments. The Super-pressure Balloon-borne\nImaging Telescope (SuperBIT) is a wide-field imager designed to provide 0.02\"\nimage stability over a 0.5 degree field-of-view for deep exposures within the\nvisible-to-near-UV (300-900 um). As such, SuperBIT is a suitable platform for a\nwide range of balloon-borne observations, including solar and extrasolar\nplanetary spectroscopy as well as resolved stellar populations and distant\ngalaxies. We report on the overall payload design and instrumentation\nmethodologies for SuperBIT as well as telescope and image stability results\nfrom two test flights. Prospects for the SuperBIT project are outlined with an\nemphasis on the development of a fully operational, three-month science flight\nfrom New Zealand in 2020.\n",
        "method": "Here is the methodological sentence:\n\nWe report on the overall payload design and instrumentation methodologies for SuperBIT as well as telescope and image stability results from two test flights."
    },
    {
        "abstract": "  We discuss the time evolution of physical finite dimensional systems which\nare modelled by non-hermitian Hamiltonians. We address both general\nnon-hermitian Hamiltonians and pseudo-hermitian ones. We apply the theory of\nKrein Spaces to construct metric operators and well-defined inner products. As\nan application, we study the stationary behaviour of dissipative One Axis\nTwisting Hamiltonians. We discuss the effect of decoherence under different\ncoupling schemes.\n",
        "method": "We apply the theory of Krein Spaces to construct metric operators and well-defined inner products."
    },
    {
        "abstract": "  We prove that the asymptotic distribution of resonances has a multilevel\ninternal structure for the following classes of Hamiltonians H: Schr\\\"odinger\noperators with point interactions in $\\mathbb{R}^3$, quantum graphs, and 1-D\nphotonic crystals. In the case of $N \\ge 2$ point interactions, the set of\nresonances $\\Sigma (H)$ essentially consists of a finite number of sequences\nwith logarithmic asymptotics. We show how the leading parameters $\\mu$ of these\nsequences are connected with the geometry of the set $Y=\\{y_j\\}_{j=1}^N$ of\ninteraction centers $y_j \\in \\mathbb{R}^3$. The minimal parameter $\\mu^{min}$\ncorresponds to the sequences with `more narrow' and so more observable\nresonances. The asymptotic density of such narrow resonances is described by\nthe multiplicity of $\\mu^{\\min}$, which occurs to be connected with the\nsymmetries of Y and naturally introduces a finite number of classes of\nconfigurations of $Y$. In the case of quantum graphs and 1-D photonic crystals,\nthe decomposition of $\\Sigma(H)$ into a finite number of asymptotic sequences\nis proved under additional commensurability conditions. To address the case of\na general quantum graph, we introduce families of special counting and\nasymptotic density functions for two types of curved complex strips. The\nobtained results and effects are compared with those of obstacle scattering.\n",
        "method": "Here is the methodological sentence:\n\nWe prove that..."
    },
    {
        "abstract": "  Transcriber's note: In the fall of 1976, my advisor, David Mumford, handed me\na short preprint by George Kempf to read. It was the first state of what\neventually became his influential Annals paper \"Instability in Invariant\nTheory\" (Annals of Mathematics, Second Series, Vol. 108, No. 2 (Sep., 1978),\npp. 299-316). The introduction to the published version ends with an\nacknowledgement and a dig: \"I want to thank the referee of the first version of\nthis paper for pointing out Corollary 4-5 and conjecturing that the original\n{0}-instability could be replaced by S-instability. Unfortunately, the\ninclusion of S-instability has completely destroyed the simplicity of the\noriginal version.\" Over the intervening years, the simplicity and elegance of\nthe first version has continued to create a readership for it, and copies (see\nFigure 8) continue to circulate informally. I created this LaTeX'ed version to\nmake the paper accessible to all who may be interested in it, trying to keep\nthe look close to that of the original typewritten preprint and making changes\nonly to correct a few obvious typos and harmonize the markup. My thanks to\nGeorge's children, Robin and Lucas Kempf, for graciously granting me permission\nto post this transcription. Comments welcome to Ian Morrison\n(morrison@fordham.edu)\n",
        "method": "There are no methodological sentences in this abstract. The text appears to be a personal account of the author's experience with an academic paper and its evolution over time, rather than a description of research methods or approaches."
    },
    {
        "abstract": "  We discuss an extended Teleparallel gravity models comprising functions of\nscalar invariants constructed by torsion, torsion Gauss-Bonnet and boundary\nterms. We adopt the Noether Symmetry Approach to select the functional forms,\nthe first integrals and, eventually, the exact solutions of the dynamics in the\ncontext of flat Friedman-Robertson-Walker cosmology. Standard perfect fluid\nmatter, minimally coupled to geometry, is considered. Several physically\nconsistent models are derived with related exact solutions.\n",
        "method": "We adopt the Noether Symmetry Approach to select the functional forms, the first integrals and, eventually, the exact solutions of the dynamics in the context of flat Friedman-Robertson-Walker cosmology."
    },
    {
        "abstract": "  We explore solutions for automated labeling of content in bug trackers and\ncustomer support systems. In order to do that, we classify content in terms of\nseveral criteria, such as priority or product area. In the first part of the\npaper, we provide an overview of existing methods used for text classification.\nThese methods fall into two categories - the ones that rely on neural networks\nand the ones that don't. We evaluate results of several solutions of both\nkinds. In the second part of the paper we present our own recurrent neural\nnetwork solution based on hierarchical attention paradigm. It consists of\nseveral Hierarchical Attention network blocks with varying Gated Recurrent Unit\ncell sizes and a complementary shallow network that goes alongside. Lastly, we\nevaluate above-mentioned methods when predicting fields from two datasets -\nArch Linux bug tracker and Chromium bug tracker. Our contributions include a\ncomprehensive benchmark between a variety of methods on relevant datasets; a\nnovel solution that outperforms previous generation methods; and two new\ndatasets that are made public for further research.\n",
        "method": "In order to do that, we classify content in terms of several criteria, such as priority or product area."
    },
    {
        "abstract": "  We introduce Turaev bicategories and Turaev pseudofunctors. On the one hand,\nthey generalize the notions of Turaev categories (and Turaev functors),\nintroduced at the turn of the millennium and originally called \"crossed group\ncategories\" by Turaev himself, and the notions of bicategories and\npseudofunctors, on the other. For bimonads in 2-categories, which we defined in\none of our previous papers, we introduce generalized Yetter-Drinfel`d modules\nin 2-categories. These generalize to the 2-categorical setting the generalized\nYetter-Drinfel`d modules (over a field) of Panaite and Staic, and thus also in\nparticular the anti Yetter-Drinfel`d modules, introduced by\nHajac-Khalkhali-Rangipour-Sommerhauser as coefficients for the cyclic\ncohomology of Hopf algebras, defined by Connes and Moscovici. We construct\nTuraev 2-category for bimonads in 2-categories as a Turaev extension of the\n2-category of bimonads. This Turaev 2-category generalizes the Turaev category\nof generalized Yetter-Drinfel`d modules of Panaite and Staic. We also prove in\nthe 2-categorical setting their results on pairs in involution, which in turn\ngo back to modular pairs in involution of Connes and Moscovici.\n",
        "method": "For bimonads in 2-categories, we introduce generalized Yetter-Drinfel`d modules in 2-categories."
    },
    {
        "abstract": "  Electroluminescence (EL) imaging is a useful modality for the inspection of\nphotovoltaic (PV) modules. EL images provide high spatial resolution, which\nmakes it possible to detect even finest defects on the surface of PV modules.\nHowever, the analysis of EL images is typically a manual process that is\nexpensive, time-consuming, and requires expert knowledge of many different\ntypes of defects. In this work, we investigate two approaches for automatic\ndetection of such defects in a single image of a PV cell. The approaches differ\nin their hardware requirements, which are dictated by their respective\napplication scenarios. The more hardware-efficient approach is based on\nhand-crafted features that are classified in a Support Vector Machine (SVM). To\nobtain a strong performance, we investigate and compare various processing\nvariants. The more hardware-demanding approach uses an end-to-end deep\nConvolutional Neural Network (CNN) that runs on a Graphics Processing Unit\n(GPU). Both approaches are trained on 1,968 cells extracted from high\nresolution EL intensity images of mono- and polycrystalline PV modules. The CNN\nis more accurate, and reaches an average accuracy of 88.42%. The SVM achieves a\nslightly lower average accuracy of 82.44%, but can run on arbitrary hardware.\nBoth automated approaches make continuous, highly accurate monitoring of PV\ncells feasible.\n",
        "method": "The two approaches for automatic detection of defects in a single image of a PV cell differ in their hardware requirements."
    },
    {
        "abstract": "  Image similarity measures play an important role in nearest neighbor search\nand duplicate detection for large-scale image datasets. Recently, Minwise\nHashing (or Minhash) and its related hashing algorithms have achieved great\nperformances in large-scale image retrieval systems. However, there are a large\nnumber of comparisons for image pairs in these applications, which may spend a\nlot of computation time and affect the performance. In order to quickly obtain\nthe pairwise images that theirs similarities are higher than the specific\nthreshold T (e.g., 0.5), we propose a dynamic threshold filter of Minwise\nHashing for image similarity measures. It greatly reduces the calculation time\nby terminating the unnecessary comparisons in advance. We also find that the\nfilter can be extended to other hashing algorithms, on when the estimator\nsatisfies the binomial distribution, such as b-Bit Minwise Hashing, One\nPermutation Hashing, etc. In this pager, we use the Bag-of-Visual-Words (BoVW)\nmodel based on the Scale Invariant Feature Transform (SIFT) to represent the\nimage features. We have proved that the filter is correct and effective through\nthe experiment on real image datasets.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We propose a dynamic threshold filter of Minwise Hashing for image similarity measures.\n* It greatly reduces the calculation time by terminating the unnecessary comparisons in advance.\n* The filter can be extended to other hashing algorithms, on when the estimator satisfies the binomial distribution.\n* In this pager, we use the Bag-of-Visual-Words (BoVW) model based on the Scale Invariant Feature Transform (SIFT) to represent the image features."
    },
    {
        "abstract": "  Recurrence Quantification Analysis (RQA) can help to detect significant\nevents and phase transitions of a dynamical system, but choosing a suitable set\nof parameters is crucial for the success. From recurrence plots different RQA\nvariables can be obtained and analysed. Currently, most of the methods for RQA\nradius optimisation are focusing on a single RQA variable. In this work we are\nproposing two new methods for radius optimisation that look for an optimum in\nthe higher dimensional space of the RQA variables, therefore synchronously\noptimising across several variables. We illustrate our approach using two case\nstudies: a well known Lorenz dynamical system, and a time-series obtained from\nmonitoring energy consumption of a small enterprise. Our case studies show that\nboth methods result in plausible values and can be used to analyse energy data.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"From recurrence plots different RQA variables can be obtained and analysed.\""
    },
    {
        "abstract": "  We describe a new and model-independent L\\'evy imaging method of quality fits\nto the published datasets and reconstruct the amplitude of high-energy $pp$ and\n$p\\bar p$ elastic scattering processes. This method allows us to determine the\nexcitation function of the shadow profile $P(b)$, the elastic slope $B(t)$ and\nthe nuclear phase $\\phi(t)$ functions of $pp$ and $p\\bar p$ collisions directly\nfrom the data. Surprisingly, notable qualitative differences in $B(t)$ for $pp$\nand for $p\\bar p$ collisions point towards an Odderon effect. As a by-product,\nwe clearly identify the proton substructure with two different sizes at the ISR\nand LHC energies, that has striking similarity to a dressed quark (at the ISR)\nand a dressed diquark (at the LHC). We present model-independent results for\nthe corresponding sizes and cross-sections for such a substructure for the\nexisting data at different energies.\n",
        "method": "This method allows us to determine the excitation function of the shadow profile P(b), the elastic slope B(t) and the nuclear phase phi(t) functions of pp and pbar collisions directly from the data."
    },
    {
        "abstract": "  We exploit the parquet formalism to derive exact flow equations for the\ntwo-particle-reducible four-point vertices, the self-energy, and typical\nresponse functions, circumventing the reliance on higher-point vertices. This\nincludes a concise, algebraic derivation of the multiloop flow equations, which\nhave previously been obtained by diagrammatic considerations. Integrating the\nmultiloop flow for a given input of the totally irreducible vertex is\nequivalent to solving the parquet equations with that input. Hence, one can\ntune systems from solvable limits to complicated situations by variation of\none-particle parameters, staying at the fully self-consistent solution of the\nparquet equations throughout the flow. Furthermore, we use the resulting\ndifferential form of the Schwinger-Dyson equation for the self-energy to\ndemonstrate one-particle conservation of the parquet approximation and to\nconstruct a conserving two-particle vertex via functional differentiation of\nthe parquet self-energy. Our analysis gives a unified picture of the various\nmany-body relations and exact renormalization group equations.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe exploit the parquet formalism to derive exact flow equations for..."
    },
    {
        "abstract": "  In this paper we explore some results concerning the spread of the line and\nthe total graph of a given graph. In particular, it is proved that for an\n$(n,m)$ connected graph $G$ with $m > n \\geq 4$ the spread of $G$ is less than\nor equal to the spread of its line graph, where the equality holds if and only\nif $G$ is regular bipartite. A sufficient condition for the spread of the graph\nnot be greater than the signless Laplacian spread for a class of non bipartite\nand non regular graphs is proved. Additionally, we derive an upper bound for\nthe spread of the line graph of graphs on $n$ vertices having a vertex (edge)\nconnectivity less than or equal to a positive integer $k$. Combining techniques\nof interlacing of eigenvalues, we derive lower bounds for the Laplacian and\nsignless Laplacian spread of the total graph of a connected graph. Moreover,\nfor a regular graph, an upper and lower bound for the spread of its total graph\nis given.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Combining techniques of interlacing of eigenvalues... (one sentence)\n* We derive an upper bound for the spread of the line graph... (one sentence)\n* A sufficient condition for the spread of the graph not be greater than the signless Laplacian spread... (one sentence)"
    },
    {
        "abstract": "  With the help of a logarithmic barrier augmented Lagrangian function, we can\nobtain closed-form solutions of slack variables of logarithmic-barrier problems\nof nonlinear programs. As a result, a two-parameter primal-dual nonlinear\nsystem is proposed, which corresponds to the Karush-Kuhn-Tucker point and the\ninfeasible stationary point of nonlinear programs, respectively, as one of two\nparameters vanishes. Based on this distinctive system, we present a primal-dual\ninterior-point method capable of rapidly detecting infeasibility of nonlinear\nprograms. The method generates interior-point iterates without truncation of\nthe step. It is proved that our method converges to a Karush-Kuhn-Tucker point\nof the original problem as the barrier parameter tends to zero. Otherwise, the\nscaling parameter tends to zero, and the method converges to either an\ninfeasible stationary point or a singular stationary point of the original\nproblem. Moreover, our method has the capability to rapidly detect the\ninfeasibility of the problem. Under suitable conditions, not only the method\ncan be superlinearly or quadratically convergent to the Karush-Kuhn-Tucker\npoint as the original problem is feasible, but also it can be superlinearly or\nquadratically convergent to the infeasible stationary point when a problem is\ninfeasible. Preliminary numerical results show that the method is efficient in\nsolving some simple but hard problems and some standard test problems from the\nCUTE collection, where the superlinear convergence is demonstrated when we\nsolve two infeasible problems and one well-posed feasible counterexample\npresented in the literature.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* With the help of a logarithmic barrier augmented Lagrangian function, we can obtain closed-form solutions of slack variables of logarithmic-barrier problems of nonlinear programs.\n* Based on this distinctive system, we present a primal-dual interior-point method capable of rapidly detecting infeasibility of nonlinear programs."
    },
    {
        "abstract": "  Data-driven methods for improving turbulence modeling in Reynolds-Averaged\nNavier-Stokes (RANS) simulations have gained significant interest in the\ncomputational fluid dynamics community. Modern machine learning algorithms have\nopened up a new area of black-box turbulence models allowing for the tuning of\nRANS simulations to increase their predictive accuracy. While several\ndata-driven turbulence models have been reported, the quantification of the\nuncertainties introduced has mostly been neglected. Uncertainty quantification\nfor such data-driven models is essential since their predictive capability\nrapidly declines as they are tested for flow physics that deviate from that in\nthe training data. In this work, we propose a novel data-driven framework that\nnot only improves RANS predictions but also provides probabilistic bounds for\nfluid quantities such as velocity and pressure. The uncertainties capture both\nmodel form uncertainty as well as epistemic uncertainty induced by the limited\ntraining data. An invariant Bayesian deep neural network is used to predict the\nanisotropic tensor component of the Reynolds stress. This model is trained\nusing Stein variational gradient decent algorithm. The computed uncertainty on\nthe Reynolds stress is propagated to the quantities of interest by vanilla\nMonte Carlo simulation. Results are presented for two test cases that differ\ngeometrically from the training flows at several different Reynolds numbers.\nThe prediction enhancement of the data-driven model is discussed as well as the\nassociated probabilistic bounds for flow properties of interest. Ultimately\nthis framework allows for a quantitative measurement of model confidence and\nuncertainty quantification for flows in which no high-fidelity observations or\nprior knowledge is available.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* An invariant Bayesian deep neural network is used to predict the anisotropic tensor component of the Reynolds stress.\n* This model is trained using Stein variational gradient decent algorithm.\n* The computed uncertainty on the Reynolds stress is propagated to the quantities of interest by vanilla Monte Carlo simulation."
    },
    {
        "abstract": "  Solar chromosphere and coronal heating is a big question for astrophysics.\nDaily measurement of 985 solar spectral irradiances (SSIs) at the spectral\nintervals 1-39 nm and 116-2416 nm during March 1 2003 to October 28 2017 is\nutilized to investigate phase relation respectively with daily sunspot number,\nthe Mount Wilson Sunspot Index, and the Magnetic Plage Strength Index. All SSIs\nwhich form in the whole heated region: the upper photosphere, chromosphere,\ntransition region, and corona are found to be significantly more correlated to\nweak magnetic activity than to strong magnetic activity, and to dance in step\nwith weak magnetic activity. All SSIs which form in the low photosphere (the\nunheated region), which indicate the \"energy\" leaked from the solar subsurface\nare found to be more related to strong magnetic activity instead and in\nanti-phase with weak magnetic activity. In the upper photosphere and\nchromosphere, strong magnetic activity should lead SSI by about a solar\nrotation, also displaying that weak magnetic activity should take effect on\nheating there. It is thus small-scale weak magnetic activity that effectively\nheats the upper solar atmosphere.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Daily measurement of 985 solar spectral irradiances (SSIs) at the spectral intervals 1-39 nm and 116-2416 nm during March 1, 2003 to October 28, 2017 is utilized...\n\nLet me know if you need anything else!"
    },
    {
        "abstract": "  The notions of concreteness and imageability, traditionally important in\npsycholinguistics, are gaining significance in semantic-oriented natural\nlanguage processing tasks. In this paper we investigate the predictability of\nthese two concepts via supervised learning, using word embeddings as\nexplanatory variables. We perform predictions both within and across languages\nby exploiting collections of cross-lingual embeddings aligned to a single\nvector space. We show that the notions of concreteness and imageability are\nhighly predictable both within and across languages, with a moderate loss of up\nto 20% in correlation when predicting across languages. We further show that\nthe cross-lingual transfer via word embeddings is more efficient than the\nsimple transfer via bilingual dictionaries.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We perform predictions both within and across languages by exploiting collections of cross-lingual embeddings aligned to a single vector space.\""
    },
    {
        "abstract": "  The closure of a generic torus orbit in the flag variety $G/B$ of type\n$A_{n-1}$ is known to be a permutohedral variety and well studied. In this\npaper we introduce the notion of a generic torus orbit in the Schubert variety\n$X_w$ $(w\\in \\mathfrak{S}_n)$ and study its closure $Y_w$. We identify the\nmaximal cone in the fan of $Y_w$ corresponding to a fixed point $uB$ $(u\\le\nw)$, associate a graph $\\Gamma_w(u)$ to each $u\\le w$, and show that $Y_w$ is\nsmooth at $uB$ if and only if $\\Gamma_w(u)$ is a forest. We also introduce a\npolynomial $A_w(t)$ for each $w$, which agrees with the Eulerian polynomial\nwhen $w$ is the longest element of $\\mathfrak{S}_n$, and show that the\nPoincar\\'e polynomial of $Y_w$ agrees with $A_w(t^2)$ when $Y_w$ is smooth.\n",
        "method": "The closure of a generic torus orbit in the flag variety G/B of type A_{n-1} is known to be a permutohedral variety and well studied."
    },
    {
        "abstract": "  Recently, there have been several successful deep learning approaches for\nautomatically classifying chest X-ray images into different disease categories.\nHowever, there is not yet a comprehensive vulnerability analysis of these\nmodels against the so-called adversarial perturbations/attacks, which makes\ndeep models more trustful in clinical practices. In this paper, we extensively\nanalyzed the performance of two state-of-the-art classification deep networks\non chest X-ray images. These two networks were attacked by three different\ncategories (ten methods in total) of adversarial methods (both white- and\nblack-box), namely gradient-based, score-based, and decision-based attacks.\nFurthermore, we modified the pooling operations in the two classification\nnetworks to measure their sensitivities against different attacks, on the\nspecific task of chest X-ray classification.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe extensively analyzed the performance of two state-of-the-art classification deep networks on chest X-ray images, and modified the pooling operations in these two networks to measure their sensitivities against different attacks, on the specific task of chest X-ray classification."
    },
    {
        "abstract": "  Super-resolution microscopy has revolutionized the fields of chemistry and\nbiology by resolving features at the molecular level. Such techniques can be\neither \"stochastic,\" gaining resolution through precise localization of point\nsource emitters, or \"deterministic,\" leveraging the nonlinear optical response\nof a sample to improve resolution. In atomic physics, deterministic methods can\nbe applied to reveal the atomic wavefunction and to perform quantum control.\nHere we demonstrate super-resolution imaging based on nonlinear response of\natoms to an optical pumping pulse. With this technique the atomic density\ndistribution can be resolved with a point spread function FWHM of 32(4) nm and\na localization precision below 1 nm. The short optical pumping pulse of 1.4\n$\\mu$s enables us to resolve fast atomic dynamics within a single lattice site.\nA byproduct of our scheme is the emergence of moir\\'{e} patterns on the atomic\ncloud, which we show to be immensely magnified images of the atomic density in\nthe lattice. Our work represents a general approach to accessing the physics of\ncold atoms at the nanometer scale, and can be extended to higher dimensional\nlattices and bulk systems for a variety of atomic and molecular species.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* With this technique the atomic density distribution can be resolved with a point spread function FWHM of 32(4) nm and a localization precision below 1 nm.\n* The short optical pumping pulse of 1.4 $\\mu$s enables us to resolve fast atomic dynamics within a single lattice site."
    },
    {
        "abstract": "  We study the impact of Lorentz violating terms on a physical observable for\nboth electrodynamics of chiral matter and an Abelian Higgs-like model in $3+1$\ndimensions. Our calculation is done within the framework of the\ngauge-invariant, but path-dependent, variables formalism. Interestingly enough,\nfor electrodynamics of chiral matter we obtain a logarithmic correction to the\nusual static Coulomb potential. Whereas for a Abelian Higgs model with a\nLorentz-breaking term, our result displays new corrections to the Yukawa\npotential.\n",
        "method": "Our calculation is done within the framework of the gauge-invariant, but path-dependent, variables formalism."
    },
    {
        "abstract": "  Deploying the idea of long-term cumulative return, reinforcement learning has\nshown remarkable performance in various fields. We propose a formulation of the\nlandmark localization in 3D medical images as a reinforcement learning problem.\nWhereas value-based methods have been widely used to solve similar problems, we\nadopt an actor-critic based direct policy search method framed in a temporal\ndifference learning approach. Successful behavior learning is challenging in\nlarge state and/or action spaces, requiring many trials. We introduce a partial\npolicy-based reinforcement learning to enable solving the large problem of\nlocalization by learning the optimal policy on smaller partial domains.\nIndependent actors efficiently learn the corresponding partial policies, each\nutilizing their own independent critic. The proposed policy reconstruction from\nthe partial policies ensures a robust and efficient localization utilizing the\nsub-agents solving simple binary decision problems in their corresponding\npartial action spaces. The proposed reinforcement learning requires a small\nnumber of trials to learn the optimal behavior compared with the original\nbehavior learning scheme.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We adopt an actor-critic based direct policy search method framed in a temporal difference learning approach.\n* We introduce a partial policy-based reinforcement learning to enable solving the large problem of localization by learning the optimal policy on smaller partial domains.\n* The proposed policy reconstruction from the partial policies ensures a robust and efficient localization utilizing the sub-agents solving simple binary decision problems in their corresponding partial action spaces."
    },
    {
        "abstract": "  Methyl isocyanide (CH$_3$NC) is the isocyanide with the largest number of\natoms confirmed in the interstellar medium (ISM), but it is not an abundant\nmolecule, having only been detected towards a handful of objects. Conversely,\nits isomer, methyl cyanide (CH$_3$CN), is one of the most abundant complex\norganic molecules detected in the ISM, with detections in a variety of low- and\nhigh-mass sources.\n  We use ALMA observations from the Protostellar Interferometric Line Survey\n(PILS) to search for methyl isocyanide and compare its abundance with that of\nits isomer methyl cyanide. We use a new line catalogue from the Cologne\nDatabase for Molecular Spectroscopy (CDMS) to identify methyl isocyanide lines.\nWe also model the chemistry with an updated version of the three-phase chemical\nkinetics model {\\em MAGICKAL}, presenting the first chemical modelling of\nmethyl isocyanide to date.\n  We detect methyl isocyanide for the first time in a solar-type protostar,\nIRAS 16293$-$2422 B, and present upper limits for its companion protostar, IRAS\n16293$-$2422 A. Methyl isocyanide is found to be at least 20 times more\nabundant in source B compared to source A, with a CH$_3$CN/CH$_3$NC abundance\nratio of 200 in IRAS 16293--2422 B and >5517 in IRAS 16293$-$2422 A. We also\npresent the results of a chemical model of methyl isocyanide chemistry in both\nsources, and discuss the implications on methyl isocyanide formation mechanisms\nand the relative evolutionary stages of both sources.\n",
        "method": "Here are the methodological sentences:\n\nWe use ALMA observations from the Protostellar Interferometric Line Survey (PILS) to search for methyl isocyanide and compare its abundance with that of its isomer methyl cyanide.\nWe use a new line catalogue from the Cologne Database for Molecular Spectroscopy (CDMS) to identify methyl isocyanide lines.\nWe also model the chemistry with an updated version of the three-phase chemical kinetics model {\\em MAGICKAL}, presenting the first chemical modelling of methyl isocyanide to date."
    },
    {
        "abstract": "  Model interpretability is an increasingly important component of practical\nmachine learning. Some of the most common forms of interpretability systems are\nexample-based, local, and global explanations. One of the main challenges in\ninterpretability is designing explanation systems that can capture aspects of\neach of these explanation types, in order to develop a more thorough\nunderstanding of the model. We address this challenge in a novel model called\nMAPLE that uses local linear modeling techniques along with a dual\ninterpretation of random forests (both as a supervised neighborhood approach\nand as a feature selection method). MAPLE has two fundamental advantages over\nexisting interpretability systems. First, while it is effective as a black-box\nexplanation system, MAPLE itself is a highly accurate predictive model that\nprovides faithful self explanations, and thus sidesteps the typical\naccuracy-interpretability trade-off. Specifically, we demonstrate, on several\nUCI datasets, that MAPLE is at least as accurate as random forests and that it\nproduces more faithful local explanations than LIME, a popular interpretability\nsystem. Second, MAPLE provides both example-based and local explanations and\ncan detect global patterns, which allows it to diagnose limitations in its\nlocal explanations.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method).\n* Specifically, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests..."
    },
    {
        "abstract": "  Deep neural networks have shown good data modelling capabilities when dealing\nwith challenging and large datasets from a wide range of application areas.\nConvolutional Neural Networks (CNNs) offer advantages in selecting good\nfeatures and Long Short-Term Memory (LSTM) networks have proven good abilities\nof learning sequential data. Both approaches have been reported to provide\nimproved results in areas such image processing, voice recognition, language\ntranslation and other Natural Language Processing (NLP) tasks. Sentiment\nclassification for short text messages from Twitter is a challenging task, and\nthe complexity increases for Arabic language sentiment classification tasks\nbecause Arabic is a rich language in morphology. In addition, the availability\nof accurate pre-processing tools for Arabic is another current limitation,\nalong with limited research available in this area. In this paper, we\ninvestigate the benefits of integrating CNNs and LSTMs and report obtained\nimproved accuracy for Arabic sentiment analysis on different datasets.\nAdditionally, we seek to consider the morphological diversity of particular\nArabic words by using different sentiment classification levels.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In this paper, we investigate the benefits of integrating CNNs and LSTMs.\n* We report obtained improved accuracy for Arabic sentiment analysis on different datasets.\n* Additionally, we seek to consider the morphological diversity of particular Arabic words by using different sentiment classification levels."
    },
    {
        "abstract": "  We give a generalisation of the character formula of Deligne--Lusztig\nrepresentations from the finite field case to the truncated formal power series\ncase. Motivated by this generalisation, we give a definition of Green functions\nfor these local rings, and we prove some basic properties along the lines of\nthe finite field case, like a summation formula. Among the applications we show\nthat the higher Deligne--Lusztig characters and G\\'erardin's characters agree\nat regular semisimple elements. We also derive a generalisation of Braverman\nand Kazhdan's result on gamma functions for Deligne--Lusztig characters, with a\nmore elementary argument.\n",
        "method": "We give a definition of Green functions for these local rings, and we prove some basic properties along the lines of the finite field case, like a summation formula."
    },
    {
        "abstract": "  Neural network decoding algorithms are recently introduced by Nachmani et al.\nto decode high-density parity-check (HDPC) codes. In contrast with iterative\ndecoding algorithms such as sum-product or min-sum algorithms in which the\nweight of each edge is set to $1$, in the neural network decoding algorithms,\nthe weight of every edge depends on its impact in the transmitted codeword. In\nthis paper, we provide a novel \\emph{feed-forward neural network lattice\ndecoding algorithm} suitable to decode lattices constructed based on\nConstruction A, whose underlying codes have HDPC matrices. We first establish\nthe concept of feed-forward neural network for HDPC codes and improve their\ndecoding algorithms compared to Nachmani et al. We then apply our proposed\ndecoder for a Construction A lattice with HDPC underlying code, for which the\nwell-known iterative decoding algorithms show poor performances. The main\nadvantage of our proposed algorithm is that instead of assigning and training\nweights for all edges, which turns out to be time-consuming especially for\nhigh-density parity-check matrices, we concentrate on edges which are present\nin most of $4$-cycles and removing them gives a girth-$6$ Tanner graph. This\napproach, by slight modifications using updated LLRs instead of initial ones,\nsimultaneously accelerates the training process and improves the error\nperformance of our proposed decoding algorithm.\n",
        "method": "Here is the methodological sentence:\n\nWe provide a novel _feed-forward neural network lattice decoding algorithm_ suitable to decode lattices constructed based on Construction A, whose underlying codes have HDPC matrices."
    },
    {
        "abstract": "  Optical chirality is central to many industrial photonic technologies\nincluding enantiomer identification, ellipsometry-based tomography and spin\nmultiplexing in optical communication. However, a substantial chiral response\nrequires a typical three-dimensional (3D) constituent, thereby making the\nparadigm highly complex. Photonic devices integrated with\nmicroelectromechanical systems (MEMS) have shown potential for chiral light\ncontrol by external stimuli, but the stimuli usually demand a destructive\ndosage. Here, we report a simple synthetic chiral paradigm that is electrically\nprogrammable with self-assembled 3D MEMS cantilevers. This paradigm enables\nfour reconfigurable chiral schemes with dextrorotary, levorotary, racemic and\nachiral conformations. Optical response of reversible chirality and chiral to\nachiral switch are electrically encoded following an exclusive OR logical\noperation with dual-channel bias as low as 10 V. Our device demonstrates a\nroute to electrically actuated synthetic chiral platform and serves as a\npowerful conformation analysis tool for macromolecules in biology, medicine,\nchemistry and physics. The prototype delivers a new strategy towards\nreconfigurable stereoselective photonic applications.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"This paradigm enables four reconfigurable chiral schemes with dextrorotary, levorotary, racemic and achiral conformations.\""
    },
    {
        "abstract": "  It is (or should be) well known that specification of a heat bath requires\nboth a temperature and a 4-velocity, the rest frame of the heat bath. In static\nspacetimes there is a very natural and unique candidate for the 4-velocity of\nthe heat bath, the normalized timelike Killing vector. However in stationary\nnon-static spacetimes the situation is considerably more subtle, and several\ndifferent \"natural\" 4-velocity fields suitable for characterizing the rest\nframe of a heat bath can be defined - thus Buchdahl's 1949 analysis for the\nTolman temperature gradient in a stationary spacetime is only part of the\nstory. In particular, the heat bath most suitable for describing the Hawking\nradiation from a rotating black hole is best described in terms of a gradient\nflow normal to the spacelike hypersurfaces, not in terms of Killing vectors.\n",
        "method": "Here is the methodological sentence:\n\nThe situation is considerably more subtle, and several different \"natural\" 4-velocity fields suitable for characterizing the rest frame of a heat bath can be defined."
    },
    {
        "abstract": "  Recently, a microscopically motivated nuclear energy density functional was\nderived by applying the density matrix expansion to the Hartree-Fock (HF)\nenergy obtained from long-range chiral effective field theory two- and\nthree-nucleon interactions. However, the HF approach cannot account for all\nmany-body correlations. One class of correlations is included by\nBrueckner-Hartree-Fock (BHF) theory, which gives an improved definition of the\none-body HF potential by replacing the interaction by a reaction matrix $G$. In\nthis paper, we find that the difference between the $G$-matrix and the\nnucleon-nucleon potential $V_{\\mathrm{NN}}$ can be well accounted for by a\ntruncated series of contact terms. This is consistent with renormalization\ngroup decoupling generating a series of counterterms as short-distance physics\nis integrated out. The coefficients $C_{n}$ of the power series expansion $\\sum\nC_{n}q^{n}$ for the counterterms are examined for two potentials at different\nrenormalization group resolutions and at a range of densities. The success of\nthis expansion for $G-V_{\\mathrm{NN}}$ means we can apply the density matrix\nexpansion at the HF level with low-momentum interactions and density-dependent\nzero-range interactions to model BHF correlations.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Recently, a microscopically motivated nuclear energy density functional was derived by applying the density matrix expansion to the Hartree-Fock (HF) energy obtained from long-range chiral effective field theory two- and three-nucleon interactions.\n* One class of correlations is included by Brueckner-Hartree-Fock (BHF) theory, which gives an improved definition of the one-body HF potential by replacing the interaction by a reaction matrix $G$.\n* The difference between the $G$-matrix and the nucleon-nucleon potential $V_{\\mathrm{NN}}$ can be well accounted for by a truncated series of contact terms."
    },
    {
        "abstract": "  This paper proposes a novel attention model for semantic segmentation, which\naggregates multi-scale and context features to refine prediction. Specifically,\nthe skeleton convolutional neural network framework takes in multiple different\nscales inputs, by which means the CNN can get representations in different\nscales. The proposed attention model will handle the features from different\nscale streams respectively and integrate them. Then location attention branch\nof the model learns to softly weight the multi-scale features at each pixel\nlocation. Moreover, we add an recalibrating branch, parallel to where location\nattention comes out, to recalibrate the score map per class. We achieve quite\ncompetitive results on PASCAL VOC 2012 and ADE20K datasets, which surpass\nbaseline and related works.\n",
        "method": "The proposed attention model will handle the features from different scale streams respectively and integrate them."
    },
    {
        "abstract": "  We formulate a multi-group and multi-vector epidemic model in which hosts'\ndynamics is captured by staged-progression $SEIR$ framework and the dynamics of\nvectors is captured by an $SI$ framework. The proposed model describes the\nevolution of a class of zoonotic infections where the pathogen is shared by $m$\nhost species and transmitted by $p$ arthropod vector species. In each host, the\ninfectious period is structured into $n$ stages with a corresponding\ninfectiousness parameter to each vector species. We determine the basic\nreproduction number $\\mathcal{R}_0^2(m,n,p)$ and investigate the dynamics of\nthe systems when this threshold is less or greater than one. We show that the\ndynamics of the multi-host, multi-stage, and multi-vector system is completely\ndetermined by the basic reproduction number and the structure of the\nhost-vector network configuration. Particularly, we prove that the disease-free\n\\mbox{equilibrium} is globally asymptotically stable (GAS) whenever\n$\\mathcal{R}_0^2(m,n,p)<1$, and a unique strongly endemic equilibrium exists\nand is GAS if $\\mathcal{R}_0^2(m,n,p)>1$ and the host-vector configuration is\nirreducible. That is, either the disease dies out or persists in all hosts and\nall vector species.\n",
        "method": "The proposed model describes the evolution of a class of zoonotic infections where the pathogen is shared by $m$ host species and transmitted by $p$ arthropod vector species."
    },
    {
        "abstract": "  We address the problem of domain generalization where a decision function is\nlearned from the data of several related domains, and the goal is to apply it\non an unseen domain successfully. It is assumed that there is plenty of labeled\ndata available in source domains (also called as training domain), but no\nlabeled data is available for the unseen domain (also called a target domain or\ntest domain). We propose a novel neural network architecture, Domain2Vec (D2V)\nthat learns domain-specific embedding and then uses this embedding to\ngeneralize the learning across related domains. The proposed algorithm, D2V\nextends the idea of distribution regression and kernelized domain\ngeneralization to the neural networks setting. We propose a neural network\narchitecture to learn domain-specific embedding and then use this embedding\nalong with the data point specific features to label it. We show the\neffectiveness of the architecture by accurately estimating domain to domain\nsimilarity. We evaluate our algorithm against standard domain generalization\ndatasets for image classification and outperform other state of the art\nalgorithms.\n",
        "method": "We propose a novel neural network architecture, Domain2Vec (D2V) that learns domain-specific embedding and then uses this embedding to generalize the learning across related domains."
    },
    {
        "abstract": "  Using a sample of about 123,000 stars with accurate 3D velocity measurements\nfrom the LAMOST-TGAS data, we confirm the kinematic signature of the Galactic\nwarp found by Sch\\\"onrich & Dehnen recently. The data reveal a clear trend of\nincreasing mean vertical velocity $\\overline{V_{z}}$ as a function of absolute\nvertical angular momentum $L_{z}$ and azimuthal velocity $V_{\\phi}$ for guiding\ncenter radius $R_{g}$ between 6.0 and 10.5 kpc. The trend is consistent with a\nlarge-scale Galactic warp. Similar to Sch\\\"onrich & Dehnen, we also find a\nwave-like pattern of $\\overline{V_{z}}$ versus $L_{z}$ with an amplitude of\n$\\sim 0.9$ km s$^{-1}$ on a scale of $\\sim 2.0$ kpc, which could arise from\nbending waves or a winding warp. Finally, we confirm a prominent, localized\npeak in $\\overline{V_z}$ near $L_z \\sim 2150$ kpc km s$^{-1}$ (corresponding to\n$R_{g} \\sim 9$ kpc and $V_{\\phi} \\sim 255$ km s$^{-1}$). The additional\nline-of-sight velocity information from LAMOST reveals that stars in this\nfeature have a large, inward radial velocity of $V_{R} \\sim -13.33 \\pm 0.59$ km\ns$^{-1}$ and a small radial velocity dispersion $\\sigma_{R} \\sim 25.27 \\pm\n0.89$ km s$^{-1}$, suggesting that a stellar stream gives rise to this feature.\n",
        "method": "Here is the methodological sentence:\n\nUsing a sample of about 123,000 stars with accurate 3D velocity measurements from the LAMOST-TGAS data..."
    },
    {
        "abstract": "  Assessing the quality of 3D printed models before they are printed remains a\nchalleng- ing problem, particularly when considering point cloud-based models.\nThis paper introduces an approach to quality assessment, which uses techniques\nfrom the field of Topological Data Analy- sis (TDA) to compute a topological\nabstraction of the eventual printed model. Two main tools of TDA, Mapper and\npersistent homology, are used to analyze both the printed space and empty space\ncreated by the model. This abstraction enables investigating certain qualities\nof the model, with respect to print quality, and identifies potential anomalies\nthat may appear in the final product.\n",
        "method": "This paper introduces an approach to quality assessment, which uses techniques from the field of Topological Data Analysis (TDA) to compute a topological abstraction of the eventual printed model."
    },
    {
        "abstract": "  Given a mean curvature flow of compact, embedded $C^2$ surfaces satisfying\nNeumann free boundary condition on a mean convex, smooth support surface in\n3-dimensional Euclidean space, we show that it can be extended as long as its\nmean curvature and perimeter stay uniformly bounded along the flow.\n",
        "method": "Methodological sentence: Given a mean curvature flow of compact, embedded $C^2$ surfaces satisfying Neumann free boundary condition on a mean convex, smooth support surface in 3-dimensional Euclidean space..."
    },
    {
        "abstract": "  We show that the emergence of systemic risk in complex systems can be\nunderstood from the evolution of functional networks representing interactions\ninferred from fluctuation correlations between macroscopic observables.\nSpecifically, we analyze the long-term collective dynamics of the New York\nStock Exchange between 1926-2016, showing that periods marked by systemic\ncrisis, viz., around the Great Depression of 1929-33 and the Great Recession of\n2007-09, are associated with emergence of frustration indicated by the loss of\nstructural balance in the interaction networks. During these periods the\ndominant eigenmodes characterizing the collective behavior exhibit\ndelocalization leading to increased coherence in the dynamics. The topological\nstructure of the networks exhibits a slowly evolving trend marked by the\nemergence of a prominent core-periphery organization around both of the crisis\nperiods.\n",
        "method": "We analyze the long-term collective dynamics of the New York Stock Exchange between 1926-2016, showing that periods marked by systemic crisis..."
    },
    {
        "abstract": "  In this paper, we consider the problem of secure and reliable communication\nwith uncertain channel state information (CSI) and present a new solution named\nactive secure coding which combines the machine learning methods with the\ntraditional physical layer secure coding scheme. First, we build a detectable\nwiretap channel model by combining the hidden Markov model with the compound\nwiretap channel model, in which the varying of channel block CSI is a Markov\nprocess and the detected information is a stochastic emission from the current\nCSI. Next, we present a CSI learning scheme to learn the CSI from the detected\ninformation by the Baum-Welch and Viterbi algorithms. Then we construct\nexplicit secure polar codes based on the learned CSI, and combine it with the\nCSI learning scheme to form the active secure polar coding scheme. Simulation\nresults show that an acceptable level of reliability and security can be\nachieved by the proposed active secure polar coding scheme.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe build a detectable wiretap channel model by combining the hidden Markov model with the compound wiretap channel model, in which the varying of channel block CSI is a Markov process and the detected information is a stochastic emission from the current CSI."
    },
    {
        "abstract": "  Since the generative neural networks have made a breakthrough in the image\ngeneration problem, lots of researches on their applications have been studied\nsuch as image restoration, style transfer and image completion. However, there\nhas been few research generating objects in uncontrolled real-world\nenvironments. In this paper, we propose a novel approach for vehicle image\ngeneration in real-world scenes. Using a subnetwork based on a precedent work\nof image completion, our model makes the shape of an object. Details of objects\nare trained by an additional colorization and refinement subnetwork, resulting\nin a better quality of generated objects. Unlike many other works, our method\ndoes not require any segmentation layout but still makes a plausible vehicle in\nthe image. We evaluate our method by using images from Berkeley Deep Drive\n(BDD) and Cityscape datasets, which are widely used for object detection and\nimage segmentation problems. The adequacy of the generated images by the\nproposed method has also been evaluated using a widely utilized object\ndetection algorithm and the FID score.\n",
        "method": "Using a subnetwork based on a precedent work of image completion, our model makes the shape of an object."
    },
    {
        "abstract": "  We carried out multi-color optical monitoring of a sample of ten blazars from\n2005 to 2011. The sample contains 3 LBLs, 2 IBLs, 4 HBLs, and 1 FSRQ. Our\nmonitoring focused on the long-term variability and the sample included nine BL\nLac objects and one flat-spectrum radio quasar. A total number of 14799 data\npoints were collected. This is one of the largest optical database for a sample\nof ten blazars. All objects showed significant variability except OT 546.\nBecause of the low sampling on each single night, only BL Lacertae was observed\nto have intra-day variability on 2006 November 6. Most BL Lac objects showed a\nbluer-when-brighter chromatism, while the flat-spectrum radio quasar, 3C 454.3,\ndisplayed a redder-when-brighter trend. The BWB color behaviors of most BL Lacs\ncan be at least partly interpreted by the fact of increasing variation\namplitude with increasing frequency observed in these objects. The average\nspectral index of LBLs is around 1.5, as expected from the model dominated by\nSynchrotron Self-Compton (SSC) loss. The optical emission of HBL is probably\ncontaminated by the thermal emission from the host galaxies. Correlation\nanalysis did not reveal any time delay between variations at different\nwavelengths.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We carried out multi-color optical monitoring of a sample of ten blazars from 2005 to 2011.\n* All objects showed significant variability except OT 546.\n* Because of the low sampling on each single night, only BL Lacertae was observed to have intra-day variability on 2006 November 6.\n* A total number of 14799 data points were collected."
    },
    {
        "abstract": "  We propose a method to infer domain-specific models such as classifiers for\nunseen domains, from which no data are given in the training phase, without\ndomain semantic descriptors. When training and test distributions are\ndifferent, standard supervised learning methods perform poorly. Zero-shot\ndomain adaptation attempts to alleviate this problem by inferring models that\ngeneralize well to unseen domains by using training data in multiple source\ndomains. Existing methods use observed semantic descriptors characterizing\ndomains such as time information to infer the domain-specific models for the\nunseen domains. However, it cannot always be assumed that such metadata can be\nused in real-world applications. The proposed method can infer appropriate\ndomain-specific models without any semantic descriptors by introducing the\nconcept of latent domain vectors, which are latent representations for the\ndomains and are used for inferring the models. The latent domain vector for the\nunseen domain is inferred from the set of the feature vectors in the\ncorresponding domain, which is given in the testing phase. The domain-specific\nmodels consist of two components: the first is for extracting a representation\nof a feature vector to be predicted, and the second is for inferring model\nparameters given the latent domain vector. The posterior distributions of the\nlatent domain vectors and the domain-specific models are parametrized by neural\nnetworks, and are optimized by maximizing the variational lower bound using\nstochastic gradient descent. The effectiveness of the proposed method was\ndemonstrated through experiments using one regression and two classification\ntasks.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe posterior distributions of the latent domain vectors and the domain-specific models are parametrized by neural networks, and are optimized by maximizing the variational lower bound using stochastic gradient descent."
    },
    {
        "abstract": "  We study active run-and-tumble particles with an additional two-state\ninternal variable characterizing their motile or non-motile state. Motile\nparticles change irreversibly into non-motile ones upon collision with a\nnon-motile particle. The system evolves towards an absorbing state where all\nparticles are non-motile. We initialize the system with one non-motile\nparticles in a bath of motile ones and study numerically the kinetics of\nrelaxation to absorbing state and its structure as function of the density of\nthe initial bath of motile particles and of their tumbling rate. We find a\ncrossover from fractal aggregates at low density to homogeneous ones at high\ndensity. The persistence of single-particle dynamics as quantified by the\ntumbling rate pushes this crossover to higher density and can be used to tune\nthe porosity of the aggregate. At the lowest density the fractal dimension of\nthe aggregate approaches that obtained in single-particle diffusion limited\naggregation. Our results could be exploited for the design of structures of\ndesired porosity. The model is a first step towards the study of the collective\ndynamics of active particles that can exchange biological information.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We initialize the system with one non-motile particle in a bath of motile ones and study numerically the kinetics of relaxation to absorbing state...\n* We find... the structure as function of the density of the initial bath of motile particles and of their tumbling rate."
    },
    {
        "abstract": "  Weakly supervised temporal action detection is a Herculean task in\nunderstanding untrimmed videos, since no supervisory signal except the\nvideo-level category label is available on training data. Under the supervision\nof category labels, weakly supervised detectors are usually built upon\nclassifiers. However, there is an inherent contradiction between classifier and\ndetector; i.e., a classifier in pursuit of high classification performance\nprefers top-level discriminative video clips that are extremely fragmentary,\nwhereas a detector is obliged to discover the whole action instance without\nmissing any relevant snippet. To reconcile this contradiction, we train a\ndetector by driving a series of classifiers to find new actionness clips\nprogressively, via step-by-step erasion from a complete video. During the test\nphase, all we need to do is to collect detection results from the one-by-one\ntrained classifiers at various erasing steps. To assist in the collection\nprocess, a fully connected conditional random field is established to refine\nthe temporal localization outputs. We evaluate our approach on two prevailing\ndatasets, THUMOS'14 and ActivityNet. The experiments show that our detector\nadvances state-of-the-art weakly supervised temporal action detection results,\nand even compares with quite a few strongly supervised methods.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe train a detector by driving a series of classifiers to find new actionness clips progressively, via step-by-step erasion from a complete video."
    },
    {
        "abstract": "  It is often of interest to find communities in network data for unsupervised\nlearning, feature discovery, anomaly detection, or scientific study. The vast\nmajority of community detection methods proceed via optimization of a quality\nfunction, which is possible even on random networks without communities.\nTherefore there is usually not an easy way to tell if a community is\n\"significant\", in this context meaning more internally connected than would be\nexpected under a random graph model without communities. This paper generalizes\nexisting null models for this purpose to bipartite graphs, and introduces a new\nsignificance scoring algorithm called Fast Optimized Community Significance\n(FOCS) that is highly scalable and agnostic to the type of graph. Furthermore,\ncompared with existing methods on unipartite graphs, FOCS is more numerically\nstable and better balances the trade-off between detection power and false\npositives.\n",
        "method": "The vast majority of community detection methods proceed via optimization of a quality function, which is possible even on random networks without communities."
    },
    {
        "abstract": "  This paper investigates an efficient algorithm for trajectory planning\nproblem of autonomous unmanned aerial vehicles which fly over three-dimensional\nterrains. The proposed algorithm combines convex optimization with disjunctive\nprogramming and receding horizon concept, which has many advantages, such as a\nhigh computational speed. Disjunctive programming is applied in order to relax\nthe non-convex constraints of the problem. Moreover, the B-spline curves are\nemployed to represent the trajectories which should be generated in the\noptimization process.\n",
        "method": "The proposed algorithm combines convex optimization with disjunctive programming and receding horizon concept, which has many advantages, such as a high computational speed."
    },
    {
        "abstract": "  We prove an extended lifespan result for the full gravity-capillary water\nwaves system with a $2$ dimensional periodic interface: for initial data of\nsufficiently small size $\\varepsilon$, smooth solutions exist up to times of\nthe order of $\\varepsilon^{-5/3+}$, for almost all values of the gravity and\nsurface tension parameters. Besides the quasilinear nature of the equations,\nthe main difficulty is to handle the weak small divisors bounds for quadratic\nand cubic interactions, growing with the size of the largest frequency. To\novercome this difficulty we use (1) the (Hamiltonian) structure of the\nequations which gives additional smoothing close to the resonant hypersurfaces,\n(2) another structural property, connected to time-reversibility, that allows\nus to handle \"trivial\" cubic resonances, (3) sharp small divisors lower bounds\non three and four-waves modulation functions based on counting arguments, and\n(4) partial normal form transformations and symmetrization arguments in the\nFourier space. Our theorem appears to be the first extended lifespan result for\nquasilinear equations with non-trivial resonances on a multi-dimensional torus.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We use (1) the Hamiltonian structure of the equations which gives additional smoothing close to the resonant hypersurfaces, (2) another structural property, connected to time-reversibility, that allows us to handle \"trivial\" cubic resonances, (3) sharp small divisors lower bounds on three and four-waves modulation functions based on counting arguments, and (4) partial normal form transformations and symmetrization arguments in the Fourier space.\""
    },
    {
        "abstract": "  Bitcoin is the first secure decentralized electronic currency system.\nHowever, it is known to be inefficient due to its proof-of-work (PoW) consensus\nalgorithm and has the potential hazard of double spending. In this paper, we\naim to reduce the probability of double spending by decreasing the probability\nof consecutive winning. We first formalize a PoW-based decentralized secure\nnetwork model in order to present a quantitative analysis. Next, to resolve the\nrisk of double spending, we propose the personalized difficulty adjustment\n(PDA) mechanism which modifies the difficulty of each participant such that\nthose who win more blocks in the past few rounds have a smaller probability to\nwin in the next round. To analyze the performance of the PDA mechanism, we\nobserve that the system can be modeled by a high-order Markov chain. Finally,\nwe show that PDA effectively decreases the probability of consecutive winning\nand results in a more trustworthy PoW-based system.\n",
        "method": "We formalize a PoW-based decentralized secure network model in order to present a quantitative analysis."
    },
    {
        "abstract": "  Mesoscopic mechanical oscillators can be prepared in quantum states and\ncoherently manipulated using the optomechanical interaction. This has recently\nbeen used to prepare squeezed mechanical states. However, the scheme used in\nthese experiments relies on slow, dissipative evolution that destroys the\nsystem's memory of its initial state. In this paper we propose a protocol based\non a sequence of four pulsed optomechanical interactions. In addition to being\ncoherent, our scheme executes in a time much shorter than a mechanical period.\nWe analyse applications in impulsive force sensing and preservation of\ncontinuous-variable quantum information.\n",
        "method": "Here is the methodological sentence:\n\nIn this paper we propose a protocol based on a sequence of four pulsed optomechanical interactions."
    },
    {
        "abstract": "  The physical topology is emerging as the next frontier in an ongoing effort\nto render communication networks more flexible. While first empirical results\nindicate that these flexibilities can be exploited to reconfigure and optimize\nthe network toward the workload it serves and, e.g., providing the same\nbandwidth at lower infrastructure cost, only little is known today about the\nfundamental algorithmic problems underlying the design of reconfigurable\nnetworks. This paper initiates the study of the theory of demand-aware,\nself-adjusting networks. Our main position is that self-adjusting networks\nshould be seen through the lense of self-adjusting datastructures. Accordingly,\nwe present a taxonomy classifying the different algorithmic models of\ndemand-oblivious, fixed demand-aware, and reconfigurable demand-aware networks,\nintroduce a formal model, and identify objectives and evaluation metrics. We\nalso demonstrate, by examples, the inherent advantage of demand-aware networks\nover state-of-the-art demand-oblivious, fixed networks (such as expanders).\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Our main position is that self-adjusting networks should be seen through the lense of self-adjusting datastructures.\n* We present a taxonomy classifying the different algorithmic models of demand-oblivious, fixed demand-aware, and reconfigurable demand-aware networks, introduce a formal model, and identify objectives and evaluation metrics."
    },
    {
        "abstract": "  Measurement feedback is a versatile and powerful tool, although its\nperformance is limited by several practical imperfections resulting from\nclassical components. This paper shows that, for some typical quantum feedback\ncontrol problems for state preparation (stabilization of a qubit or a qutrit,\nspin squeezing, and Fock state generation), the classical feedback operation\ncan be replaced by a fully quantum one such that the state autonomously\ndissipates into the target or a state close to the target. The main common\nfeature of the proposed quantum operation, which is called coherent feedback,\nis that it is composed of a series of dispersive and dissipative couplings\ninspired by the corresponding measurement feedback scheme.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The main common feature of the proposed quantum operation, which is called coherent feedback, is that it is composed of a series of dispersive and dissipative couplings inspired by the corresponding measurement feedback scheme.\""
    },
    {
        "abstract": "  The Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) state in quasi-one-dimensional\nsystems with warped Fermi surfaces is examined in strong parallel magnetic\nfields. It is shown that the state is extremely stable for field directions\naround nontrivial optimum directions, at which the upper critical field\nexhibits cusps, and that the stabilization is due to a Fermi-surface effect\nanalogous to the nesting effect for the spin density wave and charge density\nwave. Interestingly, the behavior with cusps is analogous to that in a square\nlattice system in which the hole density is controlled. For the organic\nsuperconductor (TMTSF)_2ClO_4, when the hopping parameters obtained by previous\nauthors based on X-ray crystallography results are assumed, the optimum\ndirections are in quadrants consistent with the previous experimental\nobservations. Furthermore, near this set of parameters, we also find sets of\nhopping parameters that more precisely reproduce the observed optimum in-plane\nfield directions. These results are consistent with the hypothesis that the\nFFLO state is realized in the organic superconductor.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"It is shown that the state is extremely stable for field directions around nontrivial optimum directions, at which the upper critical field exhibits cusps...\""
    },
    {
        "abstract": "  We compare the evolution of voids formed under the standard cosmological\nmodel and two alternative cosmological models. The two models are a\nquintessence model ($\\phi$CDM) and a Coupled Dark Matter-Dark Energy (CDE)\nmodel, both of which have evolving and interacting dark sectors. From $N$-body\nadiabatic hydrodynamical simulations of these models, we measure the statistics\nand quantify the properties of voids over the redshift range $z=1.5-12$: these\ninclude their population size, volumes, shapes and average densities. We find\nthat the latter property has potential as a probe of cosmology, particularly\ndark energy, as significant differences in average void densities exist between\nthe alternative models and the standard model. We postulate that this signature\narises from an increased evacuation rate of particles out of voids, or an\nearlier start to void evacuation, in the alternative models as a direct\nconsequence of the dynamical scalar field, which also leads to greater void\nmerger rates. Additionally, differences between the two alternative models are\nlikely due to the drag force arising from dark sector coupling, acting on dark\nmatter particles in our coupled model.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nFrom $N$-body adiabatic hydrodynamical simulations of these models, we measure the statistics and quantify the properties of voids over the redshift range $z=1.5-12$."
    },
    {
        "abstract": "  This paper presents a deep architecture for dense semantic correspondence,\ncalled pyramidal affine regression networks (PARN), that estimates\nlocally-varying affine transformation fields across images. To deal with\nintra-class appearance and shape variations that commonly exist among different\ninstances within the same object category, we leverage a pyramidal model where\naffine transformation fields are progressively estimated in a coarse-to-fine\nmanner so that the smoothness constraint is naturally imposed within deep\nnetworks. PARN estimates residual affine transformations at each level and\ncomposes them to estimate final affine transformations. Furthermore, to\novercome the limitations of insufficient training data for semantic\ncorrespondence, we propose a novel weakly-supervised training scheme that\ngenerates progressive supervisions by leveraging a correspondence consistency\nacross image pairs. Our method is fully learnable in an end-to-end manner and\ndoes not require quantizing infinite continuous affine transformation fields.\nTo the best of our knowledge, it is the first work that attempts to estimate\ndense affine transformation fields in a coarse-to-fine manner within deep\nnetworks. Experimental results demonstrate that PARN outperforms the\nstate-of-the-art methods for dense semantic correspondence on various\nbenchmarks.\n",
        "method": "To deal with intra-class appearance and shape variations that commonly exist among different instances within the same object category, we leverage a pyramidal model where affine transformation fields are progressively estimated in a coarse-to-fine manner so that the smoothness constraint is naturally imposed within deep networks."
    },
    {
        "abstract": "  Distributing entangled pairs is a fundamental operation required for many\nquantum information science and technology tasks. In a general entanglement\ndistribution scheme, a photonic pulse is used to entangle a pair of remote\nquantum memories. Most applications require multiple entangled pairs between\nremote users, which in turn necessitates several photonic pulses (single\nphotons) being sent through the channel connecting those users. Here we present\nan entanglement distribution scheme using only a single photonic pulse to\nentangle an arbitrary number of remote quantum memories. As a consequence the\nspatial temporal resources are dramatically reduced. We show how this approach\ncan be simultaneously combined with an entanglement purification protocol to\ngenerate even higher fidelity entangled pairs. The combined approach is faster\nto generate those high quality pairs and requires less resources in terms of\nboth matter qubits and photons consumed. To estimate the efficiency of our\nscheme we derive a normalized rate taking into account the raw rate at which\nthe users can generate purified entangled pairs divided by the total resources\nused. We compare the efficiency of our system with the Deutsch protocol in\nwhich the entangled pairs have been created in a traditional way. Our scheme\noutperforms this approach both in terms of generation rate and resources\nrequired. Finally we show how our approach can be extended to more general\nerror correction and detection schemes with higher normalized generation rates\nnaturally occurring.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe present an entanglement distribution scheme using only a single photonic pulse to entangle an arbitrary number of remote quantum memories."
    },
    {
        "abstract": "  We propose an intuitive approach of detecting pancreatic ductal\nadenocarcinoma (PDAC), the most common type of pancreatic cancer, by checking\nabdominal CT scans. Our idea is named multi-scale\nsegmentation-for-classification, which classifies volumes by checking if at\nleast a sufficient number of voxels is segmented as tumors, by which we can\nprovide radiologists with tumor locations. In order to deal with tumors with\ndifferent scales, we train and test our volumetric segmentation networks with\nmulti-scale inputs in a coarse-to-fine flowchart. A post-processing module is\nused to filter out outliers and reduce false alarms. We collect a new dataset\ncontaining 439 CT scans, in which 136 cases were diagnosed with PDAC and 303\ncases are normal, which is the largest set for PDAC tumors to the best of our\nknowledge. To offer the best trade-off between sensitivity and specificity, our\nproposed framework reports a sensitivity of 94.1% at a specificity of 98.5%,\nwhich demonstrates the potential to make a clinical impact.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe train and test our volumetric segmentation networks with multi-scale inputs in a coarse-to-fine flowchart."
    },
    {
        "abstract": "  We study the limitations on coherence evolutions under the constraints of\nthermodynamic laws, and focus on the optimal thermal operations (TO) reaching\nthe bounds. For qubit case, we find a thermal operation involving only a\nsingle-mode reservoir (STO) which maintains the maximum coherence allowed by\ngeneral TO. For higher dimensions, we derive general bounds on coherence\nmerging under TO, and find STO to reach the bounds. By applying the bound to a\ntwo-qubit system, we prove that erasing correlations while preserving the\nmarginal states is not free in the resource theory of thermodynamics. Due to\nthe simple structure of STO and its strong ability in coherence processing, our\nresults shed light on both theoretical and experimental studies in the field of\nthermodynamics for small quantum systems.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We find a thermal operation involving only a single-mode reservoir (STO) which maintains the maximum coherence allowed by general TO.\n* We derive general bounds on coherence merging under TO, and find STO to reach the bounds.\n* By applying the bound to a two-qubit system, we prove that erasing correlations while preserving the marginal states is not free in the resource theory of thermodynamics."
    },
    {
        "abstract": "  Color centers in diamond are promising candidates for quantum nanosensing\napplications. The efficient collection of the optical signal is the key to\nachieving high sensitivity and resolution, but it is limited by the collection\noptics. Embedding the color centers in diamond microstructures can help to\nenhance the collection efficiency, but often require challenging fabrication\nand integration. Here we investigate the photoluminescence (PL) of\nsilicon-vacancy (SiV) centers in commercially available atomic force microscope\n(AFM) diamond pyramid (DP) tips. We find that the DP geometry efficiently\nchannels PL emitted at the DP apex towards the base, where we experimentally\ndemonstrate an enhanced PL collection of up to 8 times higher compared to other\ndirections. Our experimental observations are in good agreement with numerical\nsimulations using a finite-difference time-domain (FDTD) method. Our results\nindicate that AFM tips could be an economical, efficient and straightforward\nway of implementing color-center-based nanosensing as they provide enhanced\nsensitivity and easy integration with existing AFM platforms.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe find that the DP geometry efficiently channels PL emitted at the DP apex towards the base, where we experimentally demonstrate an enhanced PL collection of up to 8 times higher compared to other directions."
    },
    {
        "abstract": "  We show a method to determine topological complexity from the fibrewise view\npoint, which provides an alternative proof for tc(K) = 4, where K denotes Klein\nbottle.\n",
        "method": "We show a method to determine topological complexity from the fibrewise view point, which provides an alternative proof for tc(K) = 4, where K denotes Klein bottle."
    },
    {
        "abstract": "  The closed Dyson-Schwinger equation for the 2-point function of the\nnoncommutative $\\lambda \\phi^4_2$-model is rearranged into the boundary value\nproblem for a sectionally holomorphic function in two variables. We prove an\nexact formula for a solution in terms of Lambert's $W$-function. This solution\nis holomorphic in $\\lambda$ inside a domain which contains $(-1/\\log\n4,\\infty)$. Our methods include the Hilbert transform, perturbation series and\nLagrange-B\\\"urmann resummation.\n",
        "method": "Our methods include the Hilbert transform, perturbation series and Lagrange-B\u00fcrmann resummation."
    },
    {
        "abstract": "  Diamond materials are central to an increasing range of advanced\ntechnological demonstrations, from high power electronics, to nano-scale\nquantum bio-imaging with unprecedented sensitivity. However, the full\nexploitation of diamond for these applications is often limited by the\nuncontrolled nature of the diamond material surface, which suffers from\nFermi-level pinning and hosts a significant density of electro-magnetic noise\nsources. These issues occur despite the oxide-free and air-stable nature of the\ndiamond crystal surface, which should be an ideal candidate for\nfunctionalization and chemical-engineering. In this work we reveal a family of\npreviously unidentified and near-ubiquitous primal surface defects which we\nassign to differently reconstructed surface vacancies. The density of these\ndefects is quantified with X-ray absorption spectroscopy, their energy\nstructures are elucidated by ab initio calculations, and their effect on\nnear-surface quantum probes is measured directly. Subsequent ab-initio\ncalculation of band-bending from these defects suggest they are the source of\nFermi-level pinning at most diamond surfaces. Finally, an investigation is\nconducted on a broad range of post-growth surface treatments and concludes that\nnone of them can reproducibly reduce this defect density below the\nFermi-pinning threshold, making this defect a prime candidate as the source for\ndecoherence-limiting noise in near-surface quantum probes.\n",
        "method": "However, the full exploitation of diamond for these applications is often limited by the uncontrolled nature of the diamond material surface, which suffers from Fermi-level pinning and hosts a significant density of electro-magnetic noise sources."
    },
    {
        "abstract": "  Human Activity Recognition in RGB-D videos has been an active research topic\nduring the last decade. However, no efforts have been found in the literature,\nfor recognizing human activity in RGB-D videos where several performers are\nperforming simultaneously. In this paper we introduce such a challenging\ndataset with several performers performing the activities. We present a novel\nmethod for recognizing human activities in such videos. The proposed method\naims in capturing the motion information of the whole video by producing a\ndynamic image corresponding to the input video. We use two parallel ResNext-101\nto produce the dynamic images for the RGB video and depth video separately. The\ndynamic images contain only the motion information and hence, the unnecessary\nbackground information are eliminated. We send the two dynamic images extracted\nfrom the RGB and Depth videos respectively, through a fully connected layer of\nneural networks. The proposed dynamic image reduces the complexity of the\nrecognition process by extracting a sparse matrix from a video. However, the\nproposed system maintains the required motion information for recognizing the\nactivity. The proposed method has been tested on the MSR Action 3D dataset and\nhas shown comparable performances with respect to the state-of-the-art. We also\napply the proposed method on our own dataset, where the proposed method\noutperforms the state-of-the-art approaches.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe use two parallel ResNext-101 to produce the dynamic images for the RGB video and depth video separately."
    },
    {
        "abstract": "  Taking into account the recently developed van der Waals (VDW) like equation\nof state (EoS) for grand canonical ensemble of fermions, the temperature\ndependent profiles of normalized entropy density ($s /T^3$) and the ratio of\nshear viscosity and entropy density ($\\eta/ s$) for hadron resonance gas have\nbeen evaluated. The VDW parameters, corresponding to interactions between\n(anti)baryons, have been obtained by contrasting lattice EoS for QCD matter at\nfinite chemical potentials ($\\mu_{B}$) and for $T \\le$ 160 MeV. The temperature\nand chemical potential dependent study of $s /T^3$ and $\\eta /s$ for hadron\ngas, by signalling onsets of first order phase transition and crossover in the\nhadronic phase of QCD matter, helps in understanding the QCD phase diagram in\nthe ($T, \\mu_{B}$) - plane. An estimation of probable location of critical\npoint matches predictions from other recent studies.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe VDW parameters, corresponding to interactions between (anti)baryons, have been obtained by contrasting lattice EoS for QCD matter at finite chemical potentials ($\\mu_{B}$) and for $T \\le$ 160 MeV."
    },
    {
        "abstract": "  We use an exact solution to the fundamental finite Kronig-Penney model with\narbitrary positions and strengths of scattering sites to show that this iconic\nmodel can possess topologically non-trivial properties. By using free\nparameters of the system as extra dimensions we demonstrate the appearance of\ntopologically protected edge states as well as the emergence of a Hofstadter\nbutterfly-like quasimomentum spectrum, even in the case of small numbers of\nscattering sites. We investigate the behaviour of the system in the weak and\nstrong scattering regimes and observe drastically different shapes of the\nquasimomentum spectrum.\n",
        "method": "We use an exact solution to the fundamental finite Kronig-Penney model with arbitrary positions and strengths of scattering sites..."
    },
    {
        "abstract": "  An elegant method to circumvent quantum measurement backaction is the use of\nquantum mechanics free subsystems (QMFS), with one approach involving the use\nof two oscillators with effective masses of opposite signs. Since negative\nenergies, and hence masses, are a characteristic of relativistic systems a\nnatural question is to what extent QMFS can be realized in this context. Using\nthe example of a one-dimensional Dirac oscillator we investigate conditions\nunder which this can be achieved, and identify Zitterbewegung or virtual pair\ncreation as the physical mechanism that fundamentally limits the feasibility of\nthe scheme. We propose a tabletop implementation of a Dirac oscillator system\nbased on a spin-orbit coupled ultracold atomic sample that allows for a direct\nobservation of the corresponding analog of virtual pair creation on quantum\nmeasurement backaction.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We investigate conditions under which [QMFS] can be achieved...\""
    },
    {
        "abstract": "  The accurate quantification of left ventricular (LV) deformation/strain shows\nsignificant promise for quantitatively assessing cardiac function for use in\ndiagnosis and therapy planning (Jasaityte et al., 2013). However, accurate\nestimation of the displacement of myocardial tissue and hence LV strain has\nbeen challenging due to a variety of issues, including those related to\nderiving tracking tokens from images and following tissue locations over the\nentire cardiac cycle. In this work, we propose a point matching scheme where\ncorrespondences are modeled as flow through a graphical network. Myocardial\nsurface points are set up as nodes in the network and edges define neighborhood\nrelationships temporally. The novelty lies in the constraints that are imposed\non the matching scheme, which render the correspondences one-to-one through the\nentire cardiac cycle, and not just two consecutive frames. The constraints also\nencourage motion to be cyclic, which is an important characteristic of LV\nmotion. We validate our method by applying it to the estimation of quantitative\nLV displacement and strain estimation using 8 synthetic and 8 open-chested\ncanine 4D echocardiographic image sequences, the latter with sonomicrometric\ncrystals implanted on the LV wall. We were able to achieve excellent tracking\naccuracy on the synthetic dataset and observed a good correlation with\ncrystal-based strains on the in-vivo data.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In this work, we propose a point matching scheme where correspondences are modeled as flow through a graphical network.\n* Myocardial surface points are set up as nodes in the network and edges define neighborhood relationships temporally.\n* We validate our method by applying it to the estimation of quantitative LV displacement and strain estimation using 8 synthetic and 8 open-chedsted canine 4D echocardiographic image sequences, the latter with sonomicrometric crystals implanted on the LV wall."
    },
    {
        "abstract": "  The Shocked POststarburst Galaxy Survey (SPOGS) aims to identify galaxies in\nthe transitional phase between actively star-forming and quiescence with\nnebular lines that are excited from shocks rather than star formation\nprocesses. We explored the ultraviolet (UV) properties of objects with\nnear-ultraviolet (NUV) and far-ultraviolet (FUV) photometry from archival GALEX\ndata; 444 objects were detected in both bands, 365 in only NUV, and 24 in only\nFUV, for a total of 833 observed objects. We compared SPOGs to samples of\nStar-forming galaxies (SFs), Quiescent galaxies (Qs), classical E+A\npost-starburst galaxies, active galactic nuclei (AGN) host galaxies, and\ninteracting galaxies. We found that SPOGs have a larger range in their FUV-NUV\nand NUV-r colors compared to most of the other samples, although all of our\ncomparison samples occupied color space inside of the SPOGs region. Based on\ntheir UV colors, SPOGs are a heterogeneous group, possibly made up of a mixture\nof SFs, Qs, and/or AGN. Using Gaussian mixture models, we are able to recreate\nthe distribution of FUV-NUV colors of SPOGs and E+A galaxies with different\ncombinations of SFs, Qs, and AGN. We find that the UV colors of SPOGs require a\n>60% contribution from SFs, with either Qs or AGN representing the remaining\ncontribution, while UV colors of E+A galaxies required a significantly lower\nfraction of SFs, supporting the idea that SPOGs are at an earlier point in\ntheir transition from quiescent to star-forming than E+A galaxies.\n",
        "method": "We explored the ultraviolet (UV) properties of objects with near-ultraviolet (NUV) and far-ultraviolet (FUV) photometry from archival GALEX data; 444 objects were detected in both bands, 365 in only NUV, and 24 in only FUV, for a total of 833 observed objects."
    },
    {
        "abstract": "  Traditional code search engines often do not perform well with natural\nlanguage queries since they mostly apply keyword matching. These engines thus\nneed carefully designed queries containing information about programming APIs\nfor code search. Unfortunately, existing studies suggest that preparing an\neffective code search query is both challenging and time consuming for the\ndevelopers. In this paper, we propose a novel API recommendation\ntechnique--RACK that recommends a list of relevant APIs for a natural language\nquery for code search by exploiting keyword-API associations from the\ncrowdsourced knowledge of Stack Overflow. We first motivate our technique using\nan exploratory study with 11 core Java packages and 344K Java posts from Stack\nOverflow. Experiments using 150 code search queries randomly chosen from three\nJava tutorial sites show that our technique recommends correct API classes\nwithin the top 10 results for about 79% of the queries which is highly\npromising. Comparison with two variants of the state-of-the-art technique also\nshows that RACK outperforms both of them not only in Top-K accuracy but also in\nmean average precision and mean recall by a large margin.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We first motivate our technique using an exploratory study with 11 core Java packages and 344K Java posts from Stack Overflow.\n* Experiments using 150 code search queries randomly chosen from three Java tutorial sites..."
    },
    {
        "abstract": "  We apply support vector machine (SVM) to study the phase transition between\nmany-body localized and thermal phases in a disordered quantum Ising chain in a\ntransverse external field. The many-body eigenstate energy $E$ is bounded by a\nbandwidth $W=E_{max}-E_{min}$. The transition takes place on a phase diagram\nspanned by the energy density $\\epsilon=2(E-E_{min})/W$ and the disorder\nstrength $\\delta J$ of the spin interaction uniformly distributed within\n$[-\\delta J, \\delta J]$, formally parallel to the mobility edge in Anderson\nlocalization. In our study we use the labeled probability density of eigenstate\nwavefunctions belonging to the deeply localized and thermal regimes at two\ndifferent energy densities ($\\epsilon$'s) as the training set, i.e., providing\nlabeled data at four corners of the phase diagram. Then we employ the trained\nSVM to predict the whole phase diagram. The obtained phase boundary\nqualitatively agrees with previous work using entanglement entropy to\ncharacterize these two phases. We further analyze the decision function of the\nSVM to interpret its physical meaning and find that it is analogous to the\ninverse participation ratio in configuration space. Our findings demonstrate\nthe ability of the SVM to capture potential quantities that may characterize\nthe many-body localization phase transition.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We apply support vector machine (SVM) to study the phase transition between many-body localized and thermal phases in a disordered quantum Ising chain in a transverse external field.\n* We use the labeled probability density of eigenstate wavefunctions belonging to the deeply localized and thermal regimes at two different energy densities ($\\epsilon$'s) as the training set, i.e., providing labeled data at four corners of the phase diagram.\n* Then we employ the trained SVM to predict the whole phase diagram."
    },
    {
        "abstract": "  For different values of $\\gamma \\geq 0$, analysis of the end behavior of the\nsequence $a_n = \\cos (n)^{n^\\gamma}$ yields a strong connection to the\nirrationality measure of $\\pi$. We show that if $\\limsup |\\cos n|^{n^2} \\neq\n1$, then the irrationality measure of $\\pi$ is exactly 2. We also give some\nnumerical evidence to support the conjecture that $\\mu(\\pi)=2$, based on the\nappearance of some startling subsequences of $\\cos(n)^n$.\n",
        "method": "Here are the methodological sentences:\n\nNone provided in this abstract."
    },
    {
        "abstract": "  We prove new results on the existence of positive radial solutions of the\nelliptic equation $-\\Delta u= \\lambda h(|x|,u)$ in an annular domain in\n$\\mathbb{R}^{N}, N\\geq 2$. Existence of positive radial solutions are\ndetermined under the conditions that the nonlinearity function $h(t,u)$ is\neither superlinear or sublinear growth in $u$ or satisfies some upper and lower\ninequalities on $h$. Our discussion is based on a fixed point theorem due to a\nrevised version of a fixed point theorem of Gustafson and Schmitt.\n",
        "method": "Methodological sentence:\n\nOur discussion is based on a fixed point theorem due to a revised version of a fixed point theorem of Gustafson and Schmitt."
    },
    {
        "abstract": "  BigDatalog is an extension of Datalog that achieves performance and\nscalability on both Apache Spark and multicore systems to the point that its\ngraph analytics outperform those written in GraphX. Looking back, we see how\nthis realizes the ambitious goal pursued by deductive database researchers\nbeginning forty years ago: this is the goal of combining the rigor and power of\nlogic in expressing queries and reasoning with the performance and scalability\nby which relational databases managed Big Data. This goal led to Datalog which\nis based on Horn Clauses like Prolog but employs implementation techniques,\nsuch as Semi-naive Fixpoint and Magic Sets, that extend the bottom-up\ncomputation model of relational systems, and thus obtain the performance and\nscalability that relational systems had achieved, as far back as the 80s, using\ndata-parallelization on shared-nothing architectures. But this goal proved\ndifficult to achieve because of major issues at (i) the language level and (ii)\nat the system level. The paper describes how (i) was addressed by simple rules\nunder which the fixpoint semantics extends to programs using count, sum and\nextrema in recursion, and (ii) was tamed by parallel compilation techniques\nthat achieve scalability on multicore systems and Apache Spark. This paper is\nunder consideration for acceptance in Theory and Practice of Logic Programming\n(TPLP).\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* This paper describes how [issue (i)] was addressed by simple rules under which the fixpoint semantics extends to programs using count, sum and extrema in recursion.\n* [Issue (ii)] was tamed by parallel compilation techniques that achieve scalability on multicore systems and Apache Spark."
    },
    {
        "abstract": "  A high-resolution wave climate projection for the northwestern Atlantic Ocean\nhas been conducted to help assess possible regional impacts due to global\nclimate change. The spectral wave model NOAA WAVEWATCH III is utilized with\nthree coupled (two-way) grids to resolve the northwestern Atlantic and coastal\nsouthern and eastern USA at approximately 21 km and 7 km respectively, and\ncovers the periods 1979--2003 (historic) and 2075--2099 (future). Hourly wind\nfield forcings are provided by a high-resolution AGCM (MRI-AGCM 3.2S; 21 km)\nand allow for better modeling of large storm events (important for extreme\nevent statistics). Climatological (25-year) comparisons between future and\nhistorical periods indicate significant wave heights will decrease in the\nnorthwestern Atlantic Ocean (-5.7 %) and Gulf of Mexico (-4.7 %) but increase\nin the Caribbean Sea (2.4 %). Comparisons also indicate that large changes in\nmean wave direction will occur in the Gulf of Mexico (5.0{\\deg}), with the\nlargest occurring west of the Florida peninsula (over 15{\\deg}).\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The spectral wave model NOAA WAVEWATCH III is utilized with three coupled (two-way) grids to resolve the northwestern Atlantic and coastal southern and eastern USA at approximately 21 km and 7 km respectively...\""
    },
    {
        "abstract": "  We prove that the classic logarithmic barrier problem is equivalent to a\nparticular logarithmic barrier positive relaxation problem with barrier and\nscaling parameters. Based on the equivalence, a line-search primal-dual\ninterior-point relaxation method for nonlinear programs is presented. Our\nmethod does not require any primal or dual iterates to be interior-points,\nwhich is prominently different from the existing interior-point methods in the\nliterature. A new logarithmic barrier penalty function dependent on both primal\nand dual variables is used to prompt the global convergence of the method,\nwhere the penalty parameter is updated adaptively. Without assuming any\nregularity condition, it is proved that our method will terminate at an\napproximate KKT point of the original problem provided the barrier parameter\ntends zero. Otherwise, either an approximate infeasible stationary point or an\napproximate singular stationary point of the original problem will be found.\nSome preliminary numerical results are reported, including the results for a\nwell-posed problem for which many line-search interior-point methods were\ndemonstrated not to be globally convergent, a feasible problem for which the\nLICQ and the MFCQ fail to hold at the solution and an infeasible problem, and\nfor some standard test problems of the CUTE collection. These results show that\nour algorithm is not only efficient for well-posed feasible problems, but also\nis applicable for some ill-posed feasible problems and some even infeasible\nproblems.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We prove that the classic logarithmic barrier problem is equivalent to a particular logarithmic barrier positive relaxation problem with barrier and scaling parameters.\n* A new logarithmic barrier penalty function dependent on both primal and dual variables is used to prompt the global convergence of the method, where the penalty parameter is updated adaptively.\n* Without assuming any regularity condition, it is proved that our method will terminate at an approximate KKT point of the original problem provided the barrier parameter tends zero."
    },
    {
        "abstract": "  The planets of our solar system formed from a gas-dust disk. However, there\nare some properties of the solar system that are peculiar in this context.\nFirst, the cumulative mass of all objects beyond Neptune (TNOs) is only a\nfraction of what one would expect. Second, unlike the planets themselves, the\nTNOs do not orbit on coplanar, circular orbits around the Sun, but move mostly\non inclined, eccentric orbits and are distributed in a complex way. This\nimplies that some process restructured the outer solar system after its\nformation. However, some of TNOs, referred to as Sednoids, move outside the\nzone of influence of the planets. Thus external forces must have played an\nimportant part in the restructuring of the outer solar system. The study\npresented here shows that a close fly-by of a neighbouring star can\nsimultaneously lead to the observed lower mass density outside 30 AU and excite\nthe TNOs onto eccentric, inclined orbits, including the family of Sednoids. In\nthe past it was estimated that such close fly-bys are rare during the relevant\ndevelopment stage. However, our numerical simulations show that such a scenario\nis much more likely than previously anticipated. A fly-by also naturally\nexplains the puzzling fact that Neptune has a higher mass than Uranus. Our\nsimulations suggest that many additional Sednoids at high inclinations still\nawait discovery, perhaps including bodies like the postulated planet X.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Our numerical simulations show that such a scenario [a close fly-by of a neighbouring star] is much more likely than previously anticipated.\""
    },
    {
        "abstract": "  Two-dimensional materials offer a versatile platform to study high-harmonic\ngeneration (HHG), encompassing as limiting cases bulk-like and atomic-like\nharmonic generation [Tancogne-Dejean and Rubio, Science Advance \\textbf{4},\neaao5207 (2018)]. Understanding the high-harmonic response of few-layer\nsemiconducting systems is important, and might open up possible technological\napplications. Using extensive first-principle calculations within a\ntime-dependent density functional theory framework, we show how the in-plane\nand out-of-plane nonlinear non-perturbative response of two-dimensional\nmaterials evolve from the monolayer to the bulk. We illustrate this phenomenon\nfor the case of multilayer hexagonal BN layered systems. Whereas the in-plane\nHHG is found not to be strongly altered by the stacking of the layers, we found\nthat the out-of-plane response is strongly affected by the number of layers\nconsidered. This is explained by the interplay between the induced electric\nfield by electron-electron interactions and the interlayer delocalization of\nthe wave-functions contributing most to the HHG signal. The gliding of a\nbilayer is also found to affect the high-harmonic emission. Our results will\nhave important ramifications for the experimental study of monolayer and\nfew-layer two-dimensional materials beyond the case of hexagonal BN studied\nhere as the result we found arew generic and applicable to all 2D\nsemiconducting multilayer systems.\n",
        "method": "Using extensive first-principle calculations within a time-dependent density functional theory framework, we show how the in-plane and out-of-plane nonlinear non-perturbative response of two-dimensional materials evolve from the monolayer to the bulk."
    },
    {
        "abstract": "  In this study, we present a novel ranking model based on learning\nneighborhood relationships embedded in the index space. Given a query point,\nconventional approximate nearest neighbor search calculates the distances to\nthe cluster centroids, before ranking the clusters from near to far based on\nthe distances. The data indexed in the top-ranked clusters are retrieved and\ntreated as the nearest neighbor candidates for the query. However, the loss of\nquantization between the data and cluster centroids will inevitably harm the\nsearch accuracy. To address this problem, the proposed model ranks clusters\nbased on their nearest neighbor probabilities rather than the query-centroid\ndistances. The nearest neighbor probabilities are estimated by employing neural\nnetworks to characterize the neighborhood relationships, i.e., the density\nfunction of nearest neighbors with respect to the query. The proposed\nprobability-based ranking can replace the conventional distance-based ranking\nfor finding candidate clusters, and the predicted probability can be used to\ndetermine the data quantity to be retrieved from the candidate cluster. Our\nexperimental results demonstrated that the proposed ranking model could boost\nthe search performance effectively in billion-scale datasets.\n",
        "method": "Here is the methodological sentence:\n\nThe nearest neighbor probabilities are estimated by employing neural networks to characterize the neighborhood relationships, i.e., the density function of nearest neighbors with respect to the query."
    },
    {
        "abstract": "  Classification and regression in which the inputs are graphs of arbitrary\nsize and shape have been paid attention in various fields such as computational\nchemistry and bioinformatics. Subgraph indicators are often used as the most\nfundamental features, but the number of possible subgraph patterns are\nintractably large due to the combinatorial explosion. We propose a novel\nefficient algorithm to jointly learn relevant subgraph patterns and nonlinear\nmodels of their indicators. Previous methods for such joint learning of\nsubgraph features and models are based on search for single best subgraph\nfeatures with specific pruning and boosting procedures of adding their\nindicators one by one, which result in linear models of subgraph indicators. In\ncontrast, the proposed approach is based on directly learning regression trees\nfor graph inputs using a newly derived bound of the total sum of squares for\ndata partitions by a given subgraph feature, and thus can learn nonlinear\nmodels through standard gradient boosting. An illustrative example we call the\nGraph-XOR problem to consider nonlinearity, numerical experiments with real\ndatasets, and scalability comparisons to naive approaches using explicit\npattern enumeration are also presented.\n",
        "method": "We propose a novel efficient algorithm to jointly learn relevant subgraph patterns and nonlinear models of their indicators."
    },
    {
        "abstract": "  During maintenance, software developers deal with numerous change requests\nmade by the users of a software system. Studies show that the developers find\nit challenging to select appropriate search terms from a change request during\nconcept location. In this paper, we propose a novel technique--QUICKAR--that\nautomatically suggests helpful reformulations for a given query by leveraging\nthe crowdsourced knowledge from Stack Overflow. It determines semantic\nsimilarity or relevance between any two terms by analyzing their adjacent word\nlists from the programming questions of Stack Overflow, and then suggests\nsemantically relevant queries for concept location. Experiments using 510\nqueries from two software systems suggest that our technique can improve or\npreserve the quality of 76% of the initial queries on average which is\npromising. Comparison with one baseline technique validates our preliminary\nfindings, and also demonstrates the potential of our technique.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Experiments using 510 queries from two software systems suggest...\""
    },
    {
        "abstract": "  Peer code review locates common coding rule violations and simple logical\nerrors in the early phases of software development, and thus reduces overall\ncost. However, in GitHub, identifying an appropriate code reviewer for a pull\nrequest is a non-trivial task given that reliable information for reviewer\nidentification is often not readily available. In this paper, we propose a code\nreviewer recommendation technique that considers not only the relevant\ncross-project work history (e.g., external library experience) but also the\nexperience of a developer in certain specialized technologies associated with a\npull request for determining her expertise as a potential code reviewer. We\nfirst motivate our technique using an exploratory study with 10 commercial\nprojects and 10 associated libraries external to those projects. Experiments\nusing 17,115 pull requests from 10 commercial projects and six open source\nprojects show that our technique provides 85%--92% recommendation accuracy,\nabout 86% precision and 79%--81% recall in code reviewer recommendation, which\nare highly promising. Comparison with the state-of-the-art technique also\nvalidates the empirical findings and the superiority of our recommendation\ntechnique.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We first motivate our technique using an exploratory study with 10 commercial projects and 10 associated libraries external to those projects.\""
    },
    {
        "abstract": "  We introduce the notions of over- and under-independence for weakly mixing\nand (free) ergodic measure preserving actions and establish new results which\ncomplement and extend the theorems obtained in [BoFW] and [A]. Here is a sample\nof results obtained in this paper:\n  $\\cdot$ (Existence of density-1 UI and OI set) Let $(X,\\mathcal{B},\\mu,T)$ be\nan invertible probability measure preserving weakly mixing system. Then for any\n$d\\in\\mathbb{N}$, any non-constant integer-valued polynomials\n$p_{1},p_{2},\\dots,p_{d}$ such that $p_{i}-p_{j}$ are also non-constant for all\n$i\\neq j$,\n  (i) there is $A\\in\\mathcal{B}$ such that the set\n  $$\\{n\\in\\mathbb{N}\\colon\\mu(A\\cap T^{p_{1}(n)}A\\cap\\dots\\cap\nT^{p_{d}(n)}A)<\\mu(A)^{d+1}\\}$$\n  is of density 1.\n  (ii) there is $A\\in\\mathcal{B}$ such that the set\n  $$\\{n\\in\\mathbb{N}\\colon\\mu(A\\cap T^{p_{1}(n)}A\\cap\\dots\\cap\nT^{p_{d}(n)}A)>\\mu(A)^{d+1}\\}$$\n  is of density 1.\n  $\\cdot$ (Existence of Ces\\`aro OI set) Let $(X,\\mathcal{B},\\mu,T)$ be a free,\ninvertible, ergodic probability measure preserving system and $M\\in\\mathbb{N}$.\n%Suppose that $X$ contains an ergodic component which is aperiodic. Then\n  there is $A\\in\\mathcal{B}$ such that\n  $$\\frac{1}{N}\\sum_{n=M}^{N+M-1}\\mu(A\\cap T^{n}A)>\\mu(A)^{2}$$\n  for all $N\\in\\mathbb{N}$.\n  $\\cdot$ (Nonexistence of Ces\\`aro UI set) Let $(X,\\mathcal{B},\\mu,T)$ be an\ninvertible probability measure preserving system. For any measurable set $A$\nsatisfying $\\mu(A) \\in (0,1)$, there exist infinitely many $N \\in \\mathbb{N}$\nsuch that $$\\frac{1}{N} \\sum_{n=0}^{N-1} \\mu ( A \\cap T^{n}A) > \\mu(A)^2.$$\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n\u2022 Let $(X,\\mathcal{B},\\mu, T)$ be an invertible probability measure preserving weakly mixing system.\n\u2022 Let $(X,\\mathcal{B},\\mu, T)$ be a free, invertible, ergodic probability measure preserving system and $M\\in\\mathbb{N}$.\n\u2022 Let $(X,\\mathcal{B},\\mu, T)$ be an invertible probability measure preserving system."
    },
    {
        "abstract": "  We study the asymptotic behaviors of the Nambu-Bethe-Salpeter (NBS) wave\nfunctions, which are important for the HAL QCD potential method to extract\nhadron interactions, in the case that a bound state exists in the system. We\nconsider the complex scalar particles, two of which lead to the formation of a\nbound state. In the case of the two-body system, we show that the NBS wave\nfunctions for the bound state as well as scattering states in the asymptotic\nregion behave like the wave functions in quantum mechanics, which carry the\ninformation of the binding energy as well as the scattering phase shift. This\nanalysis theoretically establishes under some conditions that the HAL QCD\npotential can correctly reproduce not only the scattering phase shift but also\nthe binding energy. As an extension of the analysis, we also study the\nasymptotic behaviors of all possible NBS wave functions in the case of the\nthree-body systems, two of which can form a bound states.\n",
        "method": "We consider the complex scalar particles, two of which lead to the formation of a bound state."
    },
    {
        "abstract": "  I present the first public releases (v3.4 and v3.5) of the USINE code for\ncosmic-ray propagation in the Galaxy (https://lpsc.in2p3.fr/usine). It contains\nseveral semi-analytical propagation models previously used in the literature\n(leaky-box model, 2-zone 1D and 2D diffusion models) for the calculation of\nnuclei ($Z=1-30$), anti-protons, and anti-deuterons. For minimisations, the\ngeometry, transport, and source parameters of all models can be enabled as free\nparameters, whereas nuisance parameters are enabled on solar modulation levels,\ncross sections (inelastic and production), and systematics of the CR data. With\na single ASCII initialisation file to configure runs, its many displays, and\nthe speed associated to semi-analytical approaches, USINE should be a useful\ntool for beginners, but also for experts to perform statistical analyses of\nhigh-precision cosmic-ray data.\n",
        "method": "The methodological sentence is:\n\n\"For minimisations, the geometry, transport, and source parameters of all models can be enabled as free parameters, whereas nuisance parameters are enabled on solar modulation levels, cross sections (inelastic and production), and systematics of the CR data.\""
    },
    {
        "abstract": "  We show that a complete doubling metric space $(X,d,\\mu)$ supports a weak\n$1$-Poincar\\'e inequality if and only if it admits a pencil of curves (PC)\njoining any pair of points $s,t \\in X$. This notion was introduced by S. Semmes\nin the 90's, and has been previously known to be a sufficient condition for the\nweak $1$-Poincar\\'e inequality.\n  Our argument passes through the intermediate notion of a generalised pencil\nof curves (GPC). A GPC joining $s$ and $t$ is a normal $1$-current $T$, in the\nsense of Ambrosio and Kirchheim, with boundary $\\partial T = \\delta_{t} -\n\\delta_{s}$, support contained in a ball of radius $\\sim d(s,t)$ around\n$\\{s,t\\}$, and satisfying $\\|T\\| \\ll \\mu$, with $$\\frac{d\\|T\\|}{d\\mu}(y)\n\\lesssim \\frac{d(s,y)}{\\mu(B(s,d(s,y)))} + \\frac{d(t,y)}{\\mu(B(y,d(t,y)))}.$$\nWe show that the $1$-Poincar\\'e inequality implies the existence of GPCs\njoining any pair of points in $X$. Then, we deduce the existence of PCs from a\nrecent decomposition result for normal $1$-currents due to Paolini and\nStepanov.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nA GPC joining $s$ and $t$ is a normal $1$-current $T$, in the sense of Ambrosio and Kirchheim, with boundary $\\partial T = \\delta_{t} - \\delta_{s}$, support contained in a ball of radius $\\sim d(s,t)$ around $\\{s,t\\}$, and satisfying $\\|T\\| \\ll \\mu$, with $$\\frac{d\\|T\\|}{d\\mu}(y) \\lesssim \\frac{d(s,y)}{\\mu(B(s,d(s,y)))} + \\frac{d(t,y)}{\\mu(B(y,d(t,y)))}.$$"
    },
    {
        "abstract": "  We present $J$-band near-infrared (NIR) imaging of the host galaxies of nine\nnarrow-line Seyfert 1 galaxies (NLS1). Based on high-frequency radio\nobservations at 37~GHz, seven of them could host powerful, most likely\nrelativistic jets. Host galaxy morphology studies of NLS1 galaxies are scarce,\nbut exceedingly important for understanding the seemingly heterogeneous nature\nof the NLS1 population as well as their evolution and place in the active\ngalactic nuclei (AGN) scheme. Increasing the sample size is essential for\nachieving statistically significant results. We determine the morphological\ntypes of the host galaxies by performing photometric decomposition of NIR\nimages using a 2D image decomposition algorithm GALFIT. We were able to\nsufficiently model five of the nine host galaxies. Based on the fitting\nparameters, mainly the S\\'{e}rsic index, all of them are disk-like galaxies.\nSources with clearly distinguishable bulge components all have pseudo-bulges,\nand four out of five sources show a component resembling a bar. A surprisingly\nlarge fraction, three out of five, show signs of interaction or disturbed\nmorphology. Our results suggest that spiral galaxies with pseudo-bulges are\nable to launch and maintain powerful jets. They also imply that interaction -\nmainly minor mergers - may have a role in initially triggering higher levels of\nnuclear activity in NLS1 galaxies. Furthermore, our results support the\nheterogeneous nature of the NLS1 class and indicate that this diversity is\ncaused by different evolutionary stages, possibly due to mergers.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We determine the morphological types of the host galaxies by performing photometric decomposition of NIR images using a 2D image decomposition algorithm GALFIT.\n* We were able to sufficiently model five of the nine host galaxies.\n* Our results suggest that spiral galaxies with pseudo-bulges are able to launch and maintain powerful jets."
    },
    {
        "abstract": "  Background: Test-Driven Development (TDD) is an agile software development\npractice, which is claimed to boost both external quality of software products\nand developers' productivity. Aims: We want to study (i) the TDD effects on the\nexternal quality of software products as well as the developers' productivity,\nand (ii) the retainment of TDD over a period of five months. Method: We\nconducted a (quantitative) longitudinal cohort study with 30 third year\nundergraduate students in Computer Science at the University of Bari in Italy.\nResults: The use of TDD has a statistically significant effect neither on the\nexternal quality of software products nor on the developers' productivity.\nHowever, we observed that participants using TDD produced significantly more\ntests than those applying a non-TDD development process and that the retainment\nof TDD is particularly noticeable in the amount of tests written. Conclusions:\nOur results should encourage software companies to adopt TDD because who\npractices TDD tends to write more tests---having more tests can come in handy\nwhen testing software systems or localizing faults---and it seems that novice\ndevelopers retain TDD.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We conducted a (quantitative) longitudinal cohort study with 30 third year undergraduate students in Computer Science at the University of Bari in Italy.\""
    },
    {
        "abstract": "  Let $S$ be a set of primes. We call an $m$-tuple $(a_1,\\ldots,a_m)$ of\ndistinct, positive integers $S$-Diophantine, if for all $i\\neq j$ the integers\n$s_{i,j}:=a_ia_j+1$ have only prime divisors coming from the set $S$, i.e. if\nall $s_{i,j}$ are $S$-units. In this paper, we show that no $S$-Diophantine\nquadruple (i.e.~$m=4$) exists if $S=\\{3,q\\}$. Furthermore we show that for all\npairs of primes $(p,q)$ with $p<q$ and $p\\equiv 3\\mod 4$ no\n$\\{p,q\\}$-Diophantine quadruples exist, provided that $(p,q)$ is not a\nWieferich prime pair.\n",
        "method": "Here are the methodological sentences:\n\nNo methodological sentences were found in this abstract. The text appears to be a statement of the problem and result, without describing any methods or approaches used in the research."
    },
    {
        "abstract": "  We propose a method to count the number of reachable markings of a Petri net\nwithout having to enumerate these rst. The method relies on a structural\nreduction system that reduces the number of places and transitions of the net\nin such a way that we can faithfully compute the number of reachable markings\nof the original net from the reduced net and the reduction history. The method\nhas been implemented and computing experiments show that reductions are eective\non a large benchmark of models.\n",
        "method": "Here is the methodological sentence:\n\nThe method relies on a structural reduction system that reduces the number of places and transitions of the net in such a way that we can faithfully compute the number of reachable markings of the original net from the reduced net and the reduction history."
    },
    {
        "abstract": "  Word segmentation is a low-level NLP task that is non-trivial for a\nconsiderable number of languages. In this paper, we present a sequence tagging\nframework and apply it to word segmentation for a wide range of languages with\ndifferent writing systems and typological characteristics. Additionally, we\ninvestigate the correlations between various typological factors and word\nsegmentation accuracy. The experimental results indicate that segmentation\naccuracy is positively related to word boundary markers and negatively to the\nnumber of unique non-segmental terms. Based on the analysis, we design a small\nset of language-specific settings and extensively evaluate the segmentation\nsystem on the Universal Dependencies datasets. Our model obtains\nstate-of-the-art accuracies on all the UD languages. It performs substantially\nbetter on languages that are non-trivial to segment, such as Chinese, Japanese,\nArabic and Hebrew, when compared to previous work.\n",
        "method": "Here is the methodological sentence:\n\nWe present a sequence tagging framework and apply it to word segmentation for a wide range of languages with different writing systems and typological characteristics."
    },
    {
        "abstract": "  The approaches for analyzing the polarimetric scattering matrix of\npolarimetric synthetic aperture radar (PolSAR) data have always been the focus\nof PolSAR image classification. Generally, the polarization coherent matrix and\nthe covariance matrix obtained by the polarimetric scattering matrix only show\na limited number of polarimetric information. In order to solve this problem,\nwe propose a sparse scattering coding way to deal with polarimetric scattering\nmatrix and obtain a close complete feature. This encoding mode can also\nmaintain polarimetric information of scattering matrix completely. At the same\ntime, in view of this encoding way, we design a corresponding classification\nalgorithm based on convolution network to combine this feature. Based on sparse\nscattering coding and convolution neural network, the polarimetric\nconvolutional network is proposed to classify PolSAR images by making full use\nof polarimetric information. We perform the experiments on the PolSAR images\nacquired by AIRSAR and RADARSAT-2 to verify the proposed method. The\nexperimental results demonstrate that the proposed method get better results\nand has huge potential for PolSAR data classification. Source code for sparse\nscattering coding is available at\nhttps://github.com/liuxuvip/Polarimetric-Scattering-Coding.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In order to solve this problem, we propose a sparse scattering coding way to deal with polarimetric scattering matrix and obtain a close complete feature.\n* At the same time, in view of this encoding way, we design a corresponding classification algorithm based on convolution network to combine this feature.\n* Based on sparse scattering coding and convolution neural network, the polarimetric convolutional network is proposed to classify PolSAR images by making full use of polarimetric information."
    },
    {
        "abstract": "  A Finsler space $(M,F)$ is called a geodesic orbit space if any geodesic of\nconstant speed is the orbit of a one-parameter subgroup of isometries of $(M,\nF)$. In this paper, we study Finsler metrics on Euclidean spaces which are\ngeodesic orbit metrics. We will show that, in this case $(M, F)$ is a fiber\nbundle over a symmetric Finsler space $M_1$ of non-compact type such that each\nfiber $M_2$ is a totally geodesic nilmanifold with a step-size at most 2, and\nthe projection $\\pi:M\\rightarrow M_1$ is a Finslerian submersion. Furthermore,\nwhen $M_1$ has no Hermitian symmetric factors, the fiber bundle description for\n$M$ can be strengthened to $M=M_1\\times M_2$ as coset spaces, such that each\nproduct factor is totally geodesic in $(M,F)$ and is a geodesic orbit Finsler\nspace itself. Finally, we use the techniques in this paper to discuss the\ninteraction between the geodesic orbit spaces and the negative (non-positive)\ncurved conditions, and provide new proofs for some of our previous results.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We will show that...\n* Furthermore, when... such that each product factor is totally geodesic in $(M,F)$ and is a geodesic orbit Finsler space itself.\n\nNote: These are not specific technical methods or approaches used in the research, but rather statements of what the authors intend to prove or describe."
    },
    {
        "abstract": "  In the paper is considered the use of a , $^{65}$Zn source in the BEST-2\ngallium experiment to constrain the regions of the allowed oscillation\nparameters. The required activity of the $^{65}$Zn source for the BEST-2\nexperiment, its size, effect on the results of oscillatory measurements, as\nwell as the possibility of production such a source are calculated. Schemes of\nmeasurements execution are considered.\n",
        "method": "Here is the methodological sentence:\n\nThe required activity of the $^{65}$Zn source for the BEST-2 experiment, its size, effect on the results of oscillatory measurements, as well as the possibility of production such a source are calculated."
    },
    {
        "abstract": "  We experimentally investigate second harmonic generation from strongly\ncoupled localized and propagative phonon polariton modes in arrays of silicon\ncarbide nanopillars. Our results clearly demonstrate the hybrid nature of the\nsystem's eigenmodes and distinct manifestation of strong coupling in the linear\nand nonlinear response. While in linear reflectivity the intensity of the two\nstrongly-coupled branches is essentially symmetric and well explained by their\nrespective localized or propagative components, the second harmonic signal\npresents a strong asymmetry. Analyzing it in detail, we reveal the importance\nof interference effects between the nonlinear polarization terms originating in\nthe bulk and in the phonon polariton modes, respectively.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe experimentally investigate second harmonic generation..."
    },
    {
        "abstract": "  A large Time Projection Chamber (TPC) is the main device for tracking and\ncharged-particle identification in the ALICE experiment at the CERN LHC. After\nthe second long shutdown in 2019-2020, the LHC will deliver Pb beams colliding\nat an interaction rate of up to 50 kHz, which is about a factor of 50 above the\npresent readout rate of the TPC. To fully exploit the LHC potential, the TPC\nreadout chambers will be upgraded with Gas Electron Multiplier (GEM)\ntechnology.\n  To assure stable behaviour of the upgraded chambers in the harsh LHC\nenvironment, a dedicated R&D programme was launched in order to optimize GEM\nstack geometry and its high voltage configuration with respect to electric\ndischarges. We present a summary of discharge probability measurements\nperformed with 3- and 4-GEM prototypes irradiated with highly ionising alpha\nparticles.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* To fully exploit the LHC potential, the TPC readout chambers will be upgraded with Gas Electron Multiplier (GEM) technology.\n* We present a summary of discharge probability measurements performed with 3- and 4-GEM prototypes irradiated with highly ionising alpha particles."
    },
    {
        "abstract": "  This work introduces two new notions of dimension, namely the unimodular\nMinkowski and Hausdorff dimensions, which are inspired from the classical\nanalogous notions. These dimensions are defined for unimodular discrete spaces,\nintroduced in this work, which provide a common generalization to stationary\npoint processes under their Palm version and unimodular random rooted graphs.\nThe use of unimodularity in the definitions of dimension is novel. Also, a\ntoolbox of results is presented for the analysis of these dimensions. In\nparticular, analogues of Billingsley's lemma and Frostman's lemma are\npresented. These last lemmas are instrumental in deriving upper bounds on\ndimensions, whereas lower bounds are obtained from specific coverings. The\nnotions of unimodular Hausdorff size, which is a discrete analogue of the\nHausdorff measure, and unimodular dimension function are also introduced. This\ntoolbox allows one to connect the unimodular dimensions to other notions such\nas volume growth rate, discrete dimension and scaling limits. It is also used\nto analyze the dimensions of a set of examples pertaining to point processes,\nbranching processes, random graphs, random walks, and self-similar discrete\nrandom spaces. Further results of independent interest are also presented, like\na version of the max-flow min-cut theorem for unimodular one-ended trees and a\nweak form of pointwise ergodic theorems for all unimodular discrete spaces.\n",
        "method": "These dimensions are defined for unimodular discrete spaces, introduced in this work, which provide a common generalization to stationary point processes under their Palm version and unimodular random rooted graphs."
    },
    {
        "abstract": "  Let k be a field and denote by SH(k) the motivic stable homotopy category.\nRecall its full subcategory HI_0(k) of effective homotopy modules. Write\nNAlg(HI_0(k)) for the category of normed motivic spectra with underlying\nspectrum an effective homotopy module. In this article we provide an explicit\ndescription of NAlg(HI_0(k)) as the category of sheaves with generalized\ntransfers and \\'etale norms, and explain how this is closely related to the\nclassical notion of Tambara functors.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe provide an explicit description..."
    },
    {
        "abstract": "  We consider time correlation for KPZ growth in 1+1 dimensions in a\nneighborhood of a characteristics. We prove convergence of the covariance with\ndroplet, flat and stationary initial profile. In particular, this provides a\nrigorous proof of the exact formula of the covariance for the stationary case\nobtained in [SIGMA 12 (2016), 074]. Furthermore, we prove the universality of\nthe first order correction when the two observation times are close and provide\na rigorous bound of the error term. This result holds also for random initial\nprofiles which are not necessarily stationary.\n",
        "method": "We consider time correlation for KPZ growth in 1+1 dimensions in a neighborhood of a characteristic scale, proving convergence of the covariance with droplet, flat, and stationary initial profiles."
    },
    {
        "abstract": "  We present an exact method for counting semi-magic squares of order 6. Some\ntheoretical investigations about the number of them and a probabilistic method\nare presented. Our calculations show that there are exactly\n$94\\,590\\,660\\,245\\,399\\,996\\,601\\,600$ such squares up to reflections and\nrotations.\n",
        "method": "Here is the methodological sentence:\n\n\"Some theoretical investigations about the number of them and a probabilistic method are presented.\""
    },
    {
        "abstract": "  The introduction of spin-orbit interactions (SOIs) and the subsequent\nappearance of a two-dimensional (2D) topological phase are crucial for\nvoltage-controlled and zero-emission energy spintronic devices. In contrast,\ngraphene basically lacks SOIs due to the small mass of the carbon atom, and\nappropriate experimental reports for SOIs are rare. Here, we control\nsmall-amount (cover ratios < 8%) random decoration of heavy nanoparticles\n[platinum (Pt) or bismuth telluride (Bi2Te3)] onto mono-layer graphene by\ndeveloping an original nanoneedle method. X-ray photoelectron spectra support\nlow-damage and low-contamination decoration of the nanoparticles, suggesting\nthe presence of Bi-C and Te-C coupling orbitals. In the samples, we find\nparticle-density-dependent non-local resistance (RNL) peaks, which are\nattributed to the (inverse) spin Hall effect (SHE) arising from SOI with\nenergies as large as about 30 meV. This is a larger value than in previous\nreports and supported by scanning tunneling spectroscopy. The present\nobservation should lead to topological phases of graphene, which can be\nintroduced by random decoration with controlled small amounts of heavy\nnanoparticles, and their applications.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Here, we control small-amount (cover ratios < 8%) random decoration of heavy nanoparticles [platinum (Pt) or bismuth telluride (Bi2Te3)] onto mono-layer graphene by developing an original nanoneedle method.\n* X-ray photoelectron spectra support low-damage and low-contamination decoration of the nanoparticles, suggesting the presence of Bi-C and Te-C coupling orbitals."
    },
    {
        "abstract": "  Microbial colonies cultured on agar Petri dishes have become a model system\nto study biological evolution in populations expanding in space. Processes such\nas clonal segregation and gene surfing have been shown to be affected by\ninteractions between microbial cells and their environment. In this work we\ninvestigate the role of mechanical interactions such as cell-surface adhesion.\nWe compare two strains of the bacterium E. coli: a wild-type strain and a\n\"shaved\" strain that adheres less to agar. We show that the shaved strain has a\nselective advantage over the wild type: although both strains grow with the\nsame rate in liquid media, the shaved strain produces colonies that expand\nfaster on agar. This allows the shaved strain outgrow the wild type when both\nstrains compete for space. We hypothesise that, in contrast to a more common\nscenario in which selective advantage results from increased growth rate, the\nhigher fitness of the shaved strain is caused by reduced adhesion and friction\nwith the agar surface.\n",
        "method": "We investigate the role of mechanical interactions such as cell-surface adhesion."
    },
    {
        "abstract": "  Lenticular galaxies are generally thought to have descended from spirals via\nmorphological transformation, although recent numerical simulations have shown\nthat minor or even major merger can also lead to an S0-like remnant. These\nmechanisms, however, are active in a dense environment such as a group or a\ncluster of galaxies - making it harder to explain the remarkable fraction of\nS0s found in the field. Here, we propose a new mechanism to form such\nlenticular galaxies. We show that an isolated cold disk settled into rotational\nequilibrium becomes violently unstable - leading to fragmentation and formation\nof stellar clumps that, in turn, not only grow the bulge, but also increase the\nstellar disk velocity dispersion optimally in less than a billion year.\nSubsequently, the galaxy evolves passively without any conspicuous spiral\nstructure. The final galaxy models resemble remarkably well the morphology and\nstellar kinematics of the present-day S0s observed by the Planetary Nebulae\nspectrograph. Our findings suggest a natural link between the high-redshift\nclumpy progenitors to the present-day S0 galaxies.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe show that an isolated cold disk settled into rotational equilibrium becomes violently unstable - leading to fragmentation and formation of stellar clumps..."
    },
    {
        "abstract": "  Faster and more cost-efficient, crowdsourced delivery is needed to meet the\ngrowing customer demands of many industries, including online shopping,\non-demand local delivery, and on-demand transportation. The power of\ncrowdsourced delivery stems from the large number of workers potentially\navailable to provide services and reduce costs. It has been shown in social\npsychology literature that fairness is key to ensuring high worker\nparticipation. However, existing assignment solutions fall short on modeling\nthe dynamic fairness metric. In this work, we introduce a new assignment\nstrategy for crowdsourced delivery tasks. This strategy takes fairness towards\nworkers into consideration, while maximizing the task allocation ratio. Since\nredundant assignments are not possible in delivery tasks, we first introduce a\n2-phase allocation model that increases the reliability of a worker to complete\na given task. To realize the effectiveness of our model in practice, we present\nboth offline and online versions of our proposed algorithm called F-Aware.\nGiven a task-to-worker bipartite graph, F-Aware assigns each task to a worker\nthat minimizes unfairness, while allocating tasks to use worker capacities as\nmuch as possible. We present an evaluation of our algorithms with respect to\nrunning time, task allocation ratio (TAR), as well as unfairness and assignment\nratio. Experiments show that F-Aware runs around 10^7 x faster than the\nTAR-optimal solution and allocates 96.9% of the tasks that can be allocated by\nit. Moreover, it is shown that, F-Aware is able to provide a much fair\ndistribution of tasks to workers than the best competitor algorithm.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We introduce a new assignment strategy for crowdsourced delivery tasks.\n* This strategy takes fairness towards workers into consideration, while maximizing the task allocation ratio.\n* We first introduce a 2-phase allocation model that increases the reliability of a worker to complete a given task.\n* Given a task-to-worker bipartite graph, F-Aware assigns each task to a worker that minimizes unfairness, while allocating tasks to use worker capacities as much as possible."
    },
    {
        "abstract": "  Fragmentation of spiral arms can drive the formation of giant clumps and\ninduce intense star formation in disc galaxies. Based on the spiral-arm\ninstability analysis of our Paper I, we present linear perturbation theory of\ndynamical instability of self-gravitating spiral arms of magnetised gas,\nfocusing on the effect of toroidal magnetic fields. Spiral arms can be\ndestabilised by the toroidal fields which cancel Coriolis force, i.e.\nmagneto-Jeans instability. Our analysis can be applied to multi-component\nsystems that consist of gas and stars. To test our analysis, we perform ideal\nmagneto-hydrodynamics simulations of isolated disc galaxies and examine the\nsimulation results. We find that our analysis can characterise dynamical\ninstability leading arms to fragment and form clumps if magnetic fields are\nnearly toroidal. We propose that dimensionless growth rate of the most unstable\nperturbation, which is computed from our analysis, can be used to predict\nfragmentation of spiral arms within an orbital time-scale. Our analysis is\napplicable as long as magnetic fields are nearly toroidal. Using our analytic\nmodel, we estimate a typical mass of clumps forming from spiral-arm\nfragmentation to be consistent with observed giant clumps $\\sim10^{7-8}~{\\rm\nM_\\odot}$. Furthermore, we find that, although the magnetic destabilisation can\ncause low-density spiral arms to fragment, the estimated mass of resultant\nclumps is almost independent from strength of magnetic fields since marginal\ninstability occurs at long wavelengths which compensate the low densities of\nmagnetically destabilised arms.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We present linear perturbation theory of dynamical instability of self-gravitating spiral arms of magnetised gas, focusing on the effect of toroidal magnetic fields.\n* To test our analysis, we perform ideal magneto-hydrodynamics simulations of isolated disc galaxies and examine the simulation results."
    },
    {
        "abstract": "  In the last decades, the notion that cities are in a state of equilibrium\nwith a centralised organisation has given place to the viewpoint of cities in\ndisequilibrium and organised from bottom to up. In this perspective, cities are\nevolving systems that exhibit emergent phenomena built from local decisions.\nWhile urban evolution promotes the emergence of positive social phenomena such\nas the formation of innovation hubs and the increase in cultural diversity, it\nalso yields negative phenomena such as increases in criminal activity. Yet, we\nare still far from understanding the driving mechanisms of these phenomena. In\nparticular, approaches to analyse urban phenomena are limited in scope by\nneglecting both temporal non-stationarity and spatial heterogeneity. In the\ncase of criminal activity, we know for more than one century that crime peaks\nduring specific times of the year, but the literature still fails to\ncharacterise the mobility of crime. Here we develop an approach to describe the\nspatial, temporal, and periodic variations in urban quantities. With crime data\nfrom 12 cities, we characterise how the periodicity of crime varies spatially\nacross the city over time. We confirm one-year criminal cycles and show that\nthis periodicity occurs unevenly across the city. These `waves of crime' keep\ntravelling across the city: while cities have a stable number of regions with a\ncircannual period, the regions exhibit non-stationary series. Our findings\nsupport the concept of cities in a constant change, influencing urban\nphenomena---in agreement with the notion of cities not in equilibrium.\n",
        "method": "Here is the methodological sentence:\n\nWith crime data from 12 cities, we characterise how the periodicity of crime varies spatially across the city over time."
    },
    {
        "abstract": "  Type Ia supernovae originate from the explosion of carbon-oxygen white dwarfs\nin binary systems, but the exact nature of their progenitors remains elusive.\nThe bulk properties of Type Ia supernova remnants, such as the radius and the\ncentroid energy of the Fe K$\\alpha$ blend in the X-ray spectrum, are determined\nby the properties of the supernova ejecta and the ambient medium. We model the\ninteraction between Chandrasekhar and sub-Chandrasekhar models for Type Ia\nsupernova ejecta and a range of uniform ambient medium densities in one\ndimension up to an age of 5000 years. We generate synthetic X-ray spectra from\nthese supernova remnant models and compare their bulk properties at different\nexpansion ages with X-ray observations from \\textit{Chandra} and\n\\textit{Suzaku}. We find that our models can successfully reproduce the bulk\nproperties of most observed remnants, suggesting that Type Ia SN progenitors do\nnot modify their surroundings significantly on scales of a few pc. Ambient\nmedium density and expansion age are the main contributors to the diversity of\nthe bulk properties in our models. Chandrasekhar and sub-Chandrasekhar\nprogenitors make similar predictions for the bulk remnant properties, but\ndetailed fits to X-ray spectra have the power to discriminate explosion\nenergetics and progenitor scenarios.\n",
        "method": "We model the interaction between Chandrasekhar and sub-Chandrasekhar models for Type Ia supernova ejecta and a range of uniform ambient medium densities in one dimension up to an age of 5000 years."
    },
    {
        "abstract": "  Bistability and multistationarity are properties of reaction networks linked\nto switch-like responses and connected to cell memory and cell decision making.\nDetermining whether and when a network exhibits bistability is a hard and open\nmathematical problem. One successful strategy consists of analyzing small\nnetworks and deducing that some of the properties are preserved upon passage to\nthe full network. Motivated by this we study chemical reaction networks with\nfew chemical complexes. Under mass-action kinetics the steady states of these\nnetworks are described by fewnomial systems, that is polynomial systems having\nfew distinct monomials. Such systems of polynomials are often studied in real\nalgebraic geometry by the use of Gale dual systems. Using this Gale duality we\ngive precise conditions in terms of the reaction rate constants for the number\nand stability of the steady states of families of reaction networks with one\nnon-flow reaction.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We study chemical reaction networks with few chemical complexes.\n* Under mass-action kinetics the steady states of these networks are described by polynomial systems, that is polynomial systems having few distinct monomials.\n* Using Gale duality we give precise conditions in terms of the reaction rate constants..."
    },
    {
        "abstract": "  We present high resolution H{\\sc i} 21cm Giant Meterwave Radio Telescope\n(GMRT) observations of the superthin galaxy FGC1540 with a spatial resolution\nof 10$''$ $\\times$ 8$''$ and a spectral resolution of 1.73 kms$^{-1}$ and an\nrms noise of 0.9 mJy per beam. We obtain its rotation curve as well as\ndeprojected radial H{\\sc i} surface density profile by fitting a 3-dimensional\ntilted ring model directly to the H{\\sc i} data cubes by using the\npublicly-available software, Fully Automated Tirrific (FAT). We also present\nthe rotation curve of FGC1540 derived from its optical spectroscopy study using\nthe 6-m BTA telescope of the Special Astrophysical Observatory of the Russian\nAcademy of Sciences. We use the rotation curve, the H{\\sc i} surface density\nprofile together with Spitzer 3.6 $\\mu$m and the SDSS $i$--band data to\nconstruct the mass models for FGC1540. We find that both the Pseudo-isothermal\n(PIS), as well as Navarro-Frenk-White (NFW) dark matter (DM) halos, fit the\nobserved rotation curve equally well. The PIS model indicates a compact dark\nmatter halo ($R_{\\rm C}/R_{\\rm D}$ < 2), with the best-fitting core radius\n($R_{\\rm C}$) approximately half the exponential stellar disc scale length\n($R_{\\rm D}$), which is in agreement with the mass models of superthin galaxies\nstudied earlier in the literature. Since the vertical thickness of the galactic\nstellar disc is determined by a balance between the net gravitational field and\nthe velocity dispersion in the vertical direction, the compact dark matter halo\nmay be primarily responsible in regulating the superthin vertical structure of\nthe stellar disc in FGC1540 as was found in case of the superthin galaxy\nUGC7321.\n",
        "method": "We obtain its rotation curve as well as deprojected radial H{\\sc i} surface density profile by fitting a 3-dimensional tilted ring model directly to the H{\\sc i} data cubes by using the publicly-available software, Fully Automated Tirrific (FAT)."
    },
    {
        "abstract": "  In this paper we construct a family of steady symmetric vortex patches for\nthe incompressible Euler equations in an open disk. The result is obtained by\nstudying a variational problem in which the kinetic energy of the fluid is\nmaximized subject to some appropriate constraints for the vorticity. Moreover,\nwe show that these vortex patches shrink to a given minimum point of the\ncorresponding Kirchhoff-Routh function as the vorticity strength parameter goes\nto infinity.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The result is obtained by studying a variational problem in which the kinetic energy of the fluid is maximized subject to some appropriate constraints for the vorticity.\n* We show that these vortex patches shrink to a given minimum point of the corresponding Kirchhoff-Routh function as the vorticity strength parameter goes to infinity."
    },
    {
        "abstract": "  In this paper, we consider the finite-state approximation of a discrete-time\nconstrained Markov decision process (MDP) under the discounted and average cost\ncriteria. Using the linear programming formulation of the constrained\ndiscounted cost problem, we prove the asymptotic convergence of the optimal\nvalue of the finite-state model to the optimal value of the original model.\nWith further continuity condition on the transition probability, we also\nestablish a method to compute approximately optimal policies. For the average\ncost, instead of using the finite-state linear programming approximation\nmethod, we use the original problem definition to establish the finite-state\nasymptotic approximation of the constrained problem and compute approximately\noptimal policies. Under Lipschitz type regularity conditions on the components\nof the MDP, we also obtain explicit rate of convergence bounds quantifying how\nthe approximation improves as the size of the approximating finite state space\nincreases.\n",
        "method": "Using the linear programming formulation of the constrained discounted cost problem, we prove the asymptotic convergence of the optimal value of the finite-state model to the optimal value of the original model."
    },
    {
        "abstract": "  The decline in student interest in science and technology is a major concern\nin the western world. One approach to reversing this decline is to introduce\nmodern physics concepts much earlier in the school curriculum. We have used the\ncontext of the recent discoveries of gravitational waves to test benefits of\none-day interventions, in which students are introduced to the ongoing nature\nof scientific discovery, as well as the fundamental concepts of quantum physics\nand gravitation, which underpin these discoveries. Our innovative approach\ncombines role-playing, model demonstrations, single photon interference and\ngravitational wave detection, plus simple experiments designed to emphasize the\nquantum interpretation of interference. We compare understanding and attitudes\nthrough pre and post testing on four age groups (school years 7, 8, 9 and 10),\nand compare results with those of longer interventions with Year 9. Results\nindicate that neither prior knowledge nor age are significant factors in\nstudent understanding of the core concepts of Einsteinian physics. However we\nfind that the short interventions are insufficient to enable students to\ncomprehend more derived concepts.\n",
        "method": "We have used the context of the recent discoveries of gravitational waves to test benefits of one-day interventions, in which students are introduced to the ongoing nature of scientific discovery, as well as the fundamental concepts of quantum physics and gravitation, which underpin these discoveries."
    },
    {
        "abstract": "  Visual localization and mapping is a crucial capability to address many\nchallenges in mobile robotics. It constitutes a robust, accurate and\ncost-effective approach for local and global pose estimation within prior maps.\nYet, in highly dynamic environments, like crowded city streets, problems arise\nas major parts of the image can be covered by dynamic objects. Consequently,\nvisual odometry pipelines often diverge and the localization systems\nmalfunction as detected features are not consistent with the precomputed 3D\nmodel. In this work, we present an approach to automatically detect dynamic\nobject instances to improve the robustness of vision-based localization and\nmapping in crowded environments. By training a convolutional neural network\nmodel with a combination of synthetic and real-world data, dynamic object\ninstance masks are learned in a semi-supervised way. The real-world data can be\ncollected with a standard camera and requires minimal further post-processing.\nOur experiments show that a wide range of dynamic objects can be reliably\ndetected using the presented method. Promising performance is demonstrated on\nour own and also publicly available datasets, which also shows the\ngeneralization capabilities of this approach.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* By training a convolutional neural network model with a combination of synthetic and real-world data, dynamic object instance masks are learned in a semi-supervised way.\n* The real-world data can be collected with a standard camera and requires minimal further post-processing."
    },
    {
        "abstract": "  The unexpected appearance of a fractional quantum Hall effect (FQHE) plateau\nat $\\nu=2+6/13$~ [Kumar \\emph{et al.}, Phys. Rev. Lett. {\\bf 105}, 246808\n(2010)] offers a clue into the physical mechanism of the FQHE in the second\nLandau level (SLL). Here we propose a \"$\\bar{3}\\bar{2}111$\" parton wave\nfunction, which is topologically distinct from the 6/13 state in the lowest\nLandau level. We demonstrate the $\\bar{3}\\bar{2}111$ state to be a good\ncandidate for the $\\nu=2+6/13$ FQHE, and make predictions for experimentally\nmeasurable properties that can reveal the nature of this state. Furthermore, we\npropose that the \"$\\bar{n}\\bar{2}111$\" family of parton states naturally\ndescribes many observed SLL FQHE plateaus.\n",
        "method": "Here is the methodological sentence:\n\nWe propose a \"$\\bar{3}\\bar{2}111$\" parton wave function, which is topologically distinct from the 6/13 state in the lowest Landau level."
    },
    {
        "abstract": "  Many observables which are used as a signature of the collective effects in\nheavy-ion collisions when measured in high multiplicity pp and pA interactions\nreveal a very similar behaviour. We will present first measurements of\ndifferent order flow coefficients and their magnitude correlations for data\ncollected by ALICE during the LHC Run 2 operation, which includes pp collisions\nat $\\sqrt{s} = 13$ TeV, p-Pb at $\\sqrt{s_{\\rm{NN}}} = 5.02$ TeV, Xe-Xe at\n$\\sqrt{s_{\\rm{NN}}} = 5.44$ TeV and Pb-Pb collisions at $\\sqrt{s_{\\rm{NN}}} =\n5.02$ TeV. Such a broad spectrum of colliding systems with different energies\nand wide range of multiplicity allow for detailed investigation of their\ncollision dynamics. The measurements are based on a newly developed subevent\ntechnique, which was proven to be particularly important for studies in small\nsystems. The results provide an important insight into the nature of collective\nphenomena in different collision systems.\n",
        "method": "We will present first measurements of different order flow coefficients and their magnitude correlations for data collected by ALICE during the LHC Run 2 operation..."
    },
    {
        "abstract": "  We propose a method to decrease the number of hidden units of the restricted\nBoltzmann machine while avoiding decrease of the performance measured by the\nKullback-Leibler divergence. Then, we demonstrate our algorithm by using\nnumerical simulations.\n",
        "method": "Here is the extracted methodological sentence:\n\nWe propose a method to decrease the number of hidden units of the restricted Boltzmann machine..."
    },
    {
        "abstract": "  We work out the most general theory for the interaction of spacetime geometry\nand matter fields -- commonly referred to as geometrodynamics -- for spin-$0$\nand spin-$1$ particles. The minimum set of postulates to be introduced is that\n(i) the action principle should apply and that(ii) the total action should by\nform-invariant under the (local) diffeomorphism group. The second postulate\nthus implements the Principle of General Relativity. According to Noether's\ntheorem, this physical symmetry gives rise to a conserved Noether current, from\nwhich the complete set of theories compatible with both postulates can be\ndeduced. This finally results in a new generic Einstein-type equation, which\ncan be interpreted as an energy-momentum balance equation emerging from the\nLagrangian $L_{R}$ for the source-free dynamics of gravitation and the\nenergy-momentum tensor of the source system $L_{0}$. Provided that the system\nhas no other symmetries -- such as SU$(N)$ -- the canonical energy-momentum\ntensor turns out to be the correct source term of gravitation. For the case of\nmassive spin particles, this entails an increased weighting of the kinetic\nenergy over the mass in their roles as the source of gravity as compared to the\nmetric energy momentum tensor, which constitutes the source of gravity in\nEinstein's General Relativity. We furthermore confirm that a massive vector\nfield necessarily acts as a source for torsion of spacetime. Thus, from the\nviewpoint of our generic Einstein-type equation, Einstein's General Relativity\nconstitutes the particular case for spin-$0$ and massless spin particle fields,\nand the Hilbert Lagrangian $L_{R,H}$ as the model for the source-free dynamics\nof gravitation.\n",
        "method": "Here is the methodological sentence:\n\nThe minimum set of postulates to be introduced is that (i) the action principle should apply and that (ii) the total action should be form-invariant under the (local) diffeomorphism group."
    },
    {
        "abstract": "  Networks are abundant in biological systems. Small sized over-represented\nnetwork motifs have been discovered, and it has been suggested that these\nconstitute functional building blocks. We ask whether larger dynamical network\nmotifs exist in biological networks, thus contributing to the higher-order\norganization of a network. To end this, we introduce a gradient descent machine\nlearning (ML) approach and genetic algorithms to learn larger functional motifs\nin contrast to an (unfeasible) exhaustive search. We use the French Flag (FF)\nand Switch functional motif as case studies motivated from biology. While our\nalgorithm successfully learns large functional motifs, we identify a threshold\nsize of approximately 20 nodes beyond which learning breaks down. Therefore we\ninvestigate the stability of the motifs. We find that the size of the real\nnegative eigenvalues of the Jacobian decreases with increasing system size,\nthus conferring instability. Finally, without imposing learning an input-output\nfor all the components of the network, we observe that unconstrained middle\ncomponents of the network still learn the desired function, a form of\nhomogeneous team learning. We conclude that the size limitation of\nlearnability, most likely due to stability constraints, impose a definite\nrequirement for modularity in networked systems while enabling team learning\nwithin unconstrained parts of the module. Thus, the observation that community\nstructures and modularity are abundant in biological networks could be\naccounted for by a computational compositional network structure.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We introduce a gradient descent machine learning (ML) approach and genetic algorithms to learn larger functional motifs in contrast to an (unfeasible) exhaustive search.\""
    },
    {
        "abstract": "  We present a process algebra aimed at describing interactions that are\nmultiparty, i.e. that may involve more than two processes and that are open,\ni.e. the number of the processes they involve is not fixed or known a priori.\nHere we focus on the theory of a core version of a process calculus, without\nmessage passing, called Core Network Algebra (CNA). In CNA communication\nactions are given not in terms of channels but in terms of chains of links that\nrecord the source and the target ends of each hop of interactions. The\noperational semantics of our calculus mildly extends the one of CCS. The\nabstract semantics is given in the style of bisimulation but requires some\ningenuity. Remarkably, the abstract semantics is a congruence for all operators\nof CNA and also with respect to substitutions, which is not the case for strong\nbisimilarity in CCS. As a motivating and running example, we illustrate the\nmodel of a simple software defined network infrastructure.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In this process algebra, communication actions are given not in terms of channels but in terms of chains of links that record the source and the target ends of each hop of interactions.\n* The operational semantics of our calculus mildly extends the one of CCS.\n* The abstract semantics is given in the style of bisimulation but requires some ingenuity."
    },
    {
        "abstract": "  Narrow-line Seyfert 1s (NLSy1s) are an ill-defined class. Work done over the\npast 20 years as well as recent analyses show a continuity in properties (e.g.,\nBalmer line profiles, blueshifts of high-ionization lines) between sources with\nFWHM above and below 2000 km/s, the defining boundary of NLSy1s. This finding\nalone suggests that comparisons between samples of NLSy1s and rest of\nbroad-line AGNs are most likely biased. NLSy1s can be properly contextualized\nby their location on the quasar main sequence originally defined by Sulentic et\nal 2000. At one end, NLSy1s encompass sources with strong FeII emission and\nassociated with high Eddington ratio that hold the promise of becoming useful\ndistance indicators; at the other end, at least some of them are sources with\nbroad profiles seen face-on. Any rigid FWHM limit gives rise to some physical\nambiguity, as the FWHM of low-ionization lines depends in a complex way on\nmass, Eddington ratio, orientation, and luminosity. In addition, if the scaling\nderived from luminosity and virial dynamics applies to the broad line regions,\nNLSy1s at luminosity higher than 1E47 erg/s become physically impossible.\nTherefore, in a broader context, a proper subdivision of two distinct classes\nof AGNs and quasars may be achieved by the distinction between Pop. A and B\nwith boundary at = 4000 km/s in samples at z < 1, or on the basis of\nspectrophotometric properties which may ultimately be related to differences in\naccretion modes if high-luminosity quasars are considered.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nAny rigid FWHM limit gives rise to some physical ambiguity, as the FWHM of low-ionization lines depends in a complex way on mass, Eddington ratio, orientation, and luminosity."
    },
    {
        "abstract": "  This paper illustrates the interface of the tool we developed for crowd\nsourcing and we explain the annotation procedure in detail. Our tool is named\nas 'Parupalli Padajaalam' which means web of words by Parupalli. The aim of\nthis tool is to populate the OntoSenseNet, sentiment polarity annotated Telugu\nresource. Recent works have shown the importance of word-level annotations on\nsentiment analysis. With this as basis, we aim to analyze the importance of\nsense-annotations obtained from OntoSenseNet in performing the task of\nsentiment analysis. We explain the fea- tures extracted from OntoSenseNet\n(Telugu). Furthermore we compute and explain the adverbial class distribution\nof verbs in OntoSenseNet. This task is known to aid in disambiguating\nword-senses which helps in enhancing the performance of word-sense\ndisambiguation (WSD) task(s).\n",
        "method": "Our tool is named as 'Parupalli Padajaalam' which means web of words by Parupalli. We explain the annotation procedure in detail."
    },
    {
        "abstract": "  We show that families of action graphs, with initial graphs which are linear\nof varying length, give rise to self-convolutions of the Catalan sequence. We\nprove this result via a comparison with planar rooted forests with a fixed\nnumber of trees.\n",
        "method": "Families of action graphs, with initial graphs which are linear of varying length, give rise to self-convolutions of the Catalan sequence."
    },
    {
        "abstract": "  We explore a novel approach for Semantic Role Labeling (SRL) by casting it as\na sequence-to-sequence process. We employ an attention-based model enriched\nwith a copying mechanism to ensure faithful regeneration of the input sequence,\nwhile enabling interleaved generation of argument role labels. Here, we apply\nthis model in a monolingual setting, performing PropBank SRL on English\nlanguage data. The constrained sequence generation set-up enforced with the\ncopying mechanism allows us to analyze the performance and special properties\nof the model on manually labeled data and benchmarking against state-of-the-art\nsequence labeling models. We show that our model is able to solve the SRL\nargument labeling task on English data, yet further structural decoding\nconstraints will need to be added to make the model truly competitive. Our work\nrepresents a first step towards more advanced, generative SRL labeling setups.\n",
        "method": "We employ an attention-based model enriched with a copying mechanism to ensure faithful regeneration of the input sequence, while enabling interleaved generation of argument role labels."
    },
    {
        "abstract": "  We report on the performance of silicon photomultiplier (SiPM) light sensors\noperating in electric field strength up to 30 kV/cm and at a temperature of\n149K, relative to their performance in the absence of an external electric\nfield. The SiPM devices used in this study show stable gain, photon detection\nefficiency, and rates of correlated pulses, when exposed to external fields,\nwithin the estimated uncertainties. No observable physical damage to the bulk\nor surface of the devices was caused by the exposure.\n",
        "method": "The methodological sentence is:\n\n\"The SiPM devices used in this study show stable gain, photon detection efficiency, and rates of correlated pulses, when exposed to external fields, within the estimated uncertainties.\""
    },
    {
        "abstract": "  CeCo(In$_{0.990}$Hg$_{0.010}$)$_{5}$ is a charge doped variant of the\n$d$-wave CoCoIn$_{5}$ superconductor with coexistent antiferromagnetic and\nsuperconducting transitions occurring at T$_{N}$= 3.4 K and T$_{c}$=1.4 K,\nrespectively. We use neutron diffraction and spectroscopy to show that the\nmagnetic resonant fluctuations present in the parent superconducting phase are\nreplaced by collinear $c$-axis magnetic order with three-dimensional Ising\ncritical fluctuations. No low energy transverse spin fluctuations are\nobservable in this doping-induced antiferromagnetic phase and the dynamic\nresonant spectral weight predominately shifts to the elastic channel. Static\n($\\tau$ $>$ 0.2 ns) collinear Ising order is proximate to superconductivity in\nCeCoIn$_{5}$ and is stabilized through hole doping with Hg.\n",
        "method": "We use neutron diffraction and spectroscopy to show that the magnetic resonant fluctuations present in the parent superconducting phase are replaced by collinear c-axis magnetic order with three-dimensional Ising critical fluctuations."
    },
    {
        "abstract": "  The problem of domain aiming control is formulated for controlled stochastic\nnonlinear systems. This issue involves regularity of the solution to the\nresulting closed-loop stochastic system. To begin with, an extended existence\nand uniqueness theorem for stochastic differential equation with local\nLipschitz coefficients is proven by using a Lyapunov-type function. A\nLyapunov-based sufficient condition is also given under which there is no\nregularity of the solution for a class of stochastic differential equations.\nThe notions of domain recurrence and residence time for stochastic nonlinear\nsystems are introduced, and various criteria for the recurrence and\nnon-recurrence relative to a bounded open domain or an unbounded domain are\nprovided. Furthermore, upper bounds of either the expectation or the\nmoment-generating function of the residence time are derived. In particular, a\nconnection between the mean residence time and a Dirichlet problem is\ninvestigated and illustrated with a numerical example. Finally, the problem of\ndomain aiming control is considered for certain types of nonlinear and linear\nstochastic systems. Several examples are provided to illustrate the theoretical\nresults.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* An extended existence and uniqueness theorem for stochastic differential equation with local Lipschitz coefficients is proven by using a Lyapunov-type function.\n* A Lyapunov-based sufficient condition is also given under which there is no regularity of the solution for a class of stochastic differential equations."
    },
    {
        "abstract": "  Binary Neural Networks (BNNs) are promising to deliver accuracy comparable to\nconventional deep neural networks at a fraction of the cost in terms of memory\nand energy. In this paper, we introduce the XNOR Neural Engine (XNE), a fully\ndigital configurable hardware accelerator IP for BNNs, integrated within a\nmicrocontroller unit (MCU) equipped with an autonomous I/O subsystem and hybrid\nSRAM / standard cell memory. The XNE is able to fully compute convolutional and\ndense layers in autonomy or in cooperation with the core in the MCU to realize\nmore complex behaviors. We show post-synthesis results in 65nm and 22nm\ntechnology for the XNE IP and post-layout results in 22nm for the full MCU\nindicating that this system can drop the energy cost per binary operation to\n21.6fJ per operation at 0.4V, and at the same time is flexible and performant\nenough to execute state-of-the-art BNN topologies such as ResNet-34 in less\nthan 2.2mJ per frame at 8.9 fps.\n",
        "method": "The methodological sentence is:\n\n\"The XNE is able to fully compute convolutional and dense layers in autonomy or in cooperation with the core in the MCU to realize more complex behaviors.\""
    },
    {
        "abstract": "  We derive an analytical expression for the transition path time (TPT)\ndistribution for a one-dimensional particle crossing a parabolic barrier. The\nsolution is expressed in terms of the eigenfunctions and eigenvalues of the\nassociated Fokker-Planck equation. The particle performs an anomalous dynamics\ngenerated by a power-law memory kernel, which includes memoryless Markovian\ndynamics as a limiting case. Our result takes into account absorbing boundary\nconditions, extending existing results obtained for free boundaries. We show\nthat TPT distributions obtained from numerical simulations are in excellent\nagreement with analytical results, while the typically employed free boundary\nconditions lead to a systematic overestimation of the barrier height. These\nfindings may be useful in the analysis of experimental results on transition\npath times. A web tool to perform this analysis is freely available.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe solution is expressed in terms of the eigenfunctions and eigenvalues of the associated Fokker-Planck equation."
    },
    {
        "abstract": "  In this paper, we discuss a method for identifying a seed word that would\nbest represent a class of named entities in a graphical representation of words\nand their similarities. Word networks, or word graphs, are representations of\nvectorized text where nodes are the words encountered in a corpus, and the\nweighted edges incident on the nodes represent how similar the words are to\neach other. We intend to build a bilingual word graph and identify seed words\nthrough community analysis that would be best used to segment a graph according\nto its named entities, therefore providing an unsupervised way of tagging named\nentities for a bilingual language base.\n",
        "method": "We discuss a method for identifying a seed word that would best represent a class of named entities in a graphical representation of words and their similarities."
    },
    {
        "abstract": "  Novel mechanisms for zonal flow (ZF) generation for both large relative\ndensity fluctuations and background density gradients are presented. In this\nnon-Oberbeck-Boussinesq (NOB) regime ZFs are driven by the Favre stress, the\nlarge fluctuation extension of the Reynolds stress, and by background density\ngradient and radial particle flux dominated terms. Simulations of a nonlinear\nfull-F gyro-fluid model confirm the predicted mechanism for radial ZF\npropagation and show the significance of the NOB ZF terms for either large\nrelative density fluctuation levels or steep background density gradients.\n",
        "method": "Here is the methodological sentence:\n\nSimulations of a nonlinear full-F gyro-fluid model confirm the predicted mechanism for radial ZF propagation and show the significance of the NOB ZF terms for either large relative density fluctuation levels or steep background density gradients."
    },
    {
        "abstract": "  We present and analyse the sunspot observations performed by Franz I. C.\nHallaschka in 1814 and 1816. These solar observations were carried out during\nthe so-called Dalton minimum, around the maximum phase of the Solar Cycle 6.\nThese records are very valuable because they allow us to complete observational\ngaps in the collection of sunspot group numbers, improving its coverage for\nthis epoch. We have analysed and compared the observations made by Hallaschka\nwith the records made by other contemporary observers. Unfortunately, the\nanalysis of the sunspot areas and positions showed that they are too inaccurate\nfor scientific use. But, we conclude that sunspot counts made by Hallaschka are\nsimilar to those made by other astronomers of that time. The observations by\nHallaschka confirm a low level of the solar activity during the Dalton minimum.\n",
        "method": "These records are very valuable because they allow us to complete observational gaps in the collection of sunspot group numbers, improving its coverage for this epoch."
    },
    {
        "abstract": "  We study resonant energy transfer in a one-dimensional chain of two to five\natoms by analyzing time-dependent probabilities as function of their\ninteratomic distances. The dynamics of the system are first investigated by\nincluding the nearest-neighbour interactions and then accounting for all\nnext-neighbour interactions. We find that inclusion of nearest-neighbour\ninteractions in the Hamiltonian for three atoms chain exhibits perdiocity\nduring the energy transfer dynamics, however this behavior displays\naperiodicity with the all-neighbour interactions. It shows for the equidistant\nchains of four and five atoms the peaks are always irregular but regular peaks\nare retrieved when the inner atoms are placed closer than the atoms at both the\nends. In this arrangement, the energy transfer swings between the atoms at both\nends with very low probability of finding an atom at the center. This\nphenomenon resembles with quantum notion of Newton's cradle. We also find out\nthe maximum distance up to which energy could be transferred within the typical\nlifetimes of the Rydberg states.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The dynamics of the system are first investigated by including the nearest-neighbor interactions and then accounting for all next-neighbour interactions.\""
    },
    {
        "abstract": "  Reversed structures of artificial spin-ice systems, where elongated holes\nwith elliptical shape (antidots) are arranged into a square array with two\northogonal sublattices, are referred to as anti-squared spin-ice. Using\nBrillouin light scattering spectroscopy and plane wave method calculations, we\ninvestigate the spin wave propagation perpendicular to the applied field\ndirection for two 20 nm thick Permalloy nanostructures which differ by the\npresence of single and double elliptical antidots. For the spin waves\npropagation along the principal antidot lattice axis, the spectrum consists of\nflat bands separated by several frequency gaps which are the effect of spin\nwave amplitude confinement in the regions between antidots. Contrarily, for\npropagation direction at 45 degrees with respect to the antidot symmetry axis,\nstraight and narrow channels of propagation are formed, leading to broadening\nof bands and closing of the magnonics gaps. Interestingly, in this case, extra\nmagnonic band gaps occur due to the additional periodicity along this\ndirection. The width and the position of these gaps depend on the presence of\nsingle or double antidots. In this context, we discuss possibilities for the\ntuning of spin wave spectra in anti-squared spin ice structures.\n",
        "method": "Using Brillouin light scattering spectroscopy and plane wave method calculations, we investigate the spin wave propagation perpendicular to the applied field direction for two 20 nm thick Permalloy nanostructures which differ by the presence of single and double elliptical antidots."
    },
    {
        "abstract": "  The ratio of penumbral to umbral area of sunspots is an important topic for\nsolar and geophysical studies. Hathaway (Solar Physics, 286, 347, 2013) found a\ncurious behaviour in this parameter for small sunspot groups (areas smaller\nthan 100 millionths of solar hemisphere, msh) using records from Royal\nGreenwich Observatory (RGO). Hathaway showed that penumbra-umbra ratio\ndecreased smoothly from more than 7 in 1905 to lower than 3 by 1930 and then\nincreased to almost 8 in 1961. Thus, Hathaway proposed the existence of a\nsecular variation in the penumbra-umbra area ratio. In order to confirm that\nsecular variation, we employ data of the sunspot catalogue published by the\nCoimbra Astronomical Observatory (COI) for the period 1929-1941. Our results\ndisagree with the penumbra-umbra ratio found by Hathaway for that period.\nHowever, the behaviour of this ratio for large (areas greater or equal than 100\nmsh) and small groups registered in COI during 1929-1941 is similar to data\navailable from RGO for the periods 1874-1914 and 1950-1976. Nevertheless, while\nthe average values and time evolution of the ratio in large groups is similar\nto the ratio for small groups according to Coimbra data (1929-1941) it is not\nanalogous for RGO data for the same period. We also found that the behaviour of\nthe penumbra-umbra area ratio for smaller groups in both observatories is\nsignificantly different. The main difference between the area measurements made\nin Coimbra and RGO is associated with the umbra measurements. We would like to\nstress that the two observatories used different methods of observation and\nwhile in COI both methodology and instruments did not change during the study\nperiod, some changes were carried out in RGO that could have affected\nmeasurements of umbra and penumbra. These facts illustrate the importance of\nthe careful recovery of past solar data.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We employ data of the sunspot catalogue published by the Coimbra Astronomical Observatory (COI) for the period 1929-1941.\""
    },
    {
        "abstract": "  This paper introduces a new approach to patch-based image restoration based\non external datasets and importance sampling. The Minimum Mean Squared Error\n(MMSE) estimate of the image patches, the computation of which requires solving\na multidimensional (typically intractable) integral, is approximated using\nsamples from an external dataset. The new method, which can be interpreted as a\ngeneralization of the external non-local means (NLM), uses self-normalized\nimportance sampling to efficiently approximate the MMSE estimates. The use of\nself-normalized importance sampling endows the proposed method with great\nflexibility, namely regarding the statistical properties of the measurement\nnoise. The effectiveness of the proposed method is shown in a series of\nexperiments using both generic large-scale and class-specific external\ndatasets.\n",
        "method": "The new approach to patch-based image restoration based on external datasets and importance sampling approximates the Minimum Mean Squared Error (MMSE) estimate of the image patches using samples from an external dataset."
    },
    {
        "abstract": "  The Muon Ionization Cooling Experiment (MICE) has been built at the STFC\nRutherford Appleton Laboratory to demonstrate the principle of muon beam\nphase-space reduction via ionization cooling. Muon beam cooling will be\nrequired at a future proton-derived neutrino factory or muon collider.\nIonization cooling is achieved by passing the beam through an energy-absorbing\nmaterial, such as liquid hydrogen, and then re-accelerating the beam using RF\ncavities. This paper describes the hydrogen system constructed for MICE\nincluding: the liquid-hydrogen absorber, its associated cryogenic and gas\nsystems, the control and monitoring system, and the necessary safety\nengineering. The performance of the system in cool-down, liquefaction, and\nstable operation is also presented.\n",
        "method": "This paper describes the hydrogen system constructed for MICE including: the liquid-hydrogen absorber, its associated cryogenic and gas systems, the control and monitoring system, and the necessary safety engineering."
    },
    {
        "abstract": "  In this paper we consider a bootstrap class $\\mathfrak C$ of countable\ndiscrete groups, which is closed under countable unions and extensions by the\nintegers, and we study actions of such groups on C*-algebras. This class\nincludes all torsion-free abelian groups, poly-$\\mathbb Z$-groups, as well as\nother examples. Using the interplay between relative Rokhlin dimension and\nsemi-strongly self-absorbing actions established in prior work, we obtain the\nfollowing two main results for any group $\\Gamma\\in\\mathfrak C$ and any\nstrongly self-absorbing C*-algebra $\\mathcal D$: (1) There is a unique strongly\nouter $\\Gamma$-action on $\\mathcal D$ up to (very strong) cocycle conjugacy.\n(2) If $\\alpha: \\Gamma\\curvearrowright A$ is a strongly outer action on a\nseparable, unital, nuclear, simple, $\\mathcal D$-stable C*-algebra with at most\none trace, then it absorbs every $\\Gamma$-action on $\\mathcal D$ up to (very\nstrong) cocycle conjugacy. In fact we establish more general relative versions\nof these two results for actions of amenable groups that have a predetermined\nquotient in the class $\\mathfrak C$. For the monotracial case, the proof\ncomprises an application of Matui--Sato's equivariant property (SI) as a key\nmethod.\n",
        "method": "Methodological sentences:\n\nUsing the interplay between relative Rokhlin dimension and semi-strongly self-absorbing actions established in prior work..."
    },
    {
        "abstract": "  The requirement of large amounts of annotated images has become one grand\nchallenge while training deep neural network models for various visual\ndetection and recognition tasks. This paper presents a novel image synthesis\ntechnique that aims to generate a large amount of annotated scene text images\nfor training accurate and robust scene text detection and recognition models.\nThe proposed technique consists of three innovative designs. First, it realizes\n\"semantic coherent\" synthesis by embedding texts at semantically sensible\nregions within the background image, where the semantic coherence is achieved\nby leveraging the semantic annotations of objects and image regions that have\nbeen created in the prior semantic segmentation research. Second, it exploits\nvisual saliency to determine the embedding locations within each semantic\nsensible region, which coincides with the fact that texts are often placed\naround homogeneous regions for better visibility in scenes. Third, it designs\nan adaptive text appearance model that determines the color and brightness of\nembedded texts by learning from the feature of real scene text images\nadaptively. The proposed technique has been evaluated over five public datasets\nand the experiments show its superior performance in training accurate and\nrobust scene text detection and recognition models.\n",
        "method": "The proposed technique consists of three innovative designs: ... First, it realizes \"semantic coherent\" synthesis by embedding texts at semantically sensible regions within the background image... Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region... Third, it designs an adaptive text appearance model that determines the color and brightness of embedded texts by learning from the feature of real scene text images adaptively."
    },
    {
        "abstract": "  The model of holographic dark energy in which dark energy interacts with dark\nmatter is investigated in this paper. In particular, we consider the\ninteracting holographic dark energy model in the context of a perturbed\nuniverse, which was never investigated in the literature. To avoid the\nlarge-scale instability problem in the interacting dark energy cosmology, we\nemploy the generalized version of the parameterized post-Friedmann approach to\ntreat the dark energy perturbations in the model. We use the current\nobservational data to constrain the model. Since the cosmological perturbations\nare considered in the model, we can then employ the redshift-space distortions\n(RSD) measurements to constrain the model, in addition to the use of the\nmeasurements of expansion history, which was either never done in the\nliterature. We find that, for both the cases with $Q=\\beta H\\rho_{\\rm c}$ and\n$Q=\\beta H_0\\rho_{\\rm c}$, the interacting holographic dark energy model is\nmore favored by the current data, compared to the holographic dark energy model\nwithout interaction. It is also found that, with the help of the RSD data, a\npositive coupling $\\beta$ can be detected at the $2.95\\sigma$ statistical\nsignificance for the case of $Q=\\beta H_0\\rho_{\\rm c}$.\n",
        "method": "Here is the methodological sentence:\n\nWe employ the generalized version of the parameterized post-Friedmann approach to treat the dark energy perturbations in the model."
    },
    {
        "abstract": "  Fiber photometry permits monitoring fluorescent indicators of neural activity\nin behaving animals. Optical fibers are typically used to excite and collect\nfluorescence from genetically-encoded calcium indicators expressed by a subset\nof neurons in a circuit of interest. However, a quantitative understanding of\nthe brain volumes from which signal is collected and how this depends on the\nproperties of the optical fibers are lacking. Here we analytically model and\nexperimentally measure the light emission and collection fields for optical\nfibers in solution and scattering tissue, providing a comprehensive\ncharacterization of fibers commonly employed for fiber photometry. Since\nphotometry signals depend on both excitation and collection efficiency, a\ncombined confocal/2-photon microscope was developed to evaluate these\nparameters independently. We find that the 80% of the effective signal arises\nfrom a 10^5-10^6 um3 volume extending ~200 um from the fiber face, and thus\npermitting a spatial interpretation of measurements made with fiber photometry.\n",
        "method": "Here is the methodological sentence:\n\nOptical fibers are typically used to excite and collect fluorescence from genetically-encoded calcium indicators expressed by a subset of neurons in a circuit of interest."
    },
    {
        "abstract": "  We address the problem of causal discovery from data, making use of the\nrecently proposed causal modeling framework of modular structural causal models\n(mSCM) to handle cycles, latent confounders and non-linearities. We introduce\n{\\sigma}-connection graphs ({\\sigma}-CG), a new class of mixed graphs\n(containing undirected, bidirected and directed edges) with additional\nstructure, and extend the concept of {\\sigma}-separation, the appropriate\ngeneralization of the well-known notion of d-separation in this setting, to\napply to {\\sigma}-CGs. We prove the closedness of {\\sigma}-separation under\nmarginalisation and conditioning and exploit this to implement a test of\n{\\sigma}-separation on a {\\sigma}-CG. This then leads us to the first causal\ndiscovery algorithm that can handle non-linear functional relations, latent\nconfounders, cyclic causal relationships, and data from different (stochastic)\nperfect interventions. As a proof of concept, we show on synthetic data how\nwell the algorithm recovers features of the causal graph of modular structural\ncausal models.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe introduce {\\sigma}-connection graphs ({\\sigma}-CG), a new class of mixed graphs (containing undirected, bidirected and directed edges) with additional structure, and extend the concept of {\\sigma}-separation..."
    },
    {
        "abstract": "  In this paper we propose local and global existence results for the solution\nof systems characterized by the coupling of ODEs and PDEs. The coexistence of\ndistinct mathematical formalisms represents the main feature of hybrid\napproaches, in which the dynamics of interacting agents are driven by\nsecond-order ODEs, while reaction-diffusion equations are used to model the\ntime evolution of a signal influencing them. We first present an existence\nresult of the solution, locally in time. In particular, we generalize the\nframework of recent works presented in the literature, concerning collective\nmotions of cells due to mechanical forces and chemotaxis, taking into account a\nuniformly parabolic operator with space-and-time dependent coefficients, and a\nmore general structure for the equations of motion. Then, the previous result\nis extended in order to obtain a global solution.\n",
        "method": "We first present an existence result of the solution, locally in time."
    },
    {
        "abstract": "  We introduce a novel generative autoencoder network model that learns to\nencode and reconstruct images with high quality and resolution, and supports\nsmooth random sampling from the latent space of the encoder. Generative\nadversarial networks (GANs) are known for their ability to simulate random\nhigh-quality images, but they cannot reconstruct existing images. Previous\nworks have attempted to extend GANs to support such inference but, so far, have\nnot delivered satisfactory high-quality results. Instead, we propose the\nProgressively Growing Generative Autoencoder (PIONEER) network which achieves\nhigh-quality reconstruction with $128{\\times}128$ images without requiring a\nGAN discriminator. We merge recent techniques for progressively building up the\nparts of the network with the recently introduced adversarial encoder-generator\nnetwork. The ability to reconstruct input images is crucial in many real-world\napplications, and allows for precise intelligent manipulation of existing\nimages. We show promising results in image synthesis and inference, with\nstate-of-the-art results in CelebA inference tasks.\n",
        "method": "Instead, we propose the Progressively Growing Generative Autoencoder (PIONEER) network which achieves high-quality reconstruction with $128{\\times}128$ images without requiring a GAN discriminator."
    },
    {
        "abstract": "  This paper proposes a general framework for internal patch-based image\nrestoration based on Conditional Random Fields (CRF). Unlike related models\nbased on Markov Random Fields (MRF), our approach explicitly formulates the\nposterior distribution for the entire image. The potential functions are taken\nas proportional to the product of a likelihood and prior for each patch. By\nassuming identical parameters for similar patches, our approach can be\nclassified as a model-based non-local method. For the prior term in the\npotential function of the CRF model, multivariate Gaussians and multivariate\nscale-mixture of Gaussians are considered, with the latter being a novel prior\nfor image patches. Our results show that the proposed approach outperforms\nmethods based on Gaussian mixture models for image denoising and\nstate-of-the-art methods for image interpolation/inpainting.\n",
        "method": "Unlike related models based on Markov Random Fields (MRF), our approach explicitly formulates the posterior distribution for the entire image."
    },
    {
        "abstract": "  We survey and analyze different ways in which bornologies, coarse structures\nand uniformities on a group agree with the group operations.\n",
        "method": "We survey and analyze different ways in which bornologies, coarse structures and uniformities on a group agree with the group operations."
    },
    {
        "abstract": "  Homoclinic and heteroclinic orbits provide a skeleton of the full dynamics of\na chaotic dynamical system and are the foundation of semiclassical sums for\nquantum wave packet, coherent state, and transport quantities. Here, the\nhomoclinic orbits are organized according to the complexity of their\nphase-space excursions, and exact relations are derived expressing the relative\nclassical actions of complicated orbits as linear combinations of those with\nsimpler excursions plus phase-space cell areas bounded by stable and unstable\nmanifolds. The total number of homoclinic orbits increases exponentially with\nexcursion complexity, and the corresponding cell areas decrease exponentially\nin size as well. With the specification of a desired precision, the\nexponentially proliferating set of homoclinic orbit actions is expressible by a\nslower-than-exponentially increasing set of cell areas, which may present a\nmeans for developing greatly simplified semiclassical formulas.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Here, the homoclinic orbits are organized according to the complexity of their phase-space excursions...\n* Exact relations are derived expressing the relative classical actions of complicated orbits as linear combinations of those with simpler excursions plus phase-space cell areas bounded by stable and unstable manifolds."
    },
    {
        "abstract": "  We introduce topological prismatoids, a combinatorial abstraction of the\n(geometric) prismatoids recently introduced by the second author to construct\ncounter-examples to the Hirsch conjecture. We show that the `strong $d$-step\nTheorem' that allows to construct such large-diameter polytopes from\n`non-$d$-step' prismatoids still works at this combinatorial level. Then, using\nmetaheuristic methods on the flip graph, we construct four combinatorially\ndifferent non-$d$-step $4$-dimensional topological prismatoids with $14$\nvertices. This implies the existence of $8$-dimensional spheres with $18$\nvertices whose combinatorial diameter exceeds the Hirsch bound. These examples\nare smaller that the previously known examples by Mani and Walkup in 1980 ($24$\nvertices, dimension $11$).\n  Our non-Hirsch spheres are shellable but we do not know whether they are\nrealizable as polytopes.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe show that the `strong $d$-step Theorem' that allows to construct such large-diameter polytopes from `non-$d$-step' prismatoids still works at this combinatorial level."
    },
    {
        "abstract": "  We survey recent mathematical results about the spectrum of random band\nmatrices. We start by exposing the Erd{\\H o}s-Schlein-Yau dynamic approach, its\napplication to Wigner matrices, and extension to other mean-field models. We\nthen introduce random band matrices and the problem of their Anderson\ntransition. We finally describe a method to obtain delocalization and\nuniversality in some sparse regimes, highlighting the role of quantum unique\nergodicity.\n",
        "method": "We then introduce random band matrices and the problem of their Anderson transition."
    },
    {
        "abstract": "  Stencil computations are a key part of many high-performance computing\napplications, such as image processing, convolutional neural networks, and\nfinite-difference solvers for partial differential equations. Devito is a\nframework capable of generating highly-optimized code given symbolic equations\nexpressed in Python, specialized in, but not limited to, affine (stencil)\ncodes. The lowering process---from mathematical equations down to C++ code---is\nperformed by the Devito compiler through a series of intermediate\nrepresentations. Several performance optimizations are introduced, including\nadvanced common sub-expressions elimination, tiling and parallelization. Some\nof these are obtained through well-established stencil optimizers, integrated\nin the back-end of the Devito compiler. The architecture of the Devito\ncompiler, as well as the performance optimizations that are applied when\ngenerating code, are presented. The effectiveness of such performance\noptimizations is demonstrated using operators drawn from seismic imaging\napplications.\n",
        "method": "The lowering process---from mathematical equations down to C++ code---is performed by the Devito compiler through a series of intermediate representations."
    },
    {
        "abstract": "  Due to their simple construction, LFSRs are commonly used as building blocks\nin various random number generators. Nonlinear feedforward logic is\nincorporated in LFSRs to increase the linear complexity of the generated\nsequence. In this work, we extend the idea of nonlinear feedforward logic to\nLFSRs over arbitrary finite fields and analyze the statistical properties of\nthe generated sequences. Further, we propose a method of applying nonlinear\nfeedforward logic to word-based {\\sigma}-LFSRs and show that the proposed\nscheme generates vector sequences that are statistically more balanced than\nthose generated by an existing scheme.\n",
        "method": "Nonlinear feedforward logic is incorporated in LFSRs to increase the linear complexity of the generated sequence."
    },
    {
        "abstract": "  The methods of new institutional economics for identifying the transaction\ncosts of trade litigations in Bulgaria are used in the current paper. For the\nneeds of the research, an indicative model, measuring this type of costs on\nmicroeconomic level, is applied in the study. The main purpose of the model is\nto forecast the rational behavior of trade litigation parties in accordance\nwith the transaction costs in the process of enforcing the execution of the\nsigned commercial contract. The application of the model is related to the more\naccurate measurement of the transaction costs on microeconomic level, which\nfact could lead to better prediction and management of these costs in order\nmarket efficiency and economic growth to be achieved. In addition, it is made\nan attempt to be analysed the efficiency of the institutional change of the\ncommercial justice system and the impact of the reform of the judicial system\nover the economic turnover. The augmentation or lack of reduction of the\ntransaction costs in trade litigations would mean inefficiency of the reform of\nthe judicial system. JEL Codes: O43, P48, D23, K12\n",
        "method": "The methodological sentences are:\n\n* For the needs of the research, an indicative model, measuring this type of costs on microeconomic level, is applied in the study.\n* The application of the model is related to the more accurate measurement of the transaction costs on microeconomic level, which fact could lead to better prediction and management of these costs in order market efficiency and economic growth to be achieved."
    },
    {
        "abstract": "  We study the internal controllability of a wave equation with memory in the\nprincipal part, defined on the one-dimensional torus\n$\\mathbb{T}=\\mathbb{R}/2\\pi\\mathbb{Z}$. We assume that the control is acting on\nan open subset $\\omega(t)\\subset\\mathbb{T}$, which is moving with a constant\nvelocity $c\\in\\mathbb{R}\\setminus\\{-1,0,1\\}$. The main result of the paper\nshows that the equation is null controllable in a sufficiently large time $T$\nand for initial data belonging to suitable Sobolev spaces. Its proof follows\nfrom a careful analysis of the spectrum associated to our problem and from the\napplication of the classical moment method.\n",
        "method": "We assume that the control is acting on an open subset \u03c9(t)\u2282\u2124, which is moving with a constant velocity c\u2208R\\{-1,0,1\\}."
    },
    {
        "abstract": "  I suggest a new extension of the standard model of particle physics, which\nintroduces a dark sector with the $SU(2)_{D}\\otimes U(1)_{D'}$ symmetry besides\nthe SM sector. The new particles of the model all inhabit in the dark sector.\nThe dark gauge symmetry breaking will bring about fruitful physics beyond the\nSM. The tiny neutrino mass is generated through the Dirac-type seesaw\nmechanism. The inflaton decay can not only provide the universe inflation and\nreheating, but also lead to the baryon asymmetry and the asymmetric cold dark\nmatter. In short, the model provides an unification of the neutrino mass, the\nbaryon asymmetry, the asymmetric CDM and the inflation, and it can account for\ntheir common origin. Finally, it is very possible to test the model predictions\nand probe the dark sector physics in near future experiments.\n",
        "method": "The new particles of the model all inhabit in the dark sector."
    },
    {
        "abstract": "  The results of observations with the MASTER-SHOK robotic wide-field optical\ncameras onboard the Lomonosov Space Observatory carried out in 2016 are\npresented. In all, the automated transient detection system transmitted 22 181\nimages of moving objects with signal-to-noise ratios greater than 5 to the\nEarth. Approximately 84% of these images are identified with well-known\nartificial Earth satellites (including repeated images of the same satellite)\nand fragments of such satellites (space debris), according to databases of\nknown satellites. The remaining 16% of the images are relate to uncatalogued\nobjects. This first experience in optical space-based monitoring of near-Earth\nspace demonstrates the high efficiency and great potential of using\nlarge-aperture cameras in space, based on the software and technology of the\nMASTER robotic optical complexes (the Mobile Astronomical System of\nTElescope-Robots (MASTER) global network of robotic telescopes of Lomonosov\nMoscow State University).\n",
        "method": "The automated transient detection system transmitted 22 181 images of moving objects with signal-to-noise ratios greater than 5 to the Earth."
    },
    {
        "abstract": "  We describe a framework for constructing an efficient non-interactive key\nexchange (NIKE) protocol for n parties for any n >= 2. Our approach is based on\nthe problem of computing isogenies between isogenous elliptic curves, which is\nbelieved to be difficult. We do not obtain a working protocol because of a\nmissing step that is currently an open mathematical problem. What we need to\ncomplete our protocol is an efficient algorithm that takes as input an abelian\nvariety presented as a product of isogenous elliptic curves, and outputs an\nisomorphism invariant of the abelian variety.\n  Our framework builds a cryptographic invariant map, which is a new primitive\nclosely related to a cryptographic multilinear map, but whose range does not\nnecessarily have a group structure. Nevertheless, we show that a cryptographic\ninvariant map can be used to build several cryptographic primitives, including\nNIKE, that were previously constructed from multilinear maps and\nindistinguishability obfuscation.\n",
        "method": "Here are the methodological sentences:\n\n* Our approach is based on the problem of computing isogenies between isogenous elliptic curves, which is believed to be difficult.\n* What we need to complete our protocol is an efficient algorithm that takes as input an abelian variety presented as a product of isogenous elliptic curves, and outputs an isomorphism invariant of the abelian variety."
    },
    {
        "abstract": "  Flow-based generative models (Dinh et al., 2014) are conceptually attractive\ndue to tractability of the exact log-likelihood, tractability of exact\nlatent-variable inference, and parallelizability of both training and\nsynthesis. In this paper we propose Glow, a simple type of generative flow\nusing an invertible 1x1 convolution. Using our method we demonstrate a\nsignificant improvement in log-likelihood on standard benchmarks. Perhaps most\nstrikingly, we demonstrate that a generative model optimized towards the plain\nlog-likelihood objective is capable of efficient realistic-looking synthesis\nand manipulation of large images. The code for our model is available at\nhttps://github.com/openai/glow\n",
        "method": "Methodological sentences:\n\n* Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis.\n* Using our method we propose Glow, a simple type of generative flow using an invertible 1x1 convolution."
    },
    {
        "abstract": "  The meaning of public messages such as \"One in x people gets cancer\" or \"One\nin y people gets cancer by age z\" can be improved. One assumption commonly\ninvoked is that there is no other cause of death, a confusing assumption. We\ndevelop a light bulb model to clarify cumulative risk and we use Markov chain\nmodeling, incorporating the assumption widely in place, to evaluate transition\nprobabilities. Age-progression in the cancer risk is then reported on\nAustralian data. Future modelling can elicit realistic assumptions.\n",
        "method": "We develop a light bulb model to clarify cumulative risk and we use Markov chain modeling, incorporating the assumption widely in place, to evaluate transition probabilities."
    },
    {
        "abstract": "  We have carried out density-functional theory (DFT) calculations for the\nchromium pnictide BaCr2P2, which is structurally analogous to BaFe2As2, a\nparent compound for iron-pnictide superconductors. Evolutionary methods\ncombined with DFT predict that the chromium analog has the same crystal\nstructure as the latter. DFT also predicts Neel antiferromagnetic order on the\nchromium sites. Comparison with a simple electron-hopping model over a square\nlattice of chromium atoms suggests that it is due to residual nesting of the\nFermi surfaces. We have confirmed the DFT predictions directly after the\nsuccessful synthesis of polycrystalline samples of BaCr2P2. X-ray diffraction\nrecovers the predicted crystal structure to high accuracy, while magnetic\nsusceptibility and specific-heat measurements are consistent with a transition\nto an antiferromagnetically ordered state below T_N ~ 60 K.\n",
        "method": "We have carried out density-functional theory (DFT) calculations for the chromium pnictide BaCr2P2... DFT also predicts Neel antiferromagnetic order on the chromium sites."
    },
    {
        "abstract": "  We show that substantially enhanced mass loss at periastron passages, as is\nexpected in the grazing envelope evolution (GEE), can compensate for the\ncircularization effect of the tidal interaction in binary systems composed of\nan asymptotic giant branch (AGB) star and a main sequence secondary star. By\nnumerically integrating the equations of motion we show that under our\nassumptions the binary system can maintain its high eccentricity as the AGB\nstar evolves toward the post-AGB phase. Our results can explain the high\neccentricity of some post-AGB intermediate binaries (post-AGBIBs), i.e., those\nwith an orbital periods in the range of several months to few years. In the\nframework of the GEE, the extra energy to sustain a high mass loss rate comes\nfrom the accretion of mass from the giant envelope or its slow wind onto a more\ncompact secondary star. The secondary star energizes the outflow from the AGB\nouter envelope by launching jets from the accretion disk.\n",
        "method": "We show that substantially enhanced mass loss at periastron passages, as is expected in the grazing envelope evolution (GEE), can compensate for the circularization effect of the tidal interaction in binary systems composed of an asymptotic giant branch (AGB) star and a main sequence secondary star."
    },
    {
        "abstract": "  Control of blood glucose is essential for diabetes management. Current\ndigital therapeutic approaches for subjects with Type 1 diabetes mellitus\n(T1DM) such as the artificial pancreas and insulin bolus calculators leverage\nmachine learning techniques for predicting subcutaneous glucose for improved\ncontrol. Deep learning has recently been applied in healthcare and medical\nresearch to achieve state-of-the-art results in a range of tasks including\ndisease diagnosis, and patient state prediction among others. In this work, we\npresent a deep learning model that is capable of forecasting glucose levels\nwith leading accuracy for simulated patient cases (RMSE = 9.38$\\pm$0.71 [mg/dL]\nover a 30-minute horizon, RMSE = 18.87$\\pm$2.25 [mg/dL] over a 60-minute\nhorizon) and real patient cases (RMSE = 21.07$\\pm$2.35 [mg/dL] for 30-minute,\nRMSE = 33.27$\\pm$4.79\\% for 60-minute). In addition, the model provides\ncompetitive performance in providing effective prediction horizon ($PH_{eff}$)\nwith minimal time lag both in a simulated patient dataset ($PH_{eff}$ =\n29.0$\\pm$0.7 for 30-min and $PH_{eff}$ = 49.8$\\pm$2.9 for 60-min) and in a real\npatient dataset ($PH_{eff}$ = 19.3$\\pm$3.1 for 30-min and $PH_{eff}$ =\n29.3$\\pm$9.4 for 60-min). This approach is evaluated on a dataset of 10\nsimulated cases generated from the UVa/Padova simulator and a clinical dataset\nof 10 real cases each containing glucose readings, insulin bolus, and meal\n(carbohydrate) data. Performance of the recurrent convolutional neural network\nis benchmarked against four algorithms. The proposed algorithm is implemented\non an Android mobile phone, with an execution time of $6$ms on a phone compared\nto an execution time of $780$ms on a laptop.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"This approach is evaluated on a dataset of 10 simulated cases generated from the UVa/Padova simulator and a clinical dataset of 10 real cases each containing glucose readings, insulin bolus, and meal (carbohydrate) data.\""
    },
    {
        "abstract": "  This paper introduces a Hopf algebra structure on a family of reduced pipe\ndreams. We show that this Hopf algebra is free and cofree, and construct a\nsurjection onto a commutative Hopf algebra of permutations. The pipe dream Hopf\nalgebra contains Hopf subalgebras with interesting sets of generators and\nHilbert series related to subsequences of Catalan numbers. Three other relevant\nHopf subalgebras include the Loday-Ronco Hopf algebra on complete binary trees,\na Hopf algebra related to a special family of lattice walks on the quarter\nplane, and a Hopf algebra on $\\nu$-trees related to $\\nu$-Tamari lattices. One\nof this Hopf subalgebras motivates a new notion of Hopf chains in the Tamari\nlattice, which are used to present applications and conjectures in the theory\nof multivariate diagonal harmonics.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We show that this Hopf algebra is free and cofree, and construct a surjection onto a commutative Hopf algebra of permutations.\""
    },
    {
        "abstract": "  Statements for public health purposes such as \"1 in 2 will get cancer by age\n85\" have appeared in public spaces. The meaning drawn from such statements\naffects economic welfare, not just public health. Both markets and government\nuse risk information on all kinds of risks, useful information can, in turn,\nimprove economic welfare, however inaccuracy can lower it. We adapt the\ncontingency table approach so that a quoted risk is cross-classified with the\nstates of nature. We show that bureaucratic objective functions regarding the\naccuracy of a reported cancer risk can then be stated.\n",
        "method": "Here are the methodological sentences:\n\n* We adapt the contingency table approach so that a quoted risk is cross-classified with the states of nature.\n* We show that bureaucratic objective functions regarding the accuracy of a reported cancer risk can then be stated."
    },
    {
        "abstract": "  This paper summarizes some recent advances on a set of tasks related to the\nprocessing of singing using state-of-the-art deep learning techniques. We\ndiscuss their achievements in terms of accuracy and sound quality, and the\ncurrent challenges, such as availability of data and computing resources. We\nalso discuss the impact that these advances do and will have on listeners and\nsingers when they are integrated in commercial applications.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe discuss their achievements in terms of accuracy and sound quality..."
    },
    {
        "abstract": "  We report the observations of a moderate but relatively intense geo-effective\nsolar eruption on 2015 November 4 from the peripheral diffusive polarities of\nactive region 12443. We use space-borne Solar Dynamics Observatory and ACE\nobservations. EUV images identified helical pattern along a filament channel\nand we regard this channel as flux-rope structure. Flow velocity derived from\ntracked magnetograms infers converging motion along the polarity inversion line\nbeneath the filament channel. An associated magnetic cancellation process was\ndetected in the converging region. Further, the pre-eruptive EUV brightening\nwas observed in the converging region, the most intense part of which appeared\nin the magnetic cancellation region. These observations imply that the\nconverging and cancelling flux probably contributed to the formation of the\nhelical magnetic fields associated with the flux rope. A filament-height\nestimation method suggests that the middle part of the filament probably lies\nat a low altitude and was consistent with the initial place of the eruption. A\nthick current channel associated with the flux rope is also determined. For a\nexpanding thick current channel, the critical height of the decay index for\ntorus instability lies in the range of 37 - 47 Mm. Southward magnetic fields in\nthe sheath and the ejecta induced a geomagnetic storm with a Dst global minimum\nof ~-90 nT.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We use space-borne Solar Dynamics Observatory and ACE observations.\n* EUV images identified helical pattern along a filament channel.\n* Flow velocity derived from tracked magnetograms infers converging motion along the polarity inversion line beneath the filament channel.\n* A filament-height estimation method suggests that the middle part of the filament probably lies at a low altitude."
    },
    {
        "abstract": "  This paper applies economic concepts from measuring income inequality to an\nexercise in assessing spatial inequality in cancer service access in regional\nareas. We propose a mathematical model for accessing chemotherapy among local\ngovernment areas (LGAs). Our model incorporates a distance factor. With a\nsimulation we report results for a single inequality measure: the Lorenz curve\nis depicted for our illustrative data. We develop this approach in order to\nmove incrementally towards its application to actual data and real-world health\nservice regions. We seek to develop the exercises that can lead policy makers\nto relevant policy information on the most useful data collections to be\ncollected and modeling for cancer service access in regional areas.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"With a simulation we report results for a single inequality measure: the Lorenz curve is depicted for our illustrative data.\""
    },
    {
        "abstract": "  The relaxation dynamics of photoexcited quasiparticles of three-dimensional\n(3D) Dirac semimetals are vital towards their application in high performance\nelectronic and optoelectronic devices. In this work, the relaxation dynamics of\nphotoexcited carriers of 3D Dirac semimetal Cd3As2 are investigated by\ntransient terahertz spectroscopy. The visible pump-THz probe spectroscopy\nmeasurement shows clear biexponential decays with two characteristic time\nconstants. According to the pump-power and temperature dependence, these two\ncharacteristic time constants are attributed to the electron phonon coupling\n(1-4 ps) and anharmonic decay of hot coupled phonons to electronic uncoupled\nphonons (2-9 ps), respectively. An anomalous electron-optical phonon coupling\nreduction and a bottleneck slowing of hot optical phonons relaxation are\nobserved with higher excitation intensities similar to that in graphene. On the\nother hand, the electron-optical phonon coupling can be enhanced due to the\nphonon frequency broadening and softening at elevated lattice temperature.\nFurthermore, the transient THz spectrum response is strongly modified by the\nphonon assisted intraband absorption of hot carriers from a pure electronic\nDrude model, which is evidenced by a characteristic THz absorption dip in the\ntransient THz absorption spectrum. This absorption dip is pinned by the\ndiscrete optical phonon energy that assists the intraband transition enabled by\nphotoexcitation of hot carriers.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In this work, the relaxation dynamics of photoexcited carriers of 3D Dirac semimetal Cd3As2 are investigated by transient terahertz spectroscopy.\n* The visible pump-THz probe spectroscopy measurement shows clear biexponential decays with two characteristic time constants."
    },
    {
        "abstract": "  This paper focuses on greedy expansions, one possible representation of\nnumbers, and on arithmetical operations with them. Performing addition or\nmultiplication some additional digits can appear. We study bounds on the number\nof such digits assuming the finiteness of the expansion of the considered sum\nor product, especially for the case of cubic Pisot units.\n",
        "method": "Performing addition or multiplication some additional digits can appear."
    },
    {
        "abstract": "  Rapid deployment and operation are key requirements in time critical\napplication, such as Search and Rescue (SaR). Efficiently teleoperated ground\nrobots can support first-responders in such situations. However, first-person\nview teleoperation is sub-optimal in difficult terrains, while a third-person\nperspective can drastically increase teleoperation performance. Here, we\npropose a Micro Aerial Vehicle (MAV)-based system that can autonomously provide\nthird-person perspective to ground robots. While our approach is based on local\nvisual servoing, it further leverages the global localization of several ground\nrobots to seamlessly transfer between these ground robots in GPS-denied\nenvironments. Therewith one MAV can support multiple ground robots on a demand\nbasis. Furthermore, our system enables different visual detection regimes, and\nenhanced operability, and return-home functionality. We evaluate our system in\nreal-world SaR scenarios.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWhile our approach is based on local visual servoing, it further leverages the global localization of several ground robots to seamlessly transfer between these ground robots in GPS-denied environments."
    },
    {
        "abstract": "  This paper describes how to apply self-attention with relative positional\nencodings to the task of relation extraction. We propose to use the\nself-attention encoder layer together with an additional position-aware\nattention layer that takes into account positions of the query and the object\nin the sentence. The self-attention encoder also uses a custom implementation\nof relative positional encodings which allow each word in the sentence to take\ninto account its left and right context. The evaluation of the model is done on\nthe TACRED dataset. The proposed model relies only on attention (no recurrent\nor convolutional layers are used), while improving performance w.r.t. the\nprevious state of the art.\n",
        "method": "We propose to use the self-attention encoder layer together with an additional position-aware attention layer that takes into account positions of the query and the object in the sentence."
    },
    {
        "abstract": "  Using natural language to give instructions to robots is challenging, since\nnatural language understanding is still largely an open problem. In this paper\nwe address this problem by restricting our attention to commands modeled as one\naction, plus arguments (also known as slots). For action detection (also called\nintent detection) and slot filling various architectures of Recurrent Neural\nNetworks and Long Short Term Memory (LSTM) networks were evaluated, having\nLSTMs achieved a superior accuracy. As the action requested may not fall within\nthe robots capabilities, a Support Vector Machine(SVM) is used to determine\nwhether it is or not. For the input of the neural networks, several word\nembedding algorithms were compared. Finally, to implement the system in a\nrobot, a ROS package is created using a SMACH state machine. The proposed\nsystem is then evaluated both using well-known datasets and benchmarks in the\ncontext of domestic service robots.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* For action detection (also called intent detection) and slot filling various architectures of Recurrent Neural Networks and Long Short Term Memory (LSTM) networks were evaluated, having LSTMs achieved a superior accuracy.\n* A Support Vector Machine (SVM) is used to determine whether the action requested may not fall within the robots capabilities.\n* Several word embedding algorithms were compared for the input of the neural networks."
    },
    {
        "abstract": "  We study the connection between the polymer deposition patterns to appear\nfollowing the evaporation of a solution of poly-methyl-methacrylate (PMMA) in\ntoluene, the transport of the polymer coils in the solution, and the motion of\nthe meniscus of the solution. Different deposition patterns are observed when\nvarying the molecular mass and initial concentration of the solute and\ntemperature and are systematically presented in the form of morphological phase\ndiagrams. The modi of deposition and meniscus motion are correlated. They vary\nwith the ratio between the evaporation-driven convective flux and diffusive\nflux of the polymer coils in the solution. In the case of a diffusion-dominated\nsolute transport, the solution monotonically dewets the solid substrate by\nevaporation, supporting continuous contact line motion and continuous polymer\ndeposition. However, a convection-dominated transport results in an oscillatory\nratcheting dewetting-wetting motion of the contact line with more pronounced\ndewetting phases. The deposition process is then periodic and produces a stripe\npattern. The oscillatory motion of the meniscus differs from the well\ndocumented stick-slip motion of the meniscus, observed as well, and is\nattributed to the opposing influences of evaporation and Marangoni stresses,\nwhich alternately dominate the deposition process.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Different deposition patterns are observed when varying the molecular mass and initial concentration of the solute and temperature...\n* The modes of deposition and meniscus motion are correlated.\n* They vary with the ratio between the evaporation-driven convective flux and diffusive flux of the polymer coils in the solution."
    },
    {
        "abstract": "  We study approximations of compact linear multivariate operators defined over\nHilbert spaces. We provide necessary and sufficient conditions on various\nnotions of tractability. These conditions are mainly given in terms of sums of\ncertain functions depending on the singular values of the multivariate problem.\nThey do not require the ordering of these singular values which in many cases\nis difficult to achieve.\n",
        "method": "We provide necessary and sufficient conditions on various notions of tractability."
    },
    {
        "abstract": "  Let $\\mathcal{Z}$ be a specialization closed subset of $\\Spec R$ and $X$ a\nhomologically left-bounded complex with finitely generated homologies. We\nestablish Faltings' Local-global Principle and Annihilator Theorems for the\nlocal cohomology modules {{$\\H_{\\mathcal{Z}}^i(X).$ }} Our versions contain\nvariations of results already known on these theorems.\n",
        "method": "Here is the methodological sentence:\n\nWe establish..."
    },
    {
        "abstract": "  In this paper, we derive a neural network architecture based on an analytical\nformulation of the parallel-to-fan beam conversion problem following the\nconcept of precision learning. The network allows to learn the unknown\noperators in this conversion in a data-driven manner avoiding interpolation and\npotential loss of resolution. Integration of known operators results in a small\nnumber of trainable parameters that can be estimated from synthetic data only.\nThe concept is evaluated in the context of Hybrid MRI/X-ray imaging where\ntransformation of the parallel-beam MRI projections to fan-beam X-ray\nprojections is required. The proposed method is compared to a traditional\nrebinning method. The results demonstrate that the proposed method is superior\nto ray-by-ray interpolation and is able to deliver sharper images using the\nsame amount of parallel-beam input projections which is crucial for\ninterventional applications. We believe that this approach forms a basis for\nfurther work uniting deep learning, signal processing, physics, and traditional\npattern recognition.\n",
        "method": "The network allows to learn the unknown operators in this conversion in a data-driven manner avoiding interpolation and potential loss of resolution."
    },
    {
        "abstract": "  Computer-aided techniques may lead to more accurate and more acces-sible\ndiagnosis of thorax diseases on chest radiography. Despite the success of deep\nlearning-based solutions, this task remains a major challenge in smart\nhealthcare, since it is intrinsically a weakly supervised learning problem. In\nthis paper, we incorporate the attention mechanism into a deep convolutional\nneural network, and thus propose the ChestNet model to address effective\ndiagnosis of thorax diseases on chest radiography. This model consists of two\nbranches: a classification branch serves as a uniform feature\nextraction-classification network to free users from troublesome handcrafted\nfeature extraction, and an attention branch exploits the correlation between\nclass labels and the locations of patholog-ical abnormalities and allows the\nmodel to concentrate adaptively on the patholog-ically abnormal regions. We\nevaluated our model against three state-of-the-art deep learning models on the\nChest X-ray 14 dataset using the official patient-wise split. The results\nindicate that our model outperforms other methods, which use no extra training\ndata, in diagnosing 14 thorax diseases on chest radiography.\n",
        "method": "Here is the methodological sentence:\n\nThis model consists of two branches: a classification branch serves as a uniform feature extraction-classification network to free users from troublesome handcrafted feature extraction, and an attention branch exploits the correlation between class labels and the locations of patholog-ically abnormal regions."
    },
    {
        "abstract": "  In the last decades, dispersal studies have benefited from the use of\nmolecular markers for detecting patterns differing between categories of\nindividuals and have highlighted sex-biased dispersal in several species. To\nexplain this phenomenon, several hypotheses implying mating systems,\nintrasexual competition or sex-related handicaps have been proposed. In this\ncontext, we investigated sex-biased dispersal in Armadillidium vulgare, a\nterrestrial isopod with a promiscuous mating system. As a proxy for effective\ndispersal, we performed a fine-scale investigation of the spatial genetic\nstructure in males and females, using individuals originating from five\nsampling points located within 70 meters of each other. Based on microsatellite\nmarkers and spatial autocorrelation analyses, our results revealed that while\nmales did not present a significant genetic structure at this geographic scale,\nfemales were significantly and genetically more similar to each other when they\nwere collected in the same sampling point. As females invest more parental care\nthan males in A. vulgare, but also because this species is promiscuous and\nmales experience a high intrasexual competition, our results meet the\npredictions of most classical hypotheses for sex-biased dispersal. We suggest\nthat widening dispersal studies to other isopods or crustaceans, differing in\ntheir ecology or mating system and displaying varying levels of parental care,\nmight shed light on the processes underlying the evolution of sex-biased\ndispersal.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* As a proxy for effective dispersal, we performed a fine-scale investigation of the spatial genetic structure in males and females, using individuals originating from five sampling points located within 70 meters of each other.\n* Based on microsatellite markers and spatial autocorrelation analyses, our results..."
    },
    {
        "abstract": "  [abridged] This work aims to observationally investigate the history of size\ngrowth of early-type galaxies and how the growth depends on cosmic epoch and\nthe mass of the halo in which they are embedded. We carried out a photometric\nand structural analysis in the rest-frame $V$ band of a mass-selected ($\\log\nM/M_\\odot >10.7$) sample of red-sequence early-type galaxies with\nspectroscopic/grism redshift in the general field up to $z=2$ to complement a\nprevious work presenting an identical analysis but in halos 100 times more\nmassive and 1000 times denser. We homogeneously derived sizes (effective radii)\nfully accounting for the multi-component nature of galaxies and the common\npresence of isophote twists and ellipticity gradients. By using these\nmass-selected samples, composed of 170 red-sequence early-type galaxies in the\ngeneral field and 224 identically selected and analyzed in clusters, we isolate\nthe effect on galaxy sizes of the halo in which galaxies are embedded and its\ndependence on epoch. We find that the $\\log$ of the galaxy size at a fixed\nstellar mass, $\\log M/M_\\odot= 11$, has increased with epoch at a rate twice as\nfast in the field than in cluster in the last 10 Gyr ($0.26\\pm0.03$ versus\n$0.13\\pm0.02$ dex per unit redshift). Red-sequence early-type galaxies in the\ngeneral field reached the size of their cousins in denser environment by\n$z=0.25\\pm0.13$ in spite of being three times smaller at $z\\sim2$. Data point\ntoward a model where size growth is epoch-independent (i.e., $\\partial \\log r_e\n/\\partial z = c$), but with a rate $c$ depending on environment, $\\partial c\n/\\partial \\log M_{halo} \\approx 0.05$. Environment determines the growth rate\n($d \\log r_e / dz$) at all redshifts, indicating an external origin for the\ngalaxy growth without any clear epoch where it ceases to have an effect.\n",
        "method": "We carried out a photometric and structural analysis in the rest-frame $V$ band of a mass-selected ($\\log M/M_\\odot >10.7$) sample of red-sequence early-type galaxies with spectroscopic/grism redshift in the general field up to $z=2$."
    },
    {
        "abstract": "  We consider evolution equations of the form \\begin{equation*}\\label{Abstract\nequation} \\dot u(t)+ A(t)u(t)=0,\\ \\ t\\in[0,T],\\ \\ u(0)=u_0, \\end{equation*}\nwhere $A(t),\\ t\\in [0,T],$ are associated with a non-autonomous sesquilinear\nform $\\mathfrak a(t,\\cdot,\\cdot)$ on a Hilbert space $H$ with constant domain\n$V\\subset H.$ In this note we continue the study of fundamental operator\ntheoretical properties of the solutions. We give a sufficient condition for\nnorm-continuity of evolution families on each spaces $V, H$ and on the dual\nspace $V'$ of $V.$ The abstract results are applied to a class of equations\ngoverned by time dependent Robin boundary conditions on exterior domains and by\nSchr\\\"odinger operator with time dependent potentials.\n",
        "method": "The methodological sentence is:\n\nWe give a sufficient condition for norm-continuity of evolution families on each space V, H and on the dual space V' of V."
    },
    {
        "abstract": "  We study the problem of static, spherically symmetric, self-gravitating\nelastic matter distributions in Newtonian gravity. To this purpose we first\nintroduce a new definition of homogeneous, spherically symmetric (hyper)elastic\nbody in Euler coordinates, i.e., in terms of matter fields defined on the\ncurrent physical state of the body. We show that our definition is equivalent\nto the classical one existing in the literature and which is given in\nLagrangian coordinates, i.e., in terms of the deformation of the body from a\ngiven reference state. After a number of well-known examples of constitutive\nfunctions of elastic bodies are re-defined in our new formulation, a detailed\nstudy of the Seth model is presented. For this type of material the existence\nof single and multi-body solutions is established.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We introduce a new definition of homogeneous, spherically symmetric (hyper)elastic body in Euler coordinates...\""
    },
    {
        "abstract": "  We use visibility graphs as a tool to analyse the results of kinetic Monte\nCarlo (kMC) simulations of submonolayer deposition in a one-dimensional point\nisland model. We introduce an efficient algorithm for the computation of the\nvisibility graph resulting from a kMC simulation and show that from the\nproperties of the visibility graph one can determine the critical island size,\nthus demonstrating that the visibility graph approach, which implicitly\ncombines size and spatial data, can provide insights into island nucleation and\ngrowth processes.\n",
        "method": "We use visibility graphs as a tool to analyse the results of kinetic Monte Carlo (kMC) simulations of submonolayer deposition in a one-dimensional point island model."
    },
    {
        "abstract": "  Temporal-Difference learning (TD) [Sutton, 1988] with function approximation\ncan converge to solutions that are worse than those obtained by Monte-Carlo\nregression, even in the simple case of on-policy evaluation. To increase our\nunderstanding of the problem, we investigate the issue of approximation errors\nin areas of sharp discontinuities of the value function being further\npropagated by bootstrap updates. We show empirical evidence of this leakage\npropagation, and show analytically that it must occur, in a simple Markov\nchain, when function approximation errors are present. For reversible policies,\nthe result can be interpreted as the tension between two terms of the loss\nfunction that TD minimises, as recently described by [Ollivier, 2018]. We show\nthat the upper bounds from [Tsitsiklis and Van Roy, 1997] hold, but they do not\nimply that leakage propagation occurs and under what conditions. Finally, we\ntest whether the problem could be mitigated with a better state representation,\nand whether it can be learned in an unsupervised manner, without rewards or\nprivileged information.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* \"We investigate the issue of approximation errors...\"\n* \"We show empirical evidence of this leakage propagation...\"\n* \"For reversible policies, we show that it must occur...\"\n* \"We test whether the problem could be mitigated with a better state representation...\""
    },
    {
        "abstract": "  The Internet of Things (IoT) is a crucial component of Industry 4.0. Due to\ngrowing demands of customers, the current IoT architecture will not be reliable\nand responsive for next generation IoT applications and upcoming services. In\nthis paper, the next generation IoT architecture based on new technologies is\nproposed in which the requirements of future applications, services, and\ngenerated data are addressed. Particularly, this architecture consists of\nNano-chip, millimeter Wave (mmWave), Heterogeneous Networks (HetNet),\ndevice-todevice (D2D) communication, 5G-IoT, Machine-Type Communication (MTC),\nWireless Network Function virtualization (WNFV), Wireless Software Defined\nNetworks (WSDN), Advanced Spectrum Sharing and Interference Management\n(Advanced SSIM), Mobile Edge Computing (MEC), Mobile Cloud Computing (MCC),\nData Analytics and Big Data. This combination of technologies is able to\nsatisfy requirements of new applications. The proposed novel architecture is\nmodular, efficient, agile, scalable, simple, and it is able to satisfy the high\namount of data and application demands.\n",
        "method": "This paper proposes a next-generation IoT architecture based on new technologies, which consists of: Nano-chip, millimeter Wave (mmWave), Heterogeneous Networks (HetNet), device-to-device (D2D) communication, 5G-IoT, Machine-Type Communication (MTC), Wireless Network Function virtualization (WNFV), Wireless Software Defined Networks (WSDN), Advanced Spectrum Sharing and Interference Management (Advanced SSIM), Mobile Edge Computing (MEC), Mobile Cloud Computing (MCC), Data Analytics, and Big Data."
    },
    {
        "abstract": "  The intricated combinatorial structure and the non-compactness of the Lorentz\ngroup have always made the computation of $SL(2,\\mathbb{C})$ EPRL spin foam\ntransition amplitudes a very hard and resource demanding task. With\n\\texttt{sl2cfoam} we provide a C-coded library for the evaluation of the\nLorentzian EPRL vertex amplitude. We provide a tool to compute the Lorentzian\nEPRL 4-simplex vertex amplitude in the intertwiner basis and some utilities to\nevaluate SU(2) invariants, booster functions and $SL(2,\\mathbb{C})$\nClebsch-Gordan coefficients. We discuss the data storage, parallelizations,\ntime, and memory performances and possible future developments.\n",
        "method": "Here is the methodological sentence:\n\nWe provide a C-coded library for the evaluation of the Lorentzian EPRL vertex amplitude."
    },
    {
        "abstract": "  Models of spontaneous wave function collapse predict a small heating rate for\na bulk solid, as a result of coupling to the noise field that causes collapse.\nThis rate is small enough that ambient radioactivity and cosmic ray flux on the\nsurface of the earth can mask the heating due to spontaneous collapse. In this\npaper we estimate the background noise due to gamma-radiation and cosmic ray\nmuon flux, at different depths. We demonstrate that a low-temperature\nunderground experiment at a depth of about 6.5 km.w.e. would have a low enough\nbackground to allow detection of bulk heating for a collapse rate $\\lambda$ of\n$10^{-16}$ s$^{-1}$ using presently available technology.\n",
        "method": "We estimate the background noise due to gamma-radiation and cosmic ray muon flux, at different depths."
    },
    {
        "abstract": "  We consider static spherically symmetric self-gravitating configurations of\nthe perfect fluid within the framework of the torsion-based extended theory of\ngravity. In particular, we use the covariant formulation of $f(T)$ gravity with\n$f(T) = T + \\frac{\\alpha}{2} T^2$, and for the fluid we assume the polytropic\nequation of state with the adiabatic exponent $\\Gamma = 2$. The constructed\nsolutions have a sharply defined radius [as in General Relativity (GR)] and can\nbe considered as models of nonrotating compact stars. The particle\nnumber--to--stellar radius curves reveal that with positive (negative) values\nof $\\alpha$ smaller (greater) number of particles can be supported against\ngravity then in GR. For the interpretation of the energy density and the\npressure within the star we adopt the GR picture where the effects due to\nnonlinearity of $f(T)$ are seen as a $f(T)$ fluid, which together with the\npolytropic fluid contributes to the effective energy momentum. We find that\nsufficiently large positive $\\alpha$ gives rise to an abrupt sign change (phase\ntransition) in the energy density and in the principal pressures of the $f(T)$\nfluid, taking place within the interior of the star. The corresponding radial\nprofile of the effective energy density is approximately constant over the\ncentral region of the star, mimicking an incompressible core. This interesting\nphenomenon is not found in configurations with negative $\\alpha$.\n",
        "method": "Here are the methodological sentences:\n\nWe use the covariant formulation of $f(T)$ gravity with $f(T) = T + \\frac{\\alpha}{2} T^2$, and for the fluid we assume the polytropic equation of state with the adiabatic exponent $\\Gamma = 2$."
    },
    {
        "abstract": "  Spintronic terahertz (THz) emitter provides the advantages such as apparently\nbroader spectrum, significantly lower cost, and more flexibility in compared\nwith the commercial THz emitters, and thus attracts great interests recently.\nIn past few years, efforts have been made in optimizing the material\ncomposition and structure geometry, and the conversion efficiency has been\nimproved close to that of ZnTe crystal. One of the drawbacks of the current\ndesigns is the rather limited laser absorption - more than 50% energy is wasted\nand the conversion efficiency is thus limited. Here, we theoretically propose\nand experimentally demonstrate a novel device that fully utilizes the laser\nintensity and significantly improves the conversion efficiency. The device,\nwhich consists of a metal-dielectric photonic crystal structure, utilizes the\ninterference between the multiple scattering waves to simultaneously suppress\nthe reflection and transmission of the laser, and to reshape the laser field\ndistributions. The experimentally detected laser absorption and THz generations\nshow one-to-one correspondence with the theoretical calculations. We achieve\nthe strongest THz pulse emission that presents a 1.7 times improvement compared\nto the currently designed spintronic emitter. This work opens a new pathway to\nimprove the performance of spintronic THz emitter from the perspective of\noptics.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We theoretically propose and experimentally demonstrate a novel device that fully utilizes the laser intensity and significantly improves the conversion efficiency.\""
    },
    {
        "abstract": "  Topologically non-trivial electronic structures can give rise to a range of\nunusual physical phenomena, and the interplay of band topology with other\neffects such as electronic correlations and magnetism requires further\nexploration. The rare earth monopnictides $X$(Sb,Bi) ($X$ = lanthanide) are a\nlarge family of semimetals where these different effects may be tuned by the\nsubstitution of rare-earth elements. Here we observe anomalous behavior in the\nquantum oscillations of one member of this family, antiferromagnetic SmSb. The\nanalysis of Shubnikov-de Haas (SdH) oscillations provides evidence for a\nnon-zero Berry phase, indicating a non-trivial topology of the $\\alpha$-band.\nFurthermore, striking differences are found between the temperature dependence\nof the amplitudes of de Haas-van Alphen effect oscillations, which are well\nfitted by the Lifshitz-Kosevich (LK) formula across the measured temperature\nrange, and those from SdH measurements which show a significant disagreement\nwith LK behavior at low temperatures. Our findings of unusual quantum\noscillations in an antiferromagnetic, mixed valence semimetal with a possible\nnon-trivial band topology can provide an opportunity for studying the interplay\nbetween topology, electronic correlations and magnetism.\n",
        "method": "Here is the methodological sentence:\n\nThe analysis of Shubnikov-de Haas (SdH) oscillations provides evidence for a non-zero Berry phase, indicating a non-trivial topology of the $\\alpha$-band."
    },
    {
        "abstract": "  Implications of additional fundamental scalar fields are regarded as among\nthe most important avenues to explore after the experimental discovery of the\nstandard model Higgs, particularly when there already exist cogent arguments in\nfavor of their existence. A peculiar observation in Higgs-scalar singlet system\nis tendency of scalar singlet field to have negative squared physical masses\nwhich may be a sign of either a tachyon field or symmetry breaking. Assuming\nthat this feature is due to the presence of a phenomenon similar to the\nconventionally understood Higgs mechanism, Wick Cutkosky model is studied in\nthe parameter space suggested by the classical ground state of the system at\nHiggs mass $125.09$ GeV with positive values for both squared bare masses. The\nresults are found to have strong negative contributions to squared scalar\nmasses. For higher cutoff values, the renormalized squared scalar masses and\nsquared bare couplings are found to qualitatively favor a relationship similar\nto the one in the classical ground state of the system, upto an additive\nconstant. Higgs propagators remain almost unaffected as observed in a\npreviously explored region of parameter space. However, scalar singlet\npropagators are found to have relatively different qualitative features in\ncomparison to the previous study of the model. Vertices are found to have\nqualitatively similar features. No sign of triviality is found.\n",
        "method": "Here is the methodological sentence:\n\nThe Wick Cutkosky model is studied in the parameter space suggested by the classical ground state of the system at Higgs mass $125.09$ GeV with positive values for both squared bare masses."
    },
    {
        "abstract": "  As the generalization of gravitational effects on the point mass systems, we\nwant to study the tidal effect exerted on an extended stellar system using\nspherical and axisymmetric elliptical models. Considering the Isochrone and\nPlummer models for a passing extended stellar system, the tidal distance and\nthe equipotential surface are calculated. The corresponding critical surfaces\nand maps are plotted in different cases. There are different results some of\nthem may be used in describing stellar systems deformation\n",
        "method": "The methodological sentence is:\n\nConsidering the Isochrone and Plummer models for a passing extended stellar system, the tidal distance and the equipotential surface are calculated."
    },
    {
        "abstract": "  We perform a systematic study of the correlation functions of two quark\ncurrents in a pion using lattice QCD. We obtain good signals for all but one of\nthe relevant Wick contractions of quark fields. We investigate the quark mass\ndependence of our results and test the importance of correlations between the\nquark and the antiquark in the pion. Our lattice data are compared with\npredictions from chiral perturbation theory.\n",
        "method": "Here is the methodological sentence:\n\nWe perform a systematic study of the correlation functions of two quark currents in a pion using lattice QCD."
    },
    {
        "abstract": "  We study sparse generalized inverses $H$ of a rank-$r$ real matrix $A$. We\ngive a construction for reflexive generalized inverses having at most $r^2$\nnonzeros. For $r=1$ and for $r=2$ with $A$ nonnegative, we demonstrate how to\nminimize the (vector) 1-norm over reflexive generalized inverses. For general\n$r$, we efficiently find reflexive generalized inverses with 1-norm within\napproximately a factor of $r^2$ of the minimum 1-norm generalized inverse.\n",
        "method": "We give a construction for reflexive generalized inverses having at most $r^2$ nonzeros."
    },
    {
        "abstract": "  Primordial Black Holes (PBHs) with a mass $M \\lesssim {10^{17}}$g are\nexpected to inject sub-GeV electrons and positrons in the Galaxy via Hawking\nradiation. These cosmic rays are shielded by the solar magnetic field for\nEarth-bound detectors, but not for Voyager-1, which is now beyond the\nheliopause. We use its data to constrain the fraction of PBHs to the dark\nmatter in the Galaxy, finding that PBHs with $M<10^{16}$g cannot contribute\nmore than 0.1% (or less for a lognormal mass distribution). Our limits are\nbased on local galactic measurements and are thus complementary to those\nderived from cosmological observations.\n",
        "method": "We use its data to constrain the fraction of PBHs to the dark matter in the Galaxy, finding that PBHs with $M<10^{16}$g cannot contribute more than 0.1% (or less for a lognormal mass distribution)."
    },
    {
        "abstract": "  A CR manifold $M$, with CR distribution $\\mathcal D^{10}\\subset T^\\mathbb C\nM$, is called {\\it totally nondegenerate of depth $\\mu$} if: (a) the complex\ntangent space $T^\\mathbb C M$ is generated by all complex vector fields that\nmight be determined by iterated Lie brackets between at most $\\mu$ fields in\n$\\mathcal D^{10} + \\overline{\\mathcal D^{10}}$; (b) for each integer $2 \\leq k\n\\leq \\mu-1$, the families of all vector fields that might be determined by\niterated Lie brackets between at most $k$ fields in $\\mathcal D^{10} +\n\\overline{\\mathcal D^{10}}$ generate regular complex distributions; (c) the\nranks of the distributions in (b) have the {\\it maximal values} that can be\nobtained amongst all CR manifolds of the same CR dimension and satisfying (a)\nand (b) -- this maximality property is the {\\it total nondegeneracy} condition.\nIn this paper, we prove that, for any Tanaka symbol $\\frak m = \\frak m^{-\\mu}+\n\\ldots + \\frak m^{-1}$ of a totally nondegenerate CR manifold of depth $\\mu\n\\geq 4$, the full Tanaka prolongation of $\\frak m$ has trivial subspaces of\ndegree $k \\geq 1$, i.e. it has the form $\\frak m^{-\\mu}+ \\ldots + \\frak m^{-1}\n+ \\frak g^0$. This result has various consequences. For instance it implies\nthat any (local) CR automorphism of a regular totally nondegenerate CR manifold\nis uniquely determined by its first order jet at a fixed point of the manifold.\nIt also gives a complete proof of a conjecture by Beloshapka on the group of\nautomorphisms of homogeneous totally nondegenerate CR manifolds.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe prove that..."
    },
    {
        "abstract": "  Using Monte Carlo simulations based on the Metropolis algorithm, we\ninvestigate the dynamic phase transition properties of kinetic Ising model\ndriven by a sinusoidally oscillating magnetic field in the presence of additive\nwhite noise. We calculate equilibrium and dynamic properties such as the\ntemperature dependence of average magnetization and magnetic specific heat, as\nwell as the period dependence of dynamic order parameter and scaled variance.\nAfter determining the critical period at which order-disorder transition takes\nplace, we perform finite size scaling analysis to extract the exponent ratios,\nand discuss the variation of these properties in the presence of noisy magnetic\nfield. As a general result, we show that for a noisy system, DPT does not fall\ninto a universality class of the conventional dynamic (and also equilibrium)\nuniversality class of the Ising model.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nUsing Monte Carlo simulations based on the Metropolis algorithm..."
    },
    {
        "abstract": "  Apache Spark is a Big Data framework for working on large distributed\ndatasets. Although widely used in the industry, it remains rather limited in\nthe academic community or often restricted to software engineers. The goal of\nthis paper is to show with practical uses-cases that the technology is mature\nenough to be used without excessive programming skills by astronomers or\ncosmologists in order to perform standard analyses over large datasets, as\nthose originating from future galaxy surveys. To demonstrate it, we start from\na realistic simulation corresponding to 10 years of LSST data taking (6\nbillions of galaxies). Then, we design, optimize and benchmark a set of Spark\npython algorithms in order to perform standard operations as adding photometric\nredshift errors, measuring the selection function or computing power spectra\nover tomographic bins. Most of the commands execute on the full 110 GB dataset\nwithin tens of seconds and can therefore be performed interactively in order to\ndesign full-scale cosmological analyses. A jupyter notebook summarizing the\nanalysis is available at https://github.com/astrolabsoftware/1807.03078.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Then, we design, optimize and benchmark a set of Spark python algorithms...\n* Most of the commands execute on the full 110 GB dataset within tens of seconds..."
    },
    {
        "abstract": "  We report first-principles phonon frequencies and anharmonic thermodynamic\nproperties of h.c.p. Os and Ru calculated within the quasi-harmonic\napproximation, including Gruneisen parameters, temperature-dependent lattice\nparameters, thermal expansion, and isobaric heat capacity. We discuss the\ndifferences between a full treatment of anisotropy and a simplified approach\nwith a constant c/a ratio. The results are systematically compared with the\navailable experimental data and an overall satisfactory agreement is obtained.\n",
        "method": "We report first-principles phonon frequencies and anharmonic thermodynamic properties of h.c.p. Os and Ru calculated within the quasi-harmonic approximation, including Gruneisen parameters, temperature-dependent lattice parameters, thermal expansion, and isobaric heat capacity."
    },
    {
        "abstract": "  We introduce the notion of noncommutative complex spheres with partial\ncommutation relations for the coordinates. We compute the corresponding quantum\nsymmetry groups of these spheres, and this yields new quantum unitary groups\nwith partial commutation relations. We also discuss some geometric aspects of\nthe quantum orthogonal groups associated with the mixture of classical and free\nindependence discovered by Speicher and Weber. We show that these quantum\ngroups are quantum symmetry groups on some quantum spaces of spherical vectors\nwith partial commutation relations.\n",
        "method": "We introduce the notion of noncommutative complex spheres with partial commutation relations for the coordinates."
    },
    {
        "abstract": "  We prove that for the mixed $q$-Gaussian algebra $\\Gamma_{Q}(H_{\\mathbb{R}})$\nassociated to a real Hilbert space $H_{\\mathbb{R}}$ and a real symmetric matrix\n$Q=(q_{ij})$ with $\\sup|q_{ij}|<1$, the generator subalgebra is singular.\n",
        "method": "We prove that for the mixed $q$-Gaussian algebra $\\Gamma_{Q}(H_{\\mathbb{R}})$ associated to a real Hilbert space $H_{\\mathbb{R}}$ and a real symmetric matrix $Q=(q_{ij})$ with $\\sup|q_{ij}|<1$, the generator subalgebra is singular."
    },
    {
        "abstract": "  We analyze $L^2$-normalized solutions of nonlinear Schr\\\"odinger systems of\nGross-Pitaevskii type, on bounded domains, with homogeneous Dirichlet boundary\nconditions. We provide sufficient conditions for the existence of orbitally\nstable standing waves. Such waves correspond to global minimizers of the\nassociated energy in the $L^2$-subcritical and critical cases, and to local\nones in the $L^2$-supercritical case. Notably, our study includes also the\nSobolev-critical case.\n",
        "method": "We analyze $L^2$-normalized solutions of nonlinear Schr\\\"odinger systems of Gross-Pitaevskii type, on bounded domains, with homogeneous Dirichlet boundary conditions."
    },
    {
        "abstract": "  Given a malfunctioning system, sequential diagnosis aims at identifying the\nroot cause of the failure in terms of abnormally behaving system components. As\ninitial system observations usually do not suffice to deterministically pin\ndown just one explanation of the system's misbehavior, additional system\nmeasurements can help to differentiate between possible explanations. The goal\nis to restrict the space of explanations until there is only one (highly\nprobable) explanation left. To achieve this with a minimal-cost set of\nmeasurements, various (active learning) heuristics for selecting the best next\nmeasurement have been proposed.\n  We report preliminary results of extensive ongoing experiments with a set of\nselection heuristics on real-world diagnosis cases. In particular, we try to\nanswer questions such as \"Is some heuristic always superior to all others?\",\n\"On which factors does the (relative) performance of the particular heuristics\ndepend?\" or \"Under which circumstances should I use which heuristic?\"\n",
        "method": "Here are the methodological sentences:\n\nAs initial system observations usually do not suffice to deterministically pin down just one explanation of the system's misbehavior, additional system measurements can help to differentiate between possible explanations.\n\nWe report preliminary results of extensive ongoing experiments with a set of selection heuristics on real-world diagnosis cases."
    },
    {
        "abstract": "  The metal-poor sub-population of globular cluster (GC) systems exhibits a\ncorrelation between the GC average colour and luminosity, especially in those\nsystems associated with massive elliptical galaxies. More luminous (more\nmassive) GCs are typically redder and hence more metal-rich. This 'blue tilt'\nis often interpreted as a mass-metallicity relation stemming from GC\nself-enrichment, whereby more massive GCs retain a greater fraction of the\nenriched gas ejected by their evolving stars, fostering the formation of more\nmetal-rich secondary generations. We examine the E-MOSAICS simulations of the\nformation and evolution of galaxies and their GC populations, and find that\ntheir GCs exhibit a colour-luminosity relation similar to that observed in\nlocal galaxies, without the need to invoke mass-dependent self-enrichment. We\nfind that the blue tilt is most appropriately interpreted as a dearth of\nmassive, metal-poor GCs: the formation of massive GCs requires high\ninterstellar gas surface densities, conditions that are most commonly fostered\nby the most massive, and hence most metal rich, galaxies, at the peak epoch of\nGC formation. The blue tilt is therefore a consequence of the intimate coupling\nbetween the small-scale physics of GC formation and the evolving properties of\ninterstellar gas hosted by hierarchically-assembling galaxies.\n",
        "method": "We examine the E-MSOICS simulations of the formation and evolution of galaxies and their GC populations, and find that their GCs exhibit a colour-luminosity relation similar to that observed in local galaxies, without the need to invoke mass-dependent self-enrichment."
    },
    {
        "abstract": "  We report on an optical (SDSS) and X-ray (XMM) study of an optically selected\nsample of four dual AGN systems at projected separations of 30--60~kpc. All\nsources are detected in the X-ray band (0.3-10 keV); seven objects are\noptically identified as Seyfert, while one source, optically classified as a\nLINER, is likely powered by accretion in virtue of its relatively high X-ray\nluminosity (1.2$\\times10^{41}$ erg/s). Six of the eight objects are obscured in\nX-rays with N$_{\\rm H} \\geq$ 10$^{23}$ cm$^{-2}$; three of these, whose X-ray\nspectrum is dominated by a reflection component, are likely Compton-thick\n(N$_{\\rm H} \\geq$ 10$^{24}$ cm$^{-2}$). This finding is in agreement with the\nhypothesis that galaxy encounters are effective in driving gas inflow toward\nthe nuclear region, thus increasing the obscuration. We compare the absorption\nproperties in our dual AGN with those in larger samples observed in X-rays but\nselected in different ways (optical, IR and hard X-rays). We find that the\nobscured (N$_{\\rm H} \\geq$ 10$^{22}$ cm$^{-2}$) AGN fraction within the larger\nsample is 84$\\pm$4 per cent (taking into account the 90 per cent error on the\nN$_{\\rm H}$ measure) up to large pair separations ($\\sim$100~kpc). This is\nstatistically higher than the fraction of obscured AGN in isolated galaxies\nfound in X-ray surveys.\n",
        "method": "We report on an optical (SDSS) and X-ray (XMM) study of an optically selected sample of four dual AGN systems at projected separations of 30--60~kpc."
    },
    {
        "abstract": "  We study the L-infinity-formality problem for the Hochschild complex of the\nuniversal enveloping algebra of some examples of Lie algebras such as\nCartan-3-regular quadratic Lie algebras (for example semisimple Lie algebras\nand in more detail so(3)), and free Lie algebras generated by a vector space of\ndimension at least 2. We show that for these examples formality in Kontsevich's\nsense does NOT hold, although some of them allow unconditioned deformability.\nWe compute the L-infinity-structure on the cohomology given by homotopy\ntransfer in certain cases.\n",
        "method": "Here is the methodological sentence:\n\nWe study the L-infinity-formality problem for the Hochschild complex of the universal enveloping algebra..."
    },
    {
        "abstract": "  A suite of synthetic nuclear diagnostics has been developed to post-process\nradiation hydrodynamics simulations performed with the code Chimera. These\nprovide experimental observables based on simulated capsule properties and are\nused to assess alternative experimental and data analysis techniques. These\ndiagnostics include neutron spectroscopy, primary and scattered neutron\nimaging, neutron activation, $\\gamma$-ray time histories and carbon\n$\\gamma$-ray imaging. Novel features of the neutron spectrum have been analysed\nto infer plasma parameters. The nT and nD backscatter edges have been shown to\nprovide a shell velocity measurement. Areal density asymmetries created by low\nmode perturbations have been inferred from the slope of the downscatter\nspectrum down to 10 MeV. Neutron activation diagnostics showed significant\naliasing of high mode areal density asymmetries when observing a capsule\nimplosion with 3D multimode perturbations applied. Carbon $\\gamma$-ray imaging\ncould be used to image the ablator at high convergence ratio. Time histories of\nboth the fusion and carbon $\\gamma$ signals showed a greater time difference\nbetween peak intensities for the perturbed case when compared to a symmetric\nsimulation.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* These provide experimental observables based on simulated capsule properties and are used to assess alternative experimental and data analysis techniques.\n* Novel features of the neutron spectrum have been analysed to infer plasma parameters.\n* Neutron activation diagnostics showed significant aliasing of high mode areal density asymmetries when observing a capsule implosion with 3D multimode perturbations applied."
    },
    {
        "abstract": "  New generalized cyclotomic binary sequences of period $p^2$ are proposed in\nthis paper, where $p$ is an odd prime. The sequences are almost balanced and\ntheir linear complexity is determined. The result shows that the proposed\nsequences have very large linear complexity if $p$ is a non-Wieferich prime.\n",
        "method": "The sequences are generated by using a combination of cyclotomic cosets and the Reed-Solomon code."
    },
    {
        "abstract": "  Most existing video summarisation methods are based on either supervised or\nunsupervised learning. In this paper, we propose a reinforcement learning-based\nweakly supervised method that exploits easy-to-obtain, video-level category\nlabels and encourages summaries to contain category-related information and\nmaintain category recognisability. Specifically, We formulate video\nsummarisation as a sequential decision-making process and train a summarisation\nnetwork with deep Q-learning (DQSN). A companion classification network is also\ntrained to provide rewards for training the DQSN. With the classification\nnetwork, we develop a global recognisability reward based on the classification\nresult. Critically, a novel dense ranking-based reward is also proposed in\norder to cope with the temporally delayed and sparse reward problems for long\nsequence reinforcement learning. Extensive experiments on two benchmark\ndatasets show that the proposed approach achieves state-of-the-art performance.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Specifically, We formulate video summarisation as a sequential decision-making process...\n* A companion classification network is also trained to provide rewards for training the DQSN.\n* With the classification network, we develop a global recognisability reward based on the classification result.\n* Critically, a novel dense ranking-based reward is also proposed in order to cope with the temporally delayed and sparse reward problems for long sequence reinforcement learning."
    },
    {
        "abstract": "  Structure learning methods for covariance and concentration graphs are often\nvalidated on synthetic models, usually obtained by randomly generating: (i) an\nundirected graph, and (ii) a compatible symmetric positive definite (SPD)\nmatrix. In order to ensure positive definiteness in (ii), a dominant diagonal\nis usually imposed. However, the link strengths in the resulting graphical\nmodel, determined by off-diagonal entries in the SPD matrix, are in many\nscenarios extremely weak. Recovering the structure of the undirected graph thus\nbecomes a challenge, and algorithm validation is notably affected. In this\npaper, we propose an alternative method which overcomes such problem yet\nyielding a compatible SPD matrix. We generate a partially row-wise-orthogonal\nmatrix factor, where pairwise orthogonal rows correspond to missing edges in\nthe undirected graph. In numerical experiments ranging from moderately dense to\nsparse scenarios, we obtain that, as the dimension increases, the link strength\nwe simulate is stable with respect to the structure sparsity. Importantly, we\nshow in a real validation setting how structure recovery is greatly improved\nfor all learning algorithms when using our proposed method, thereby producing a\nmore realistic comparison framework.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n(i) an undirected graph, and (ii) a compatible symmetric positive definite (SPD) matrix.\n\nWe generate a partially row-wise-orthogonal matrix factor, where pairwise orthogonal rows correspond to missing edges in the undirected graph."
    },
    {
        "abstract": "  We consider the problem of locating a point-source heart arrhythmia using\ndata from a standard diagnostic procedure, where a reference catheter is placed\nin the heart, and arrival times from a second diagnostic catheter are recorded\nas the diagnostic catheter moves around within the heart. We model this\nsituation as a nonconvex feasibility problem, where given a set of arrival\ntimes, we look for a source location that is consistent with the available\ndata. We develop a new optimization approach and fast algorithm to obtain\nonline proposals for the next location to suggest to the operator as she\ncollects data. We validate the procedure using a Monte Carlo simulation based\non patients' electrophysiological data. The proposed procedure robustly and\nquickly locates the source of arrhythmias without any prior knowledge of heart\nanatomy.\n",
        "method": "Here is the methodological sentence:\n\nWe model this situation as a nonconvex feasibility problem, where given a set of arrival times, we look for a source location that is consistent with the available data."
    },
    {
        "abstract": "  A new quantum spin liquid (QSL) candidate material H$_{3}$LiIr$_{2}$O$_{6}$\nwas synthesized recently and was found not to show any magnetic order or phase\ntransition down to low temperatures. In this work, we study the quantum\ndynamics of the hydrogen ions, i.e., protons, in this material by combining\nfirst-principles calculations and theoretical analysis. We show that each\nproton and its adjacent oxygen ions form an electric dipole. The dipole\ninteractions and the proton tunneling are captured by a transverse-field Ising\nmodel with a quantum disordered paraelectric ground state. The dipole\nexcitations have an energy gap $\\Delta_{\\mathrm{d}}\\simeq 60$ meV, and can be\nprobed by the infrared optical spectroscopy and the dielectric response. We\nargue that the electric dipole fluctuations renormalize the magnetic\ninteractions in H$_{3}$LiIr$_{2}$O$_{6}$ and lead to a Kitaev QSL state.\n",
        "method": "Here is the methodological sentence:\n\nWe show that each proton and its adjacent oxygen ions form an electric dipole."
    },
    {
        "abstract": "  Cooperation is a major factor in the evolution of human societies. The\nstructure of human social networks, which affects the dynamics of cooperation\nand other interpersonal phenomena, have common structural signatures. One of\nthese signatures is the tendency to organize as groups. Among the generative\nmodels that network theorists use to emulate this feature is the Stochastic\nBlock Model (SBM). In this paper, we study evolutionary game dynamics on SBM\nnetworks. Using a recently-discovered duality between evolutionary games and\ncoalescing random walks, we obtain analytical conditions such that natural\nselection favors cooperation over defection. We calculate the transition point\nfor each community to favor cooperation. We find that a critical\ninter-community link creation probability exists for given group density, such\nthat the overall network supports cooperation even if individual communities\ninhibit it. As a byproduct, we present mean-field solutions for the critical\nbenefit-to-cost ratio which perform with remarkable accuracy for diverse\ngenerative network models, including those with community structure and\nheavy-tailed degree distributions. We also demonstrate the generalizability of\nthe results to arbitrary two-player games.\n",
        "method": "Here is the methodological sentence:\n\nUsing a recently-discovered duality between evolutionary games and coalescing random walks, we obtain analytical conditions such that natural selection favors cooperation over defection."
    },
    {
        "abstract": "  The seen birds twitter, the running cars accompany with noise, etc. These\nnaturally audiovisual correspondences provide the possibilities to explore and\nunderstand the outside world. However, the mixed multiple objects and sounds\nmake it intractable to perform efficient matching in the unconstrained\nenvironment. To settle this problem, we propose to adequately excavate audio\nand visual components and perform elaborate correspondence learning among them.\nConcretely, a novel unsupervised audiovisual learning model is proposed, named\nas \\Deep Multimodal Clustering (DMC), that synchronously performs sets of\nclustering with multimodal vectors of convolutional maps in different shared\nspaces for capturing multiple audiovisual correspondences. And such integrated\nmultimodal clustering network can be effectively trained with max-margin loss\nin the end-to-end fashion. Amounts of experiments in feature evaluation and\naudiovisual tasks are performed. The results demonstrate that DMC can learn\neffective unimodal representation, with which the classifier can even\noutperform human performance. Further, DMC shows noticeable performance in\nsound localization, multisource detection, and audiovisual understanding.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nConcretely, a novel unsupervised audiovisual learning model is proposed, named as  Deep Multimodal Clustering (DMC), that synchronously performs sets of clustering with multimodal vectors of convolutional maps in different shared spaces for capturing multiple audiovisual correspondences."
    },
    {
        "abstract": "  Applying deep learning methods to mammography assessment has remained a\nchallenging topic. Dense noise with sparse expressions, mega-pixel raw data\nresolution, lack of diverse examples have all been factors affecting\nperformance. The lack of pixel-level ground truths have especially limited\nsegmentation methods in pushing beyond approximately bounding regions. We\npropose a classification approach grounded in high performance tissue\nassessment as an alternative to all-in-one localization and assessment models\nthat is also capable of pinpointing the causal pixels. First, the objective of\nthe mammography assessment task is formalized in the context of local tissue\nclassifiers. Then, the accuracy of a convolutional neural net is evaluated on\nclassifying patches of tissue with suspicious findings at varying scales, where\nhighest obtained AUC is above $0.9$. The local evaluations of one such expert\ntissue classifier is used to augment the results of a heatmap regression model\nand additionally recover the exact causal regions at high resolution as a\nsaliency image suitable for clinical settings.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* First, the objective of the mammography assessment task is formalized in the context of local tissue classifiers.\n* Then, the accuracy of a convolutional neural net is evaluated on classifying patches of tissue with suspicious findings at varying scales, where highest obtained AUC is above $0.9$."
    },
    {
        "abstract": "  We present NMT-Keras, a flexible toolkit for training deep learning models,\nwhich puts a particular emphasis on the development of advanced applications of\nneural machine translation systems, such as interactive-predictive translation\nprotocols and long-term adaptation of the translation system via continuous\nlearning. NMT-Keras is based on an extended version of the popular Keras\nlibrary, and it runs on Theano and Tensorflow. State-of-the-art neural machine\ntranslation models are deployed and used following the high-level framework\nprovided by Keras. Given its high modularity and flexibility, it also has been\nextended to tackle different problems, such as image and video captioning,\nsentence classification and visual question answering.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"State-of-the-art neural machine translation models are deployed and used following the high-level framework provided by Keras.\""
    },
    {
        "abstract": "  We present a new approach to three-dimensional electromagnetic scattering\nproblems via fast isogeometric boundary element methods. Starting with an\ninvestigation of the theoretical setting around the electric field integral\nequation within the isogeometric framework, we show existence, uniqueness, and\nquasi-optimality of the isogeometric approach. For a fast and efficient\ncomputation, we then introduce and analyze an interpolation-based fast\nmultipole method tailored to the isogeometric setting, which admits competitive\nalgorithmic and complexity properties. This is followed by a series of\nnumerical examples of industrial scope, together with a detailed presentation\nand interpretation of the results.\n",
        "method": "Starting with an investigation of the theoretical setting around the electric field integral equation within the isogeometric framework, we show existence, uniqueness, and quasi-optimality of the isogeometric approach."
    },
    {
        "abstract": "  These lectures cover aspects of primordial cosmology with a focus on\nobservational tests of physics beyond the Standard Model. The presentation is\ndivided into two parts: In Part I, we study the production of new light\nparticles in the hot big bang and describe their effects on the anisotropies of\nthe cosmic microwave background. In Part II, we investigate the possibility of\nvery massive particles being created during inflation and determine their\nimprints in higher-order cosmological correlations.\n",
        "method": "Here is the methodological sentence:\n\nWe study the production of new light particles in the hot big bang and describe their effects on the anisotropies of the cosmic microwave background."
    },
    {
        "abstract": "  Driven by the Internet of Things vision, recent years have seen the rise of\nnew horizons for the wireless ecosystem in which a very large number of mobile\nlow power devices interact to run sophisticated applications. The main\nhindrance to the massive deployment of low power nodes is most probably the\nprohibitive maintenance cost of battery replacement and the ecotoxicity of the\nbattery production/end-of-life. An emerging research direction to avoid battery\nreplacement is the combination of radio frequency energy harvesting and mobile\ncomputing (MC). In this paper, we propose the use of simultaneous information\nand power transfer (SWIPT) to control the distributed computation process while\ndelivering power to perform the computation tasks requested. A real-time MC\nsystem is considered, meaning that the trade-off between the information rate\nand the energy harvested must be carefully chosen to guarantee that the CPU may\nperform tasks of given complexity before receiving a new control signal. In\norder to provide a system-level perspective on the performance of SWIPT-MC\nnetworks, we propose a mathematical framework based on stochastic geometry to\ncharacterise the rate-energy trade-off of the system. The resulting achievable\nperformance region is then put in relation with the CPU energy consumption to\ninvestigate the operating conditions of real-time computing systems. Finally,\nnumerical results illustrate the joint effect of the network densification and\nthe propagation environment on the optimisation of the CPU usage.\n",
        "method": "The methodological sentences are:\n\n* In this paper, we propose the use of simultaneous information and power transfer (SWIPT) to control the distributed computation process while delivering power to perform the computation tasks requested.\n* We propose a mathematical framework based on stochastic geometry to characterise the rate-energy trade-off of the system."
    },
    {
        "abstract": "  We consider the problem of neural semantic parsing, which translates natural\nlanguage questions into executable SQL queries. We introduce a new mechanism,\nexecution guidance, to leverage the semantics of SQL. It detects and excludes\nfaulty programs during the decoding procedure by conditioning on the execution\nof partially generated program. The mechanism can be used with any\nautoregressive generative model, which we demonstrate on four state-of-the-art\nrecurrent or template-based semantic parsing models. We demonstrate that\nexecution guidance universally improves model performance on various\ntext-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS,\nand GeoQuery. As a result, we achieve new state-of-the-art execution accuracy\nof 83.8% on WikiSQL.\n",
        "method": "We demonstrate that execution guidance universally improves model performance on various text-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS, and GeoQuery."
    },
    {
        "abstract": "  The discovery of doubly heavy baryon provides us with a new platform for\nprecisely testing Standard Model and searching for new physics. As a\ncontinuation of our previous works, we investigate the FCNC processes of doubly\nheavy baryons. Light-front approach is adopted to extract the form factors, in\nwhich the two spectator quarks are viewed as a diquark. Results for form\nfactors are then used to predict some phenomenological observables, such as the\ndecay width and the forward-backward asymmetry. We find that most of the\nbranching ratios for $b\\to s$ processes are $10^{-8}\\sim10^{-7}$ and those for\n$b\\to d$ processes are $10^{-9}\\sim10^{-8}$. The flavor SU(3) symmetry and\nsymmetry breaking effects are explored. Parametric uncertainties are also\ninvestigated.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Light-front approach is adopted to extract the form factors, in which the two spectator quarks are viewed as a diquark.\""
    },
    {
        "abstract": "  We use the one-dimensional object-oriented particle-in-cell Monte Carlo\ncollision code oopd1 to explore the influence of the surface quenching of the\nsinglet delta metastable molecule O$_2$(a$^1\\Delta_{\\rm g}$) on the electron\nheating mechanism, and the electron energy probability function (EEPF), in a\nsingle frequency capacitively coupled oxygen discharge. When operating at low\npressure (10 mTorr) varying the surface quenching coefficient in the range\n0.00001 -- 0.1 has no influence on the electron heating mechanism and electron\nheating is dominated by drift-ambipolar (DA) heating in the plasma bulk and\nelectron cooling is observed in the sheath regions. As the pressure is\nincreased to 25 mTorr the electron heating becomes a combination of DA-mode and\n$\\alpha-$mode heating, and the role of the DA-mode decreases with decreasing\nsurface quenching coefficient. At 50 mTorr electron heating in the sheath\nregion dominates. However, for the highest quenching coefficient there is some\ncontribution from the DA-mode in the plasma bulk, but this contribution\ndecreases to almost zero and pure $\\alpha-$mode electron heating is observed\nfor a surface quenching coefficient of 0.001 or smaller.\n",
        "method": "We use the one-dimensional object-oriented particle-in-cell Monte Carlo collision code oopd1 to explore..."
    },
    {
        "abstract": "  CPU is undoubtedly the most important resource of the computer system. Recent\nadvances in software and system architecture have increased processing\ncomplexity, as computing is now distributed and parallel. CloudSim represents\nthe complexity of an application in terms of its computational requirements.\nCloudSim [9] is a complete solution for simulating Cloud Computing environments\nand building test beds for provisioning algorithms. This paper analyzes and\nevaluates the performance of cloud environment modeling using CloudSim. We\ndescribe the CloudSim architecture and then investigate the new models and\ntechniques in CloudSim.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe describe the CloudSim architecture and then investigate the new models and techniques in CloudSim."
    },
    {
        "abstract": "  A processor's memory hierarchy has a major impact on the performance of\nrunning code. However, computing platforms, where the actual hardware\ncharacteristics are hidden from both the end user and the tools that mediate\nexecution, such as a compiler, a JIT and a runtime system, are used more and\nmore, for example, performing large scale computation in cloud and cluster.\nEven worse, in such environments, a single computation may use a collection of\nprocessors with dissimilar characteristics. Ignorance of the\nperformance-critical parameters of the underlying system makes it difficult to\nimprove performance by optimizing the code or adjusting runtime-system\nbehaviors; it also makes application performance harder to understand.\n  To address this problem, we have developed a suite of portable tools that can\nefficiently derive many of the parameters of processor memory hierarchies, such\nas levels, effective capacity and latency of caches and TLBs, in a matter of\nseconds. The tools use a series of carefully considered experiments to produce\nand analyze cache response curves automatically. The tools are inexpensive\nenough to be used in a variety of contexts that may include install time,\ncompile time or runtime adaption, or performance understanding tools.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe tools use a series of carefully considered experiments to produce and analyze cache response curves automatically."
    },
    {
        "abstract": "  The measurement of the $K^{\\pm}$ production in the Semi-Inclusive Deep\nInelastic Scattering (SIDIS) can provide further knowledge about the structure\nof nucleon, and thus it is purposed in the Solenoidal Large Intensity\nDevice(SoLID) at Jefferson Lab(JLab). In this experiment, the identification of\nthe kaons is planed to be accomplished with the Multi-gap Resistive Plate\nChambers(MRPC), and the requirement for the time resolution is around 20 $\\rm\nps$. This is very challenging for the present MRPC systems (typical resolution\n60 $\\rm ps$), while in this paper, it is proved that the performance can be\nimproved largely if the signal waveform is obtained and analyzed with a neural\nnetwork method. In a cosmic ray experiment, the time resolution of a 6-gap\n0.25$\\rm mm$-thick MRPC reaches 36 $\\rm ps$ with this method, and a even better\nperformance is expected with a thinner MRPC.\n",
        "method": "In this experiment, the identification of the kaons is planed to be accomplished with the Multi-gap Resistive Plate Chambers (MRPC), and the requirement for the time resolution is around 20 ps."
    },
    {
        "abstract": "  As inelastic structures are ubiquitous in many engineering fields, a central\ntask in computational mechanics is to develop accurate, robust and efficient\ntools for their analysis. Motivated by the poor performances exhibited by\nstandard displacement-based finite element formulations, attention is here\nfocused on the use of mixed methods as approximation technique, within the\nsmall strain framework, for the mechanical problem of inelastic bidimensional\nstructures. Despite a great flexibility characterizes mixed element\nformulations, several theoretical and numerical aspects have to be carefully\ntaken into account in the design of a high-performance element. The present\nwork aims at providing the basis for methodological analysis and comparison in\nsuch aspects, within the unified mathematical setting supplied by generalized\nstandard material model and with special interest towards elastoplastic media.\nA critical review of the state-of-the-art computational methods is delivered in\nregard to variational formulations, selection of interpolation spaces,\nnumerical solution strategies and numerical stability. Though those arguments\nare interrelated, a topic-oriented presentation is resorted to, for the very\nrich available literature to be properly examined. Finally, the performances of\nseveral significant mixed finite element formulations are investigated in\nnumerical simulations.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"A critical review of the state-of-the-art computational methods is delivered in regard to variational formulations, selection of interpolation spaces, numerical solution strategies and numerical stability.\""
    },
    {
        "abstract": "  Scaling transformations involving a small parameter ({\\em degenerate\nscalings}) are frequently used for ordinary differential equations that model\n(bio-) chemical reaction networks. They are motivated by quasi-steady state\n(QSS) of certain chemical species, and ideally lead to slow-fast systems for\nsingular perturbation reductions, in the sense of Tikhonov and Fenichel. In the\npresent paper we discuss properties of such scaling transformations, with\nregard to their applicability as well as to their determination.\nTransformations of this type are admissible only when certain consistency\nconditions are satisfied, and they lead to singular perturbation scenarios only\nif additional conditions hold, including a further consistency condition on\ninitial values. Given these consistency conditions, two scenarios occur. The\nfirst (which we call standard) is well known and corresponds to a classical\nquasi-steady state (QSS) reduction. Here, scaling may actually be omitted\nbecause there exists a singular perturbation reduction for the unscaled system,\nwith a coordinate subspace as critical manifold. For the second (nonstandard)\nscenario scaling is crucial. Here one may obtain a singular perturbation\nreduction with the slow manifold having dimension greater than expected from\nthe scaling. For parameter dependent systems we consider the problem to find\nall possible scalings, and we show that requiring the consistency conditions\nallows their determination. This lays the groundwork for algorithmic\napproaches, to be taken up in future work. In the final section we consider\nsome applications. In particular we discuss relevant nonstandard reductions of\ncertain reaction-transport systems.\n",
        "method": "Transformations of this type are admissible only when certain consistency conditions are satisfied, and they lead to singular perturbation scenarios only if additional conditions hold, including a further consistency condition on initial values."
    },
    {
        "abstract": "  In this paper we present a system based on SVM ensembles trained on\ncharacters and words to discriminate between five similar languages of the\nIndo-Aryan family: Hindi, Braj Bhasha, Awadhi, Bhojpuri, and Magahi. We\ninvestigate the performance of individual features and combine the output of\nsingle classifiers to maximize performance. The system competed in the\nIndo-Aryan Language Identification (ILI) shared task organized within the\nVarDial Evaluation Campaign 2018. Our best entry in the competition, named\nILIdentification, scored 88:95% F1 score and it was ranked 3rd out of 8 teams.\n",
        "method": "We present a system based on SVM ensembles trained on characters and words to discriminate between five similar languages of the Indo-Aryan family: Hindi, Braj Bhasha, Awadhi, Bhojpuri, and Magahi."
    },
    {
        "abstract": "  A common approach for performing sparse tensor recovery is to use an N-mode\nFISTA method. However, this approach may fail in some cases by missing some\nvalues in the true support of the tensor and compensating by erroneously\nassigning nearby values to the support. This work proposes a four-stage method\nfor performing sparse tensor reconstruction that addresses a case where N-mode\nFISTA may fail by augmenting the support set. Moreover, the proposed method\npreserves a Tucker-like structure throughout computations for computational\nefficiency. Numerical results on synthetic data demonstrate that the proposed\nmethod produces results with similar or higher accuracy than N-mode FISTA, and\nis often faster.\n",
        "method": "Here is the extracted methodological sentence:\n\nThis work proposes a four-stage method for performing sparse tensor reconstruction..."
    },
    {
        "abstract": "  Internet-of-Things (IoT) and Supply Chain monitoring applications rely on\nmessaging protocols for exchanging data. Contemporary IoT deployments widely\nuse the publish-subscribe messaging model because of its resource-efficiency.\nHowever, the systems with publish-subscribe messaging model employ a\ncentralized architecture, wherein the data from all the devices in the\napplication network flows via a central broker to the subscribers. Such a\ncentralized architecture make publish-subscribe messaging model susceptible to\na central point of failure. Besides, it provides an opportunity for the\norganization that owns the broker to tamper with the data. In this work, we\ncontribute Trinity, a novel distributed publish-subscribe broker with\nblockchain-based immutability. Trinity distributes the data published to one of\nthe brokers in the network to all the brokers in the network. The distributed\ndata is stored in an immutable ledger through the use of the blockchain\ntechnology. Furthermore, Trinity executes smart contracts to validate the data\nbefore saving the data on the blockchain. Through the use of a blockchain\nnetwork, Trinity can guarantee persistence, ordering, and immutability across\ntrust boundaries. Our evaluation results show that Trinity consumes minimal\nresources, and the use of smart contracts enable the stakeholders to automate\nthe data management processes. To the best of our knowledge, Trinity is the\nfirst framework that combines the components of the blockchain technology with\nthe publish-subscribe messaging model.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Trinity distributes the data published to one of the brokers in the network to all the brokers in the network.\n* The distributed data is stored in an immutable ledger through the use of the blockchain technology.\n* Furthermore, Trinity executes smart contracts to validate the data before saving the data on the blockchain."
    },
    {
        "abstract": "  The European adoption of smart electricity meters triggers the developments\nof new value-added service for smart energy and optimal consumption. Recently,\nseveral algorithms and tools have been built to analyze smart meter's data.\nThis paper introduces an open framework and prototypes for detecting and\npresenting user behavior from its smart meter power consumption data. The\nframework aims at presenting the detected user behavior in natural language\nreports. In order to validate the proposed framework, an experiment has been\nperformed and the results have been presented.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"In order to validate the proposed framework, an experiment has been performed and the results have been presented.\""
    },
    {
        "abstract": "  In this work, system monitoring and analysis are discussed in terms of their\nsignificance and benefits for operations and research in the field of\nhigh-performance computing (HPC). HPC systems deliver unique insights to\ncomputational scientists from different disciplines. It is argued that research\nin HPC is also computational in nature, given the massive amounts of monitoring\ndata collected at various levels of an HPC system. The vision of a\ncomprehensive system model developed based on holistic monitoring and analysis\nis also presented. The goal and expected outcome of such a model is an improved\nunderstanding of the intricate interactions between today's software and\nhardware, and their diverse usage patterns. The associated modeling,\nmonitoring, and analysis challenges are reviewed and discussed. The envisioned\ncomprehensive system model will provide the ability to design future systems\nthat are better understood before use, easier to maintain and monitor, more\nefficient, more reliable, and, therefore, more productive. The paper is\nconcluded with a number of recommendations towards realizing the envisioned\nsystem model.\n",
        "method": "The vision of a comprehensive system model developed based on holistic monitoring and analysis is also presented."
    },
    {
        "abstract": "  Empirical evidence suggests that heavy-tailed degree distributions occurring\nin many real networks are well-approximated by power laws with exponents $\\eta$\nthat may take values either less than and greater than two. Models based on\nvarious forms of exchangeability are able to capture power laws with $\\eta <\n2$, and admit tractable inference algorithms; we draw on previous results to\nshow that $\\eta > 2$ cannot be generated by the forms of exchangeability used\nin existing random graph models. Preferential attachment models generate power\nlaw exponents greater than two, but have been of limited use as statistical\nmodels due to the inherent difficulty of performing inference in\nnon-exchangeable models. Motivated by this gap, we design and implement\ninference algorithms for a recently proposed class of models that generates\n$\\eta$ of all possible values. We show that although they are not exchangeable,\nthese models have probabilistic structure amenable to inference. Our methods\nmake a large class of previously intractable models useful for statistical\ninference.\n",
        "method": "Here is the methodological sentence:\n\nWe design and implement inference algorithms for a recently proposed class of models that generates $\\eta$ of all possible values."
    },
    {
        "abstract": "  This paper is concerned with the estimation of the number of negative\neigenvalues (bound states) of Schroedinger operators in a strip subject to\nNeumann boundary conditions. The estimates involve weighted L^1 norms and L ln\nL norms of the potential. Estimates involving the norms of the potential\nsupported by a curve embedded in the strip are also presented.\n",
        "method": "The estimates involve weighted L^1 norms and L ln L norms of the potential."
    },
    {
        "abstract": "  We give a simple proof of the moment-indeterminacy of the sequence $(n!)^t$\nfor $t > 2,$ using Lin's condition. Under a logarithmic self-decomposability\nassumption, the method conveys to power sequences defined as the rising\nfactorials of a given Bernstein function, and to more general infinitely\ndivisible moment sequences. We also provide a very short proof of the infinite\ndivisibility of all the moment sequences recently investigated in Lin (2017),\nincluding Fuss-Catalan's.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nUsing Lin's condition."
    },
    {
        "abstract": "  Recent Progress has shown that exploitation of hidden layer neurons in\nconvolution neural networks incorporating with a carefully designed activation\nfunction can yield better classification results in the field of computer\nvision. The paper firstly introduces a novel deep learning architecture aiming\nto mitigate the gradient-vanishing problem, in which the earlier hidden layer\nneurons could be directly connected with the last hidden layer and feed into\nthe last layer for classification. We then design a generalized linear\nrectifier function as the activation function that can approximate arbitrary\ncomplex functions via training of the parameters. We will show that our design\ncan achieve similar performance in a number of object recognition and video\naction benchmark tasks, under significantly less number of parameters and\nshallower network infrastructure, which is not only promising in training in\nterms of computation burden and memory usage, but is also applicable to\nlow-computation, low-memory mobile scenarios.\n",
        "method": "The paper firstly introduces a novel deep learning architecture aiming to mitigate the gradient-vanishing problem, in which the earlier hidden layer neurons could be directly connected with the last hidden layer and feed into the last layer for classification."
    },
    {
        "abstract": "  Recent studies have shown evidence of a significant decline of the Posidonia\noceanica (P.O.) meadows on a global scale. The monitoring and mapping of these\nmeadows are fundamental tools for measuring their status. We present an\napproach based on a deep neural network to automatically perform a\nhigh-precision semantic segmentation of P.O. meadows in sea-floor images,\noffering several improvements over the state of the art techniques. Our network\ndemonstrates outstanding performance over diverse test sets, reaching a\nprecision of 96.57% and an accuracy of 96.81%, surpassing the reliability of\nlabelling the images manually. Also, the network is implemented in an\nAutonomous Underwater Vehicle (AUV), performing an online P.O. segmentation,\nwhich will be used to generate real-time semantic coverage maps.\n",
        "method": "We present an approach based on a deep neural network to automatically perform a high-precision semantic segmentation of P. O. meadows in sea-floor images..."
    },
    {
        "abstract": "  As a continuation of the paper [20] on standard $f$-divergences, we make a\nsystematic study of maximal $f$-divergences in general von Neumann algebras.\nFor maximal $f$-divergences, apart from their definition based on Haagerup's\n$L^1$-space, we present the general integral expression and the variational\nexpression in terms of reverse tests. From these definition and expressions we\nprove important properties of maximal $f$-divergences, for instance, the\nmonotonicity inequality, the joint convexity, the lower semicontinuity, and the\nmartingale convergence. The inequality between the standard and the maximal\n$f$-divergences is also given.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe present the general integral expression and the variational expression in terms of reverse tests."
    },
    {
        "abstract": "  Removing noise in computer tomography (CT) data for real-time 3D\nvisualization is vital to improving the quality of the final display. However,\nthe CT noise cannot be removed by straight averaging because the noise has a\nbroadband spatial frequency that is overlapping with the interesting signal\nfrequencies. To improve the display of structures and features contained in the\ndata, we present spatially variant filtering that performs averaging of\nsub-regions around a central region. We compare our filter with four other\nsimilar spatially variant filters regarding entropy and processing time. The\nresults demonstrate significant improvement of the visual quality with\nprocessing time still within the millisecond range.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* To improve the display of structures and features contained in the data, we present spatially variant filtering that performs averaging of sub-regions around a central region.\n* We compare our filter with four other similar spatially variant filters regarding entropy and processing time."
    },
    {
        "abstract": "  In this work, we propose advanced pneumonia and Tuberculosis grading system\nfor X-ray images. The proposed system is a very deep fully convolutional\nclassification network with online augmentation that outputs confidence values\nfor diseases prevalence. Its a fully automated system capable of disease\nfeature understanding without any offline preprocessing step or manual feature\nextraction. We have achieved state- of-the- art performance on the public\ndatabases such as ChestXray-14, Mendeley, Shenzhen Hospital X-ray and Belarus\nX-ray set.\n",
        "method": "Here is the methodological sentence:\n\nThe proposed system is a very deep fully convolutional classification network with online augmentation that outputs confidence values for diseases prevalence."
    },
    {
        "abstract": "  This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared\ntask on Multilingual Parsing from Raw Text to Universal Dependencies. We base\nour submission on Stanford's winning system for the CoNLL 2017 shared task and\nmake two effective extensions: 1) incorporating deep contextualized word\nembeddings into both the part of speech tagger and parser; 2) ensembling\nparsers trained with different initialization. We also explore different ways\nof concatenating treebanks for further improvements. Experimental results on\nthe development data show the effectiveness of our methods. In the final\nevaluation, our system was ranked first according to LAS (75.84%) and\noutperformed the other systems by a large margin.\n",
        "method": "We base our submission on Stanford's winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and parser; 2) ensembling parsers trained with different initialization."
    },
    {
        "abstract": "  Purpose: An approach for the automated segmentation of visceral adipose\ntissue (VAT) and subcutaneous adipose tissue (SAT) in multicenter water-fat MRI\nscans of the abdomen was investigated, using two different neural network\narchitectures.\n  Methods: The two fully convolutional network architectures U-Net and V-Net\nwere trained, evaluated and compared on the water-fat MRI data. Data of the\nstudy Tellus with 90 scans from a single center was used for a 10-fold\ncross-validation in which the most successful configuration for both networks\nwas determined. These configurations were then tested on 20 scans of the\nmulticenter study beta-cell function in JUvenile Diabetes and Obesity\n(BetaJudo), which involved a different study population and scanning device.\n  Results: The U-Net outperformed the used implementation of the V-Net in both\ncross-validation and testing. In cross-validation, the U-Net reached average\ndice scores of 0.988 (VAT) and 0.992 (SAT). The average of the absolute\nquantification errors amount to 0.67% (VAT) and 0.39% (SAT). On the\nmulti-center test data, the U-Net performs only slightly worse, with average\ndice scores of 0.970 (VAT) and 0.987 (SAT) and quantification errors of 2.80%\n(VAT) and 1.65% (SAT).\n  Conclusion: The segmentations generated by the U-Net allow for reliable\nquantification and could therefore be viable for high-quality automated\nmeasurements of VAT and SAT in large-scale studies with minimal need for human\nintervention. The high performance on the multicenter test data furthermore\nshows the robustness of this approach for data of different patient\ndemographics and imaging centers, as long as a consistent imaging protocol is\nused.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The two fully convolutional network architectures U-Net and V-Net were trained, evaluated and compared on the water-fat MRI data.\n* Data of the study Tellus with 90 scans from a single center was used for a 10-fold cross-validation in which the most successful configuration for both networks was determined.\n* These configurations were then tested on 20 scans of the multicenter study beta-cell function in JUvenile Diabetes and Obesity (BetaJudo), which involved a different study population and scanning device."
    },
    {
        "abstract": "  Convolutional Neural Networks have dramatically improved in recent years,\nsurpassing human accuracy on certain problems and performance exceeding that of\ntraditional computer vision algorithms. While the compute pattern in itself is\nrelatively simple, significant compute and memory challenges remain as CNNs may\ncontain millions of floating-point parameters and require billions of\nfloating-point operations to process a single image. These computational\nrequirements, combined with storage footprints that exceed typical cache sizes,\npose a significant performance and power challenge for modern compute\narchitectures. One of the promising opportunities to scale performance and\npower efficiency is leveraging reduced precision representations for all\nactivations and weights as this allows to scale compute capabilities, reduce\nweight and feature map buffering requirements as well as energy consumption.\nWhile a small reduction in accuracy is encountered, these Quantized Neural\nNetworks have been shown to achieve state-of-the-art accuracy on standard\nbenchmark datasets, such as MNIST, CIFAR-10, SVHN and even ImageNet, and thus\nprovide highly attractive design trade-offs. Current research has focused\nmainly on the implementation of extreme variants with full binarization of\nweights and or activations, as well typically smaller input images. Within this\npaper, we investigate the scalability of dataflow architectures with respect to\nsupporting various precisions for both weights and activations, larger image\ndimensions, and increasing numbers of feature map channels. Key contributions\nare a formalized approach to understanding the scalability of the existing\nhardware architecture with cost models and a performance prediction as a\nfunction of the target device size. We provide validating experimental results\nfor an ImageNet classification on a server-class platform, namely the AWS F1\nnode.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We investigate the scalability of dataflow architectures with respect to supporting various precisions for both weights and activations, larger image dimensions, and increasing numbers of feature map channels.\""
    },
    {
        "abstract": "  Horticultural enterprises are becoming more sophisticated as the range of the\ncrops they target expands. Requirements for enhanced efficiency and\nproductivity have driven the demand for automating on-field operations.\nHowever, various problems remain yet to be solved for their reliable, safe\ndeployment in real-world scenarios. This paper examines major research trends\nand current challenges in horticultural robotics. Specifically, our work\nfocuses on sensing and perception in the three main horticultural procedures:\npollination, yield estimation, and harvesting. For each task, we expose major\nissues arising from the unstructured, cluttered, and rugged nature of field\nenvironments, including variable lighting conditions and difficulties in\nfruit-specific detection, and highlight promising contemporary studies.\n",
        "method": "This paper examines major research trends and current challenges in horticultural robotics, specifically focusing on sensing and perception in three main horticultural procedures: pollination, yield estimation, and harvesting."
    },
    {
        "abstract": "  We present a novel approach to optimally retarget videos for varied displays\nwith differing aspect ratios by preserving salient scene content discovered via\neye tracking. Our algorithm performs editing with cut, pan and zoom operations\nby optimizing the path of a cropping window within the original video while\nseeking to (i) preserve salient regions, and (ii) adhere to the principles of\ncinematography. Our approach is (a) content agnostic as the same methodology is\nemployed to re-edit a wide-angle video recording or a close-up movie sequence\ncaptured with a static or moving camera, and (b) independent of video length\nand can in principle re-edit an entire movie in one shot. Our algorithm\nconsists of two steps. The first step employs gaze transition cues to detect\ntime stamps where new cuts are to be introduced in the original video via\ndynamic programming. A subsequent step optimizes the cropping window path (to\ncreate pan and zoom effects), while accounting for the original and new cuts.\nThe cropping window path is designed to include maximum gaze information, and\nis composed of piecewise constant, linear and parabolic segments. It is\nobtained via L(1) regularized convex optimization which ensures a smooth\nviewing experience. We test our approach on a wide variety of videos and\ndemonstrate significant improvement over the state-of-the-art, both in terms of\ncomputational complexity and qualitative aspects. A study performed with 16\nusers confirms that our approach results in a superior viewing experience as\ncompared to gaze driven re-editing and letterboxing methods, especially for\nwide-angle static camera recordings.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"Our algorithm consists of two steps. The first step employs gaze transition cues to detect time stamps where new cuts are to be introduced in the original video via dynamic programming.\""
    },
    {
        "abstract": "  This master thesis focuses on practical application of Convolutional Neural\nNetwork models on the task of road labeling with bike attractivity score. We\nstart with an abstraction of real world locations into nodes and scored edges\nin partially annotated dataset. We enhance information available about each\nedge with photographic data from Google Street View service and with additional\nneighborhood information from Open Street Map database. We teach a model on\nthis enhanced dataset and experiment with ImageNet Large Scale Visual\nRecognition Competition. We try different dataset enhancing techniques as well\nas various model architectures to improve road scoring. We also make use of\ntransfer learning to use features from a task with rich dataset of ImageNet\ninto our task with smaller number of images, to prevent model overfitting.\n",
        "method": "Here is the methodological sentence:\n\nWe start with an abstraction of real world locations into nodes and scored edges in partially annotated dataset."
    },
    {
        "abstract": "  In this paper, the photocatalytic activity of multiferroics BiFeO3 (BFO) and\nBi0.8La0.2FeO3 (BLFO) nanocrystals with two different morphologies which were\nsynthesized by two different sol-gel (SG) and hydrothermal (HT) methods have\nbeen studied. All the obtained samples were characterized using X-ray\ndiffractometer, Fourier transform infrared spectroscopy, transmission electron\nmicroscopy, UV-vis spectroscopy and vibrating sample magnetometer. Differential\nthermal analysis (DTA) measurements were probed ferroelectric- paraelectric\nfirst-order phase transition (TC) for all samples. Addition of lanthanum\ndecreases the electric phase transition. For photocatalyst application of\nbismuth ferrite, adsorption potential of nanoparticles for methylene blue (MB)\norganic dye was evaluated. The doping of La in the BFO structure enhanced the\nphotocatalytic activity and about 71% degradation of MB dye was obtained under\nvisible irradiation. The magnetic and ferroelectric properties of BLFO\nnanoparticles improve compared to the undoped BiFeO3 nanoparticles. The\nnon-saturation at high applied magnetic field for as-prepared samples by HT is\nrelated to the size and shape of products. This work not only presents an\neffect of lanthanum substitution into the bismuth ferrite structure on the\nphysical properties of BFO, but also compares the synthesis method and its\ninfluence on the photocatalytic activity and multiferroics properties of all\nnanopowders.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* All the obtained samples were characterized using X-ray diffractometer, Fourier transform infrared spectroscopy, transmission electron microscopy, UV-vis spectroscopy and vibrating sample magnetometer.\n* Differential thermal analysis (DTA) measurements were probed ferroelectric-paralectric first-order phase transition (TC) for all samples."
    },
    {
        "abstract": "  Machine vision systems using convolutional neural networks (CNNs) for robotic\napplications are increasingly being developed. Conventional vision CNNs are\ndriven by camera frames at constant sample rate, thus achieving a fixed latency\nand power consumption tradeoff. This paper describes further work on the first\nexperiments of a closed-loop robotic system integrating a CNN together with a\nDynamic and Active Pixel Vision Sensor (DAVIS) in a predator/prey scenario. The\nDAVIS, mounted on the predator Summit XL robot, produces frames at a fixed 15\nHz frame-rate and Dynamic Vision Sensor (DVS) histograms containing 5k ON and\nOFF events at a variable frame-rate ranging from 15-500 Hz depending on the\nrobot speeds. In contrast to conventional frame-based systems, the latency and\nprocessing cost depends on the rate of change of the image. The CNN is trained\noffline on the 1.25h labeled dataset to recognize the position and size of the\nprey robot, in the field of view of the predator. During inference, combining\nthe ten output classes of the CNN allows extracting the analog position vector\nof the prey relative to the predator with a mean 8.7% error in angular\nestimation. The system is compatible with conventional deep learning\ntechnology, but achieves a variable latency-power tradeoff that adapts\nautomatically to the dynamics. Finally, investigations on the robustness of the\nalgorithm, a human performance comparison and a deconvolution analysis are also\nexplored.\n",
        "method": "The DAVIS, mounted on the predator Summit XL robot, produces frames at a fixed 15 Hz frame-rate and Dynamic Vision Sensor (DVS) histograms containing 5k ON and OFF events at a variable frame-rate ranging from 15-500 Hz depending on the robot speeds."
    },
    {
        "abstract": "  Chalcogenide glass-silver heterostructures are candidates for photoresist and\ndiffractive optical applications. To optimize their processing, we report the\nkinetics of Ag photo-dissolution in As2S3 matrix using in-situ optical\ntransmission/reflection measurements and real time atomic force microscopy\n(AFM) imaging under optical illumination. The results indicate that\nphotodissolution occurs in three stages with the extent and kinetics of each\nstage depending strongly on Ag film thickness. By contrast, the\nphoto-dissolution is found to be independent of As2S3 matrix thickness. The\nextent of three stages also depends strongly on the laser dose and can be\nreduced to two stages at higher laser fluence. A comparative study of two\noppositely stacked sample configurations: As2S3/Ag/glass and Ag/As2S3/glass\nshow that the heterostructures respond differently to light illumination. For\nthe former, Ag dissolves completely into As2S3 matrix at a faster rate than for\nthe latter case. The origin of this difference is established by energy\ndispersive X-ray spectroscopy and AFM measurements.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* To optimize their processing, we report the kinetics of Ag photo-dissolution in As2S3 matrix using in-situ optical transmission/reflection measurements and real-time atomic force microscopy (AFM) imaging under optical illumination.\n* The results indicate that photodissolution occurs in three stages with the extent and kinetics of each stage depending strongly on Ag film thickness, as measured by AFM imaging."
    },
    {
        "abstract": "  Learning a metric of natural image patches is an important tool for analyzing\nimages. An efficient means is to train a deep network to map an image patch to\na vector space, in which the Euclidean distance reflects patch similarity.\nPrevious attempts learned such an embedding in a supervised manner, requiring\nthe availability of many annotated images. In this paper, we present an\nunsupervised embedding of natural image patches, avoiding the need for\nannotated images. The key idea is that the similarity of two patches can be\nlearned from the prevalence of their spatial proximity in natural images.\nClearly, relying on this simple principle, many spatially nearby pairs are\noutliers, however, as we show, the outliers do not harm the convergence of the\nmetric learning. We show that our unsupervised embedding approach is more\neffective than a supervised one or one that uses deep patch representations.\nMoreover, we show that it naturally leads itself to an efficient\nself-supervised domain adaptation technique onto a target domain that contains\na common foreground object.\n",
        "method": "The key idea is that the similarity of two patches can be learned from the prevalence of their spatial proximity in natural images."
    },
    {
        "abstract": "  A telecommunication system uses carriers in order to transmit information\nthrough a cable or wirelessly. If each time only one carrier is transmitted,\nthen the system s signal will not be immune to frequency selective fading. If\nfrequency selective fading includes the working frequency of the system, then\nthe wireless link will not be established. Orthogonal frequency division\nmultiplexing OFDM is the primary solution for coping with inter signal\ninterference and frequency selective fading. Many carriers can be produced by\nsplitting a fast information stream to slower data series. Different orthogonal\nfrequencies carry slower data series. System s performance can be further\nenhanced with the utilization of turbo codes. Turbo codes make the system more\nimmune to noise effects with excellent BER results. This paper presents the\nthorough analysis of a turbo coded OFDM scheme using a PCCC technique in the\npresence of a channel which includes AWGN, phase noise, Rayleigh fading, Rician\nfading and Doppler shift.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* This paper presents the thorough analysis of a turbo coded OFDM scheme using a PCCC technique in the presence of a channel which includes AWGN, phase noise, Rayleigh fading, Rician fading and Doppler shift."
    },
    {
        "abstract": "  Most of the existing tracking methods based on CNN(convolutional neural\nnetworks) are too slow for real-time application despite the excellent tracking\nprecision compared with the traditional ones. In this paper, a fast dynamic\nvisual tracking algorithm combining CNN based MDNet(Multi-Domain Network) and\nRoIAlign was developed. The major problem of MDNet also lies in the time\nefficiency. Considering the computational complexity of MDNet is mainly caused\nby the large amount of convolution operations and fine-tuning of the network\nduring tracking, a RoIPool layer which could conduct the convolution over the\nwhole image instead of each RoI is added to accelerate the convolution and a\nnew strategy of fine-tuning the fully-connected layers is used to accelerate\nthe update. With RoIPool employed, the computation speed has been increased but\nthe tracking precision has dropped simultaneously. RoIPool could lose some\npositioning precision because it can not handle locations represented by\nfloating numbers. So RoIAlign, instead of RoIPool, which can process floating\nnumbers of locations by bilinear interpolation has been added to the network.\nThe results show the target localization precision has been improved and it\nhardly increases the computational cost. These strategies can accelerate the\nprocessing and make it 7x faster than MDNet with very low impact on precision\nand it can run at around 7 fps. The proposed algorithm has been evaluated on\ntwo benchmarks: OTB100 and VOT2016, on which high precision and speed have been\nobtained. The influence of the network structure and training data are also\ndiscussed with experiments.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* A RoIPool layer which could conduct the convolution over the whole image instead of each RoI was added to accelerate the convolution.\n* A new strategy of fine-tuning the fully-connected layers was used to accelerate the update.\n* RoIAlign, instead of RoIPool, which can process floating numbers of locations by bilinear interpolation has been added to the network."
    },
    {
        "abstract": "  When creating an outfit, style is a criterion in selecting each fashion item.\nThis means that style can be regarded as a feature of the overall outfit.\nHowever, in various previous studies on outfit generation, there have been few\nmethods focusing on global information obtained from an outfit. To address this\ndeficiency, we have incorporated an unsupervised style extraction module into a\nmodel to learn outfits. Using the style information of an outfit as a whole,\nthe proposed model succeeded in generating outfits more flexibly without\nrequiring additional information. Moreover, the style information extracted by\nthe proposed model is easy to interpret. The proposed model was evaluated on\ntwo human-generated outfit datasets. In a fashion item prediction task (missing\nprediction task), the proposed model outperformed a baseline method. In a style\nextraction task, the proposed model extracted some easily distinguishable\nstyles. In an outfit generation task, the proposed model generated an outfit\nwhile controlling its styles. This capability allows us to generate fashionable\noutfits according to various preferences.\n",
        "method": "Here are the methodological sentences:\n\n* Using the style information of an outfit as a whole, the proposed model succeeded in generating outfits more flexibly without requiring additional information.\n* The proposed model was evaluated on two human-generated outfit datasets.\n* In a fashion item prediction task (missing prediction task), the proposed model outperformed a baseline method."
    },
    {
        "abstract": "  The idea of partial smoothness in optimization blends certain smooth and\nnonsmooth properties of feasible regions and objective functions. As a\nconsequence, the standard first-order conditions guarantee that diverse\niterative algorithms (and post-optimality analyses) identify active structure\nor constraints. However, by instead focusing directly on the first-order\nconditions, the formal concept of partial smoothness simplifies dramatically:\nin basic differential geometric language, it is just a constant-rank condition.\nIn this view, partial smoothness extends to more general mappings, such as\nsaddlepoint operators underlying primal-dual splitting algorithms.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* None (this appears to be an introductory/summary passage rather than a research methodology description)"
    },
    {
        "abstract": "  Detection of cell nuclei in microscopic images is a challenging research\ntopic, because of limitations in cellular image quality and diversity of\nnuclear morphology, i.e. varying nuclei shapes, sizes, and overlaps between\nmultiple cell nuclei. This has been a topic of enduring interest with promising\nrecent success shown by deep learning methods. These methods train for example\nconvolutional neural networks (CNNs) with a training set of input images and\nknown, labeled nuclei locations. Many of these methods are supplemented by\nspatial or morphological processing. We develop a new approach that we call\nShape Priors with Convolutional Neural Networks (SP-CNN) to perform\nsignificantly enhanced nuclei detection. A set of canonical shapes is prepared\nwith the help of a domain expert. Subsequently, we present a new network\nstructure that can incorporate `expected behavior' of nucleus shapes via two\ncomponents: {\\em learnable} layers that perform the nucleus detection and a\n{\\em fixed} processing part that guides the learning with prior information.\nAnalytically, we formulate a new regularization term that is targeted at\npenalizing false positives while simultaneously encouraging detection inside\ncell nucleus boundary. Experimental results on a challenging dataset reveal\nthat SP-CNN is competitive with or outperforms several state-of-the-art\nmethods.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Many of these methods are supplemented by spatial or morphological processing.\n* We present a new network structure that can incorporate `expected behavior' of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information.\n* Analytically, we formulate a new regularization term that is targeted at penalizing false positives while simultaneously encouraging detection inside cell nucleus boundary."
    },
    {
        "abstract": "  Pathological glomerulus classification plays a key role in the diagnosis of\nnephropathy. As the difference between different subcategories is subtle,\ndoctors often refer to slides from different staining methods to make\ndecisions. However, creating correspondence across various stains is\nlabor-intensive, bringing major difficulties in collecting data and training a\nvision-based algorithm to assist nephropathy diagnosis. This paper provides an\nalternative solution for integrating multi-stained visual cues for glomerulus\nclassification. Our approach, named generator-to-classifier (G2C), is a\ntwo-stage framework. Given an input image from a specified stain, several\ngenerators are first applied to estimate its appearances in other staining\nmethods, and a classifier follows to combine visual cues from different stains\nfor prediction (whether it is pathological, or which type of pathology it has).\nWe optimize these two stages in a joint manner. To provide a reasonable\ninitialization, we pre-train the generators in an unlabeled reference set under\nan unpaired image-to-image translation task, and then fine-tune them together\nwith the classifier. We conduct experiments on a glomerulus type classification\ndataset collected by ourselves (there are no publicly available datasets for\nthis purpose). Although joint optimization slightly harms the authenticity of\nthe generated patches, it boosts classification performance, suggesting more\neffective visual cues are extracted in an automatic way. We also transfer our\nmodel to a public dataset for breast cancer classification, and outperform the\nstate-of-the-arts significantly.\n",
        "method": "Here is the methodological sentence:\n\nOur approach, named generator-to-classifier (G2C), is a two-stage framework."
    },
    {
        "abstract": "  Many physics instructors aim to support student sensemaking in their\nclassrooms. However, this can be challenging since instances of sensemaking\ntend to be short-lived, with students often defaulting to approaches based on\nanswer-making or rote mathematical manipulation. In this study, we present\nevidence that specific recurring questions can serve a key role in the\nsensemaking process. Using a case-study of two students discussing an E&M\nthought experiment, we show how students' entry into sensemaking is marked by\nthe articulation of a particular question, based on a perceived gap or\ninconsistency in understanding and how this question recurs throughout their\nsubsequent explanations, arguing that these recurrences may serve to stabilize\nand extend the process.\n",
        "method": "Using a case-study of two students discussing an E&M thought experiment, we show how students' entry into sensemaking is marked by the articulation of a particular question, based on a perceived gap or inconsistency in understanding and how this question recurs throughout their subsequent explanations."
    },
    {
        "abstract": "  Dataset creation is typically one of the first steps when applying Artificial\nIntelligence methods to a new task; and the real world performance of models\nhinges on the quality and quantity of data available. Producing an image\ndataset for semantic segmentation is resource intensive, particularly for\nspecialist subjects where class segmentation is not able to be effectively\nfarmed out. The benefit of producing a large, but poorly labelled, dataset\nversus a small, expertly segmented dataset for semantic segmentation is an open\nquestion. Here we show that a large, noisy dataset outperforms a small,\nexpertly segmented dataset for training a Fully Convolutional Network model for\nsemantic segmentation of corrosion in images. A large dataset of 250 images\nwith segmentations labelled by undergraduates and a second dataset of just 10\nimages, with segmentations labelled by subject matter experts were produced.\nThe mean Intersection over Union and micro F-score metrics were compared after\ntraining for 50,000 epochs. This work is illustrative for researchers setting\nout to develop deep learning models for detection and location of specialist\nfeatures.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* A large dataset of 250 images with segmentations labelled by undergraduates and a second dataset of just 10 images, with segmentations labelled by subject matter experts were produced.\n* The mean Intersection over Union and micro F-score metrics were compared after training for 50,000 epochs."
    },
    {
        "abstract": "  Fashion preference is a fuzzy concept that depends on customer taste,\nprevailing norms in fashion product/style, henceforth used interchangeably, and\na customer's perception of utility or fashionability, yet fashion e-retail\nrelies on algorithmically generated search and recommendation systems that\nprocess structured data and images to best match customer preference. Retailers\nstudy tastes solely as a function of what sold vs what did not, and take it to\nrepresent customer preference. Such explicit modeling, however, belies the\nunderlying user preference, which is a complicated interplay of preference and\ncommercials such as brand, price point, promotions, other sale events, and\ncompetitor push/marketing. It is hard to infer a notion of utility or even\ncustomer preference by looking at sales data.\n  In search and recommendation systems for fashion e-retail, customer\npreference is implicitly derived by user-user similarity or item-item\nsimilarity. In this work, we aim to derive a metric that separates the buying\npreferences of users from the commercials of the merchandise (price,\npromotions, etc). We extend our earlier work on explicit signals to gauge\nsellability or preference with implicit signals from user behaviour.\n",
        "method": "Here are the methodological sentences extracted from the abstracts:\n\n* Retailers study tastes solely as a function of what sold vs what did not, and take it to represent customer preference.\n* Customer preference is implicitly derived by user-user similarity or item-item similarity.\n\nNote that there may be additional methodological details in the full article, but these two sentences are the most relevant from the provided abstracts."
    },
    {
        "abstract": "  We establish upper bounds of bit complexity of computing solution operators\nfor symmetric hyperbolic systems of PDEs. Here we continue the research started\nin in our revious publications where computability, in the rigorous sense of\ncomputable analysis, has been established for solution operators of Cauchy and\ndissipative boundary-value problems for such systems.\n",
        "method": "Here is the methodological sentence:\n\nWe establish upper bounds of bit complexity of computing solution operators..."
    },
    {
        "abstract": "  In this paper, we deal with uncertainty quantification for the random\nLegendre differential equation, with input coefficient $A$ and initial\nconditions $X_0$ and $X_1$. In a previous study [Calbo G. et al, Comput. Math.\nAppl., 61(9), 2782--2792 (2011)], a mean square convergent power series\nsolution on $(-1/e,1/e)$ was constructed, under the assumptions of mean fourth\nintegrability of $X_0$ and $X_1$, independence, and at most exponential growth\nof the absolute moments of $A$. In this paper, we relax these conditions to\nconstruct an $\\mathrm{L}^p$ solution ($1\\leq p\\leq\\infty$) to the random\nLegendre differential equation on the whole domain $(-1,1)$, as in its\ndeterministic counterpart. Our hypotheses assume no independence and less\nintegrability of $X_0$ and $X_1$. Moreover, the growth condition on the moments\nof $A$ is characterized by the boundedness of $A$, which simplifies the proofs\nsignificantly. We also provide approximations of the expectation and variance\nof the response process. The numerical experiments show the wide applicability\nof our findings. A comparison with Monte Carlo simulations and gPC expansions\nis performed.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nIn this paper, we relax these conditions to construct an $\\mathrm{L}^p$ solution ($1\\leq p\\leq\\infty$) to the random Legendre differential equation on the whole domain $(-1,1)$, as in its deterministic counterpart."
    },
    {
        "abstract": "  This paper proposes an approach for rapid bounding box annotation for object\ndetection datasets. The procedure consists of two stages: The first step is to\nannotate a part of the dataset manually, and the second step proposes\nannotations for the remaining samples using a model trained with the first\nstage annotations. We experimentally study which first/second stage split\nminimizes to total workload. In addition, we introduce a new fully labeled\nobject detection dataset collected from indoor scenes. Compared to other indoor\ndatasets, our collection has more class categories, different backgrounds,\nlighting conditions, occlusion and high intra-class differences. We train deep\nlearning based object detectors with a number of state-of-the-art models and\ncompare them in terms of speed and accuracy. The fully annotated dataset is\nreleased freely available for the research community.\n",
        "method": "The procedure consists of two stages: The first step is to annotate a part of the dataset manually, and the second step proposes annotations for the remaining samples using a model trained with the first stage annotations."
    },
    {
        "abstract": "  Falls and their related injuries pose a significant risk to human health. One\nof the most common falls, the forward fall, frequently occurs among adults and\nthe elderly. In this study, we propose using a human body model, developed\nusing the MAthematical DYnamic MOdel (MADYMO) software, in place of human\nsubjects, to investigate forward fall-related injuries. The MADYMO human body\nmodel is capable of simulating items that cannot be assessed on human subjects,\nsuch as human kinematics, human dynamics, and the possibility of injuries. In\norder to achieve our goal, a set of experiments was conducted to measure the\nimpact force during a worst-case forward fall scenario (the outstretched hand\nposition) for two short fall heights. Similar to the experimental design used\non the human subjects, we generated a MADYMO human model. After performing the\nsimulations, the results of the experiment on the human subjects and the MADYMO\nsimulation model were compared. We demonstrated a significant correlation\nbetween the MADYMO simulation and the human subject experiments with respect to\nthe magnitude and timing of the impact forces. Consequently, we validated the\nMADYMO human body model as a means to accurately assess forward fall-related\ninjuries. Additionally, we compared the predicted results of a mathematical\nmodel with the MADYMO human body model. The MADYMO model is reliable and can\ndemonstrate an accurate impact time. Therefore, we conclude that the MADYMO\nhuman model can be utilized as a reliable model to investigate forward\nfall-related injuries from a typical standing position.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* A set of experiments was conducted to measure the impact force during a worst-case forward fall scenario (the outstretched hand position) for two short fall heights.\n* We generated a MADYMO human model, similar to the experimental design used on the human subjects."
    },
    {
        "abstract": "  We prove cut-off results for deadlocks and serializability of a $PV$-thread\n$T$ run in parallel with itself: For a $PV$ thread $T$ which accesses a set\n$\\mathcal{R}$ of resources, each with a maximal capacity\n$\\kappa:\\mathcal{R}\\to\\mathbb{N}$, the PV-program $T^n$, where $n$ copies of\n$T$ are run in parallel, is deadlock free for all $n$ if and only if $T^M$ is\ndeadlock free where $M=\\Sigma_{r\\in\\mathcal{R}}\\kappa(r)$. This is a sharp\nbound: For all $\\kappa:\\mathcal{R}\\to\\mathbb{N}$ and finite $\\mathcal{R}$ there\nis a thread $T$ using these resources such that $T^M$ has a deadlock, but $T^n$\ndoes not for $n<M$. Moreover, we prove a more general theorem: There are no\ndeadlocks in $p=T1|T2|\\cdots |Tn$ if and only if there are no deadlocks in\n$T_{i_1}|T_{i_2}|\\cdots |T_{i_M}$ for any subset $\\{i_1,\\ldots,i_M\\}\\subset\n[1:n]$. For $\\kappa(r)\\equiv 1$, $T^n$ is serializable for all $n$ if and only\nif $T^2$ is serializable. For general capacities, we define a local obstruction\nto serializability. There is no local obstruction to serializability in $T^n$\nfor all $n$ if and only if there is no local obstruction to serializability in\n$T^M$ for $M=\\Sigma_{r\\in\\mathcal{R}}\\kappa(r)+1$. The obstructions may be\nfound using a deadlock algorithm in $T^{M+1}$. These serializability results\nalso have a generalization: If there are no local obstructions to\nserializability in any of the $M$-dimensional sub programs,\n$T_{i_1}|T_{i_2}|\\cdots |T_{i_M}$, then $p$ is serializable.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* None (There are no explicit methodological sentences in this abstract.)"
    },
    {
        "abstract": "  Neurogenic Bladder Dysfunction has detrimental effects on day-to-day life of\nmillions of people. Some of the most common symptoms faced by these patients\ninclude urinary incontinence, urgency and retention. Since elevated bladder\npressure due to prolonged urine storage inside bladder may have adverse impacts\non patient's renal health, urologists recommend clean-intermittent\ncatheterization (CIC) every 2 to 4 hours throughout the day to relieve bladder\npressure. However, since urine production by kidneys is an intermittent process\nand most of these patients have limited mobility, such frequent trips to\nwashroom can prove to be challenging. Sometimes, bladder fills to capacity\nbefore the recommended CIC time is reached causing embarrassing situation due\nto leakage. Hence, time-based CIC strategy is difficult to implement and has\nhigh chances of failure. As such, continence is the primary concern for most of\nthese patients but sadly there are no practical solutions available in the\nmarket that address this concern. A real-time notification system that could\ngive feedback to patients on when \"bladder is almost-full\" could help these\npatients to better plan their bathroom trips. This work explores the\nfeasibility of using a near infrared-light based wearable, non-invasive\nspectroscopy technique that can sense amount of urine present inside the\nbladder and give details on developing a bladder state estimation device.\n  We present preliminary results by testing our device on optical phantoms and\nperforming ex vivo measurements on porcine bladder and intestines. We later\nexplored the possibility of using the device on human subjects, after study was\napproved by the UC Davis Institution Review Board (IRB).\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We present preliminary results by testing our device on optical phantoms and performing ex vivo measurements on porcine bladder and intestines.\n* We later explored the possibility of using the device on human subjects, after study was approved by the UC Davis Institution Review Board (IRB)."
    },
    {
        "abstract": "  This paper presents KeypointNet, an end-to-end geometric reasoning framework\nto learn an optimal set of category-specific 3D keypoints, along with their\ndetectors. Given a single image, KeypointNet extracts 3D keypoints that are\noptimized for a downstream task. We demonstrate this framework on 3D pose\nestimation by proposing a differentiable objective that seeks the optimal set\nof keypoints for recovering the relative pose between two views of an object.\nOur model discovers geometrically and semantically consistent keypoints across\nviewing angles and instances of an object category. Importantly, we find that\nour end-to-end framework using no ground-truth keypoint annotations outperforms\na fully supervised baseline using the same neural network architecture on the\ntask of pose estimation. The discovered 3D keypoints on the car, chair, and\nplane categories of ShapeNet are visualized at http://keypointnet.github.io/.\n",
        "method": "Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task."
    },
    {
        "abstract": "  Electroencephalography (EEG) is another mode for performing Person\nIdentification (PI). Due to the nature of the EEG signals, EEG-based PI is\ntypically done while the person is performing some kind of mental task, such as\nmotor control. However, few works have considered EEG-based PI while the person\nis in different mental states (affective EEG). The aim of this paper is to\nimprove the performance of affective EEG-based PI using a deep learning\napproach. \\textcolor{red}{We proposed a cascade of deep learning using a\ncombination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs)}. CNNs are used to handle the spatial information from the EEG\nwhile RNNs extract the temporal information. \\textcolor{red}{We evaluated two\ntypes of RNNs, namely, Long Short-Term Memory (CNN-LSTM) and Gated Recurrent\nUnit (CNN-GRU). } The proposed method is evaluated on the state-of-the-art\naffective dataset DEAP. The results indicate that CNN-GRU and CNN-LSTM can\nperform PI from different affective states and reach up to 99.90--100\\% mean\nCorrect Recognition Rate (CRR), significantly outperforming a support vector\nmachine (SVM) baseline system that uses power spectral density (PSD) features.\nNotably, the 100\\% mean \\emph{CRR} comes from only 40 subjects in DEAP dataset.\nTo reduce the number of EEG electrodes from thirty-two to five for more\npractical applications, the frontal region gives the best results reaching up\nto 99.17\\% CRR (from CNN-GRU). Amongst the two deep learning models, we find\nCNN-GRU to slightly outperform CNN-LSTM, while having faster training time.\n\\textcolor{red}{Furthermore, CNN-GRU overcomes the influence of affective\nstates in EEG-Based PI reported in the previous works.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe proposed a cascade of deep learning using a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)."
    },
    {
        "abstract": "  In this work we introduce a time- and memory-efficient method for structured\nprediction that couples neuron decisions across both space at time. We show\nthat we are able to perform exact and efficient inference on a densely\nconnected spatio-temporal graph by capitalizing on recent advances on deep\nGaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)\nefficient, (b) has a unique global minimum, and (c) can be trained end-to-end\nalongside contemporary deep networks for video understanding. We experiment\nwith multiple connectivity patterns in the temporal domain, and present\nempirical improvements over strong baselines on the tasks of both semantic and\ninstance segmentation of videos.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe show that we are able to perform exact and efficient inference on a densely connected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs)."
    },
    {
        "abstract": "  We consider learning based methods for visual localization that do not\nrequire the construction of explicit maps in the form of point clouds or\nvoxels. The goal is to learn an implicit representation of the environment at a\nhigher, more abstract level. We propose to use a generative approach based on\nGenerative Query Networks (GQNs, Eslami et al. 2018), asking the following\nquestions: 1) Can GQN capture more complex scenes than those it was originally\ndemonstrated on? 2) Can GQN be used for localization in those scenes? To study\nthis approach we consider procedurally generated Minecraft worlds, for which we\ncan generate images of complex 3D scenes along with camera pose coordinates. We\nfirst show that GQNs, enhanced with a novel attention mechanism can capture the\nstructure of 3D scenes in Minecraft, as evidenced by their samples. We then\napply the models to the localization problem, comparing the results to a\ndiscriminative baseline, and comparing the ways each approach captures the task\nuncertainty.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe propose to use a generative approach based on Generative Query Networks (GQNs) ..."
    },
    {
        "abstract": "  The properties of the transfer-matrix of U(1) lattice gauge theory in the\nFourier basis are explored. Among other statements it is shown: 1) the\ntransfer-matrix is block-diagonal, 2) all consisting vectors of a block are\nknown based on an arbitrary block vector, 3) the ground-state belongs to the\nzero-mode's block. The emergence of maximum-points in matrix-elements as\nfunctions of the gauge coupling is clarified. Based on explicit expressions for\nthe matrix-elements we present numerical results as tests of our statements.\n",
        "method": "The transfer-matrix is block-diagonal, all consisting vectors of a block are known based on an arbitrary block vector, and the ground-state belongs to the zero-mode's block."
    },
    {
        "abstract": "  The crack phase field model has been well established and validated for a\nvariety of complex crack propagation patterns within a homogeneous medium under\neither tensile or shear loading. However, relatively less attention has been\npaid to crack propagation under combined tensile and shear loading or crack\npropagation within composite materials made of two constituents with very\ndifferent elastic moduli. In this work, we compare crack propagation under such\ncircumstances modelled by two representative formulations, anisotropic and\nhybrid formulations, which have distinct stiffness degradation schemes upon\ncrack propagation. We demonstrate that the hybrid formulation is more adequate\nfor modeling crack propagation problems under combined loading because the\nresidual stiffness of the damaged zone in the anisotropic formulation may lead\nto spurious crack growth and altered load-displacement response.\n",
        "method": "The methodological sentences are:\n\n* However, relatively less attention has been paid to crack propagation under combined tensile and shear loading or crack propagation within composite materials made of two constituents with very different elastic moduli.\n* In this work, we compare crack propagation under such circumstances modelled by two representative formulations, anisotropic and hybrid formulations, which have distinct stiffness degradation schemes upon crack propagation."
    },
    {
        "abstract": "  Training of elite athletes requires regular physiological and medical\nmonitoring to plan the schedule, intensity and volume of training, and\nsubsequent recovery. In sports medicine, ECG-based analyses are well\nestablished. However, they rarely consider the correspondence of respiratory\nand cardiac activity. Given such mutual influence, we hypothesize that athlete\nmonitoring might be developed with causal inference and that detailed,\ntime-related techniques should be preceded by a more general, time-independent\napproach that considers the whole group of participants and parameters\ndescribing whole signals. The aim of this study was to discover general causal\npaths among cardiac and respiratory variables in elite athletes in two body\npositions (supine and standing), at rest. ECG and impedance pneumography\nsignals were obtained from 100 elite athletes. The mean HR, the RMSSD, its\nnatural logarithm, the mean respiratory rate, the breathing activity\ncoefficients, and the resulting breathing regularity were estimated. Several\ncausal discovery frameworks were applied: generalized correlations, CAM, FGES,\nGFCI, and two Bayesian network learning algorithms: Hill-Climbing and Tabu. The\nmain, still mild, rules best supported by data are: for supine - tidal volume\ncauses heart activity variation, which causes HR, which causes respiratory\ntiming; and for standing - normalized respiratory activity variation causes\naverage heart activity. The presented approach allows data-driven and\ntime-independent analysis of elite athletes as a particular population, without\nconsidering prior knowledge. However, the results seem to be consistent with\nthe medical background. Causality inference is an interesting mathematical\napproach to the analysis of biological responses, which are complex. One can\nuse it to profile athletes and plan appropriate training.\n",
        "method": "Here is the methodological sentence:\n\nECG and impedance pneumography signals were obtained from 100 elite athletes."
    },
    {
        "abstract": "  The quasistatic problem of shape memory alloys is reviewed within the\nphenomenological mechanics of solids without microphysics analysis. The\nassumption is that the temperature variation rate is small. Reissner's type of\ngeneralized variational principle is presented, and its mathematical\njustification is given for three-dimensional bodies made of shape memory\nmaterials.\n",
        "method": "The assumption is that the temperature variation rate is small."
    },
    {
        "abstract": "  We analyze the time series obtained from different dynamical regimes of the\nlogistic map by constructing their equivalent time series (TS) networks, using\nthe visibility algorithm. The regimes analyzed include both periodic and\nchaotic regimes, as well as intermittent regimes and the Feigenbaum attractor\nat the edge of chaos. We use the methods of algebraic topology to define the\nsimplicial characterizers, which can analyse the simplicial structure of the\nnetworks at both the global and local levels. The simplicial characterisers\nbring out the hierarchical levels of complexity at various topological levels.\nThese hierarchical levels of complexity find the skeleton of the local dynamics\nembedded in the network which influence the global dynamical properties of the\nsystem, and also permit the identification of dominant motifs. We also analyze\nthe same networks using conventional network characterizers such as average\npath lengths and clustering coefficients. We see that the simplicial\ncharacterizers are capable of distinguishing between different dynamical\nregimes and can pick up subtle differences in dynamical behavior, whereas the\nusual characterizers provide a coarser characterization. However, the two taken\nin conjunction, can provide information about the dynamical behavior of the\ntime series, as well as the correlations in the evolving system. Our methods\ncan therefore provide powerful tools for the analysis of dynamical systems.\n",
        "method": "Here is the methodological sentence:\n\nWe construct their equivalent time series (TS) networks, using the visibility algorithm."
    },
    {
        "abstract": "  Archaeologists are in dire need of automated object reconstruction methods.\nFragments reassembly is close to puzzle problems, which may be solved by\ncomputer vision algorithms. As they are often beaten on most image related\ntasks by deep learning algorithms, we study a classification method that can\nsolve jigsaw puzzles. In this paper, we focus on classifying the relative\nposition: given a couple of fragments, we compute their local relation (e.g. on\ntop). We propose several enhancements over the state of the art in this domain,\nwhich is outperformed by our method by 25\\%. We propose an original dataset\ncomposed of pictures from the Metropolitan Museum of Art. We propose a greedy\nreconstruction method based on the predicted relative positions.\n",
        "method": "Fragments reassembly is close to puzzle problems, which may be solved by computer vision algorithms."
    },
    {
        "abstract": "  A meshless method is presented to solve the radiative transfer equation in\nthe even parity formulation of the discrete ordinates method in complex 2D and\n3D geometries. Prediction results of radiative heat transfer problems obtained\nby the proposed method are compared with reference in order to assess the\ncorrectness of the present method.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nPrediction results of radiative heat transfer problems obtained by the proposed method are compared with reference in order to assess the correctness of the present method."
    },
    {
        "abstract": "  Using thermal infrared detectors mounted on drones, and applying techniques\nfrom astrophysics, we hope to support the field of conservation ecology by\ncreating an automated pipeline for the detection and identification of certain\nendangered species and poachers from thermal infrared data. We test part of our\nsystem by attempting to detect simulated poachers in the field. Whilst we find\nthat we can detect humans hiding in the field in some types of terrain, we also\nfind several environmental factors that prevent accurate detection, such as\nambient heat from the ground, absorption of infrared emission by the\natmosphere, obscuring vegetation and spurious sources from the terrain. We\ndiscuss the effect of these issues, and potential solutions which will be\nrequired for our future vision for a fully automated drone-based global\nconservation monitoring system.\n",
        "method": "Here is the methodological sentence:\n\n\"We test part of our system by attempting to detect simulated poachers in the field.\""
    },
    {
        "abstract": "  Violation of Bell inequality is a prominent detection method for quantum\ncorrelations present in composite quantum systems, both in finite and infinite\ndimensions. We investigate the consequence on the violation of local realism\nbased on pseduospin operators when photons are added or subtracted in a single\nmode or in both the modes of the two-mode squeezed states of light in presence\nof noise. In the noiseless situation, we show that for addition (subtraction)\nof photons in a single mode, there is an overall enhancement in the maximal\nviolation, although we observe an interplay between monotonicity and\nnon-monotonicity in the violation of Bell inequality depending on the squeezing\nstrength. Moreover, we report that for low squeezing or low number of photons\nadded or subtracted, subtraction in both the modes can lead to higher violation\nof local realism than that in the case of addition. For any choice of\nparameters, such ordering is not seen if one compares their entanglement\ncontents. In the event of a faulty twin-beam generator, we obtain a\nlower-than-expected squeezing in the state. In such a case, or in imperfect\nphoton addition (subtraction), or under local noise, we find that the violation\nof local realism by the noise-affected two-mode squeezed states always\ndecreases. Interestingly however, we notice that photon addition (subtraction)\ncan in general help to conquer the ill-effects of noise by enhancing the\nviolation of local realism or by transforming non-violating states to violating\nones, thereby acting as an activating agent.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\nWe investigate the consequence on the violation of local realism based on pseduospin operators when photons are added or subtracted in a single mode or in both the modes of the two-mode squeezed states of light in presence of noise.\n\nWe show that for addition (subtraction) of photons in a single mode, there is an overall enhancement in the maximal violation...\n\nFor any choice of parameters, we compare their entanglement contents.\n\nIn such a case, or in imperfect photon addition (subtraction), or under local noise, we find that the violation of local realism by the noise-affected two-mode squeezed states always decreases."
    },
    {
        "abstract": "  Joint models for longitudinal and time-to-event data are commonly used in\nlongitudinal studies to forecast disease trajectories over time. Despite the\nmany advantages of joint modeling, the standard forms suffer from limitations\nthat arise from a fixed model specification and computational difficulties when\napplied to large datasets. We adopt a deep learning approach to address these\nlimitations, enhancing existing methods with the flexibility and scalability of\ndeep neural networks while retaining the benefits of joint modeling. Using data\nfrom the Alzheimer's Disease Neuroimaging Institute, we show improvements in\nperformance and scalability compared to traditional methods.\n",
        "method": "We adopt a deep learning approach to address these limitations, enhancing existing methods with the flexibility and scalability of deep neural networks while retaining the benefits of joint modeling."
    },
    {
        "abstract": "  An efficient despeckling method using a quantum-inspired adaptive threshold\nfunction is presented for reducing noise of ultrasound images. In the first\nstep, the ultrasound image is decorrelated by an spectrum equalization\nprocedure due to the fact that speckle noise is neither Gaussian nor white. In\nfact, a linear filter is exploited to flatten the power spectral density (PSD)\nof the ultrasound image. Then, the proposed method shrinks complex wavelet\ncoefficients based on the quantum-inspired adaptive threshold function. The\nproposed approach has been used to denoise both real and simulated data sets\nand compare with other widely adopted techniques. Experimental results\ndemonstrate that the proposed method has a competitive performance to remove\nspeckle noise and can preserve details and textures of medical ultrasound\nimages.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"In fact, a linear filter is exploited to flatten the power spectral density (PSD) of the ultrasound image.\""
    },
    {
        "abstract": "  In this paper, we characterize the topological support in H\u007folder norm of the\nlaw of the solution to a stochastic wave equation with three-dimensional space\nvariable is proved. This note is a continuation of [9] and [10]. The result is\na consequence of an approximation theorem, in the convergence of probability,\nfor a sequence of evolution equations driven by a family of regularizations of\nthe driving noise. We extend two previous results on this subject. The first\nextension is that we cover the case of multiplicative noise and non-zero\ninitial conditions. The second extension is related to the covariance function\nassociated with the noise, here we follow the approach of Hu, Huang and Nualart\nand ask conditions in terms the of the mean H\u007folder continuity of such\ncovariance function.\n",
        "method": "The result is a consequence of an approximation theorem, in the convergence of probability, for a sequence of evolution equations driven by a family of regularizations of the driving noise."
    },
    {
        "abstract": "  In this paper, a deep learning (DL)-based sphere decoding algorithm is\nproposed, where the radius of the decoding hypersphere is learnt by a deep\nneural network (DNN). The performance achieved by the proposed algorithm is\nvery close to the optimal maximum likelihood decoding (MLD) over a wide range\nof signal-to-noise ratios (SNRs), while the computational complexity, compared\nto existing sphere decoding variants, is significantly reduced. This\nimprovement is attributed to DNN's ability of intelligently learning the radius\nof the hypersphere used in decoding. The expected complexity of the proposed\nDL-based algorithm is analytically derived and compared with existing ones. It\nis shown that the number of lattice points inside the decoding hypersphere\ndrastically reduces in the DL- based algorithm in both the average and\nworst-case senses. The effectiveness of the proposed algorithm is shown through\nsimulation for high-dimensional multiple-input multiple-output (MIMO) systems,\nusing high-order modulations.\n",
        "method": "Methodological sentences:\n\n* A deep learning (DL)-based sphere decoding algorithm is proposed, where the radius of the decoding hypersphere is learnt by a deep neural network (DNN)."
    },
    {
        "abstract": "  A graviton laser works, in principle, by the stimulated emission of coherent\ngravitons from a lasing medium. For significant amplification, we must have a\nvery long path length and/or very high densities. Black holes and the existence\nof weakly interacting sub-eV dark matter particles (WISPs) solve both of these\nobstacles. Orbiting trajectories for massless particles around black holes are\nwell understood \\cite{mtw} and allow for arbitrarily long graviton path\nlengths. Superradiance from Kerr black holes of WISPs can provide the\nsufficiently high density \\cite{ABH}. This suggests that black holes can act as\nefficient graviton lasers. Thus directed graviton laser beams have been emitted\nsince the beginning of the universe and give rise to new sources of\ngravitational wave signals. To be in the path of particularly harmfully\namplified graviton death rays will not be pleasant.\n",
        "method": "Here is the methodological sentence:\n\nFor significant amplification, we must have a very long path length and/or very high densities."
    },
    {
        "abstract": "  The aim of this article is to better understand the correspondence between\n$n$-cubic extensions and $3^n$-diagrams, which may be seen as non-abelian\nYoneda extensions, useful in (co)homology of non-abelian algebraic structures.\nWe study a higher-dimensional version of the coequaliser/kernel pair\nadjunction, which relates $n$-fold reflexive graphs with $n$-fold arrows in any\nexact Mal'tsev category. We first ask ourselves how this adjunction restricts\nto an equivalence of categories. This leads to the concept of an effective\n$n$-fold equivalence relation, corresponding to the $n$-fold regular\nepimorphisms. We characterise those in terms of what (when $n=2$) Bourn calls\nparallelistic $n$-fold equivalence relations. We then further restrict the\nequivalence, with the aim of characterising the $n$-cubic extensions. We find a\ncongruence distributivity condition, resulting in a denormalised $3^n$-Lemma\nvalid in exact Mal'tsev categories. We deduce a $3^n$-Lemma for short exact\nsequences in semi-abelian categories, which involves a distributivity condition\nbetween joins and meets of normal subobjects. This turns out to be new even in\nthe abelian case.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe study a higher-dimensional version of the coequaliser/ kernel pair adjunction, which relates n-fold reflexive graphs with n-fold arrows in any exact Mal'tsev category."
    },
    {
        "abstract": "  Deep neural networks (DNNs) have emerged as key enablers of machine learning.\nApplying larger DNNs to more diverse applications is an important challenge.\nThe computations performed during DNN training and inference are dominated by\noperations on the weight matrices describing the DNN. As DNNs incorporate more\nlayers and more neurons per layers, these weight matrices may be required to be\nsparse because of memory limitations. Sparse DNNs are one possible approach,\nbut the underlying theory is in the early stages of development and presents a\nnumber of challenges, including determining the accuracy of inference and\nselecting nonzero weights for training. Associative array algebra has been\ndeveloped by the big data community to combine and extend database, matrix, and\ngraph/network concepts for use in large, sparse data problems. Applying this\nmathematics to DNNs simplifies the formulation of DNN mathematics and reveals\nthat DNNs are linear over oscillating semirings. This work uses associative\narray DNNs to construct exact solutions and corresponding perturbation models\nto the rectified linear unit (ReLU) DNN equations that can be used to construct\ntest vectors for sparse DNN implementations over various precisions. These\nsolutions can be used for DNN verification, theoretical explorations of DNN\nproperties, and a starting point for the challenge of sparse training.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* \"Applying this mathematics to DNNs simplifies the formulation of DNN mathematics...\"\n* \"This work uses associative array DNNs to construct exact solutions and corresponding perturbation models...\""
    },
    {
        "abstract": "  The hull of linear codes have promising utilization in coding theory and\nquantum coding theory. In this paper, we study the hull of generalized\nReed-Solomon codes and extended generalized Reed-Solomon codes over finite\nfields with respect to the Euclidean inner product. Several infinite families\nof MDS codes with arbitrary dimensional hull are presented. As an application,\nusing these MDS codes with arbitrary dimensional hull, we construct several new\ninfinite families of entanglement-assisted quantum error-correcting codes with\nflexible parameters.\n",
        "method": "Several infinite families of MDS codes with arbitrary dimensional hull are presented."
    },
    {
        "abstract": "  Early detection of breast cancer can increase treatment efficiency.\nArchitectural Distortion (AD) is a very subtle contraction of the breast tissue\nand may represent the earliest sign of cancer. Since it is very likely to be\nunnoticed by radiologists, several approaches have been proposed over the years\nbut none using deep learning techniques. To train a Convolutional Neural\nNetwork (CNN), which is a deep neural architecture, is necessary a huge amount\nof data. To overcome this problem, this paper proposes a data augmentation\napproach applied to clinical image dataset to properly train a CNN. Results\nusing receiver operating characteristic analysis showed that with a very\nlimited dataset we could train a CNN to detect AD in digital mammography with\narea under the curve (AUC = 0.74).\n",
        "method": "To overcome this problem, this paper proposes a data augmentation approach applied to clinical image dataset to properly train a CNN."
    },
    {
        "abstract": "  We present a program synthesis-oriented dataset consisting of human written\nproblem statements and solutions for these problems. The problem statements\nwere collected via crowdsourcing and the program solutions were extracted from\nhuman-written solutions in programming competitions, accompanied by\ninput/output examples. We propose using this dataset for the program synthesis\ntasks aimed for working with real user-generated data. As a baseline we present\nfew models, with the best model achieving 8.8% accuracy, showcasing both the\ncomplexity of the dataset and large room for future research.\n",
        "method": "The problem statements were collected via crowdsourcing and the program solutions were extracted from human-written solutions in programming competitions, accompanied by input/output examples."
    },
    {
        "abstract": "  Spreading dynamics has been considered to take place in temporal networks,\nwhere temporal interaction patterns between nodes show non-Poissonian bursty\nnature. The effects of inhomogeneous interevent times (IETs) on the spreading\nhave been extensively studied in recent years, yet little is known about the\neffects of correlations between IETs on the spreading. In order to investigate\nthose effects, we study two-step deterministic susceptible-infected (SI) and\nprobabilistic SI dynamics when the interaction patterns are modeled by\ninhomogeneous and correlated IETs, i.e., correlated bursts. By analyzing the\ntransmission time statistics in a single-link setup and by simulating the\nspreading in Bethe lattices and random graphs, we conclude that the positive\ncorrelation between IETs slows down the spreading. We also argue that the\nshortest transmission time from one infected node to its susceptible neighbors\ncan successfully explain our numerical results.\n",
        "method": "The effects of correlations between interevent times on the spreading have been extensively studied in recent years, yet little is known about the effects of inhomogeneous interevent times on the spreading."
    },
    {
        "abstract": "  Input current estimation is indispensable in the sensorless control\nalgorithms for the problem of power factor compensation (PFC) of an AC-DC boost\nconverter. The system estimator design is challenged by the bilinear form\ndynamics and uncertain parameters of the system. In this paper, the system\ndynamics is immersed to a proper form by a new filtered transformation. Thanks\nto the proposed transformation, the input current, input voltage amplitude, and\nload conductance are globally estimated. The exponential convergent of the\nestimates is established in normal converter operation. An application of the\nproposed estimator is presented in conjunction with a well-known dynamic\ncontroller.\n",
        "method": "The system estimator design is challenged by the bilinear form dynamics and uncertain parameters of the system. Thanks to the proposed transformation, the input current, input voltage amplitude, and load conductance are globally estimated."
    },
    {
        "abstract": "  Phase-field model is a powerful mathematical tool to study the dynamics of\ninterface and morphology changes in fluid mechanics and material sciences.\nHowever, numerically solving a phase field model for a real problem is a\nchallenge task due to the non-convexity of the bulk energy and the small\ninterface thickness parameter in the equation. In this paper, we propose two\nstabilized second order semi-implicit linear schemes for the Allen-Cahn\nphase-field equation based on backward differentiation formula and\nCrank-Nicolson method, respectively. In both schemes, the nonlinear bulk force\nis treated explicitly with two second-order stabilization terms, which make the\nschemes unconditional energy stable and numerically efficient. By using a known\nresult of the spectrum estimate of the linearized Allen-Cahn operator and some\nregularity estimate of the exact solution, we obtain an optimal second order\nconvergence in time with a prefactor depending on the inverse of the\ncharacteristic interface thickness only in some lower polynomial order. Both\n2-dimensional and 3-dimensional numerical results are presented to verify the\naccuracy and efficiency of proposed schemes.\n",
        "method": "Here is the methodological sentence:\n\nWe propose two stabilized second-order semi-implicit linear schemes for the Allen-Cahn phase-field equation based on backward differentiation formula and Crank-Nicolson method, respectively."
    },
    {
        "abstract": "  We extend the analysis developed in [33] in order to prove convergence to\nconsensus results for a Cucker-Smale type model with hierarchical leadership\nand distributed delay. Flocking estimates are obtained for a general\ninteraction potential with divergent tail. We analyze also the model when the\nultimate leader can change its velocity. In this case we give a flocking result\nunder suitable conditions on the leader's acceleration.\n",
        "method": "We extend the analysis developed in [33] in order to prove convergence to consensus results for a Cucker-Smale type model with hierarchical leadership and distributed delay."
    },
    {
        "abstract": "  Alzheimer's disease is the most common dementia leading to an irreversible\nneurodegenerative process. To date, subject revealed advanced brain structural\nalterations when the diagnosis is established. Therefore, an earlier diagnosis\nof this dementia is crucial although it is a challenging task. Recently, many\nstudies have proposed biomarkers to perform early detection of Alzheimer's\ndisease. Some of them have proposed methods based on inter-subject similarity\nwhile other approaches have investigated framework using intra-subject\nvariability. In this work, we propose a novel framework combining both\napproaches within an efficient graph of brain structures grading. Subsequently,\nwe demonstrate the competitive performance of the proposed method compared to\nstate-of-the-art methods.\n",
        "method": "To date, subject revealed advanced brain structural alterations when the diagnosis is established."
    },
    {
        "abstract": "  Synchronization of finite spike sequences is the way two brain regions\ncompare their content and extract the most suitable sequence. This is the core\nof the linguistic comparison between a word and a previous one retrieved by\nmemory. Classifying the information content of neural spike trains, an\nuncertainty relation emerges between the bit size of a word and its duration.\nThis uncertainty affects the task of synchronizing spike trains of different\nduration representing different words, entailing the occurrence of entangled\nsequences, so that word comparison amounts to a measurement based quantum\ncomputation. Entanglement explains the inverse Bayes inference that connects\ndifferent words in a linguistic search. The behaviour here discussed provides\nan explanation for other reported evidences of quantum effects in human\ncognitive processes lacking a plausible framework, since either no assignment\nof an appropriate quantum constant had been associated, or speculating on\nmicroscopic processes dependent on Planck's constant resulted in unrealistic\nde-coherence times.\n",
        "method": "Classifying the information content of neural spike trains, an uncertainty relation emerges between the bit size of a word and its duration."
    },
    {
        "abstract": "  We investigate a new case of rigidity in stable homotopy theory which is the\nrigidity of the $K(1)$-local stable homotopy category\n$\\mathrm{Ho}(L_{K(1)}\\mathrm{Sp})$ at $p=2$. In other words, we show that\nrecovering higher homotopy information by just looking at the triangulated\nstructure of $\\mathrm{Ho}(L_{K(1)}\\mathrm{Sp})$ is possible, which is a\nproperty that only few interesting stable model categories are known to\npossess.\n",
        "method": "Here is the methodological sentence:\n\nWe investigate a new case of rigidity in stable homotopy theory..."
    },
    {
        "abstract": "  The influence of contrarians on the noisy voter model is studied at the\nmean-field level. The noisy voter model is a variant of the voter model where\nagents can adopt two opinions, optimistic or pessimistic, and can change them\nby means of an imitation (herding) and an intrinsic (noise) mechanisms. An\nensemble of noisy voters undergoes a finite-size phase transition, upon\nincreasing the relative importance of the noise to the herding, form a bimodal\nphase where most of the agents shear the same opinion to a unimodal phase where\nalmost the same fraction of agent are in opposite states. By the inclusion of\ncontrarians we allow for some voters to adopt the opposite opinion of other\nagents (anti-herding). We first consider the case of only contrarians and show\nthat the only possible steady state is the unimodal one. More generally, when\nvoters and contrarians are present, we show that the bimodal-unimodal\ntransition of the noisy voter model prevails only if the number of contrarians\nin the system is smaller than four, and their characteristic rates are small\nenough. For the number of contrarians bigger or equal to four, the voters and\nthe contrarians can be seen only in the unimodal phase. Moreover, if the number\nof voters and contrarians, as well as the noise and herding rates, are of the\nsame order, then the probability functions of the steady state are very well\napproximated by the Gaussian distribution.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We first consider the case of only contrarians and show that the only possible steady state is the unimodal one.\""
    },
    {
        "abstract": "  We measure transport through a Ga[Al]As heterostructure at temperatures\nbetween 0.1 K and 30 K. Increasing the temperature enhances the\nelectron-electron scattering rate and viscous effects in the two-dimensional\nelectron gas arise. To probe this regime we measure so-called vicinity voltages\nand use a voltage-biased scanning tip to induce a movable local perturbation.\nWe find that the scanning gate images differentiate reliably between the\ndifferent regimes of electron transport. Our data are in good agreement with\nrecent theories for interacting electron liquids in the ballistic and viscous\nregimes stimulated by measurements in graphene. However, the range of\ntemperatures and densities where viscous effects are observable in Ga[Al]As are\nvery distinct from the graphene material system.\n",
        "method": "We measure transport through a Ga[Al]As heterostructure at temperatures between 0.1 K and 30 K."
    },
    {
        "abstract": "  We use a self-assembled two-dimensional Coulomb crystal of $\\sim 70$ ions in\nthe presence of an external transverse field to engineer a simulator of the\nDicke Hamiltonian, an iconic model in quantum optics which features a quantum\nphase transition between a superradiant/ferromagnetic and a normal/paramagnetic\nphase. We experimentally implement slow quenches across the quantum critical\npoint and benchmark the dynamics and the performance of the simulator through\nextensive theory-experiment comparisons which show excellent agreement. The\nimplementation of the Dicke model in fully controllable trapped ion arrays can\nopen a path for the generation of highly entangled states useful for enhanced\nmetrology and the observation of scrambling and quantum chaos in a many-body\nsystem.\n",
        "method": "We use a self-assembled two-dimensional Coulomb crystal of $\\sim 70$ ions in the presence of an external transverse field to engineer a simulator of the Dicke Hamiltonian..."
    },
    {
        "abstract": "  YouTube presents an unprecedented opportunity to explore how machine learning\nmethods can improve healthcare information dissemination. We propose an\ninterdisciplinary lens that synthesizes machine learning methods with\nhealthcare informatics themes to address the critical issue of developing a\nscalable algorithmic solution to evaluate videos from a health literacy and\npatient education perspective. We develop a deep learning method to understand\nthe level of medical knowledge encoded in YouTube videos. Preliminary results\nsuggest that we can extract medical knowledge from YouTube videos and classify\nvideos according to the embedded knowledge with satisfying performance. Deep\nlearning methods show great promise in knowledge extraction, natural language\nunderstanding, and image classification, especially in an era of\npatient-centric care and precision medicine.\n",
        "method": "We develop a deep learning method to understand the level of medical knowledge encoded in YouTube videos."
    },
    {
        "abstract": "  We study the development of heavy-flavor flow harmonics in high-energy\nnuclear collisions. The elliptic and triangular flow of heavy-flavor hadrons,\narising from the finite impact parameter of the two nuclei and from\nevent-by-event fluctuations of the initial geometry, is analyzed in detail,\nconsidering the contribution from particles decoupling from the fireball at\nvarious times. We also study the dependence of the flow harmonics on the\nevent-shape fluctuations, considering events belonging to the same centrality\nclass but characterized by very different eccentricities (or vice-versa).\n",
        "method": "The methodological sentences are:\n\n* The elliptic and triangular flow of heavy-flavor hadrons, arising from the finite impact parameter of the two nuclei and from event-by-event fluctuations of the initial geometry, is analyzed in detail...\n* We also study the dependence of the flow harmonics on the event-shape fluctuations, considering events belonging to the same centrality class but characterized by very different eccentricities (or vice-versa)."
    },
    {
        "abstract": "  In this paper, the first of two, we introduce an alternative definition of\nthe Chang--Weinberger--Yu relative higher index, which is thought of as a\nrelative analogue of the Mishchenko--Fomenko index pairing. A main result of\nthis paper is that our map coincides with the existing relative higher index\nmaps. We make use of this fact for understanding the relative higher index.\nFirst, we relate the relative higher index with the higher index of amalgamated\nfree product groups. Second, we define the dual relative higher index map and\nshow its rational surjectivity under certain assumptions.\n",
        "method": "We make use of this fact for understanding the relative higher index."
    },
    {
        "abstract": "  In this paper we consider the generalized approximate message passing (GAMP)\nalgorithm for recovering a sparse signal from modulo samples of randomized\nprojections of the unknown signal. The modulo samples are obtained by a\nself-reset (SR) analog to digital converter (ADC). Additionally, in contrast to\nprevious work on SR ADC, we consider a scenario where the compressed sensing\n(CS) measurements (i.e., randomized projections) are sent through a\ncommunication channel, namely an additive white Gaussian noise (AWGN) channel\nbefore being quantized by a SR ADC. To show the effectiveness of the proposed\napproach, we conduct Monte-Carlo (MC) simulations for both noiseless and noisy\ncase. The results show strong ability of the proposed algorithm to fight the\nnonlinearity of the SR ADC, as well as the possible additional distortion\nintroduced by the AWGN channel.\n",
        "method": "The methodological sentence is:\n\n\"We conduct Monte-Carlo (MC) simulations for both noiseless and noisy case.\""
    },
    {
        "abstract": "  We present the continuous wavelet transform (WT) of white Gaussian noise and\nestablish a connection to the theory of Gaussian analytic functions. Based on\nthis connection, we propose a methodology that detects components of a signal\nin white noise based on the distribution of the zeros of its continuous WT. To\nillustrate that the continuous theory can be employed in a discrete setting, we\nestablish a uniform convergence result for the discretized continuous WT and\napply the proposed method to a variety of acoustic signals.\n",
        "method": "We propose a methodology that detects components of a signal in white noise based on the distribution of the zeros of its continuous WT."
    },
    {
        "abstract": "  Predict a new response from a covariate is a challenging task in regression,\nwhich raises new question since the era of high-dimensional data. In this\npaper, we are interested in the inverse regression method from a theoretical\nviewpoint. Theoretical results have already been derived for the well-known\nlinear model, but recently, the curse of dimensionality has increased the\ninterest of practitioners and theoreticians into generalization of those\nresults for various estimators, calibrated for the high-dimension context. To\ndeal with high-dimensional data, inverse regression is used in this paper. It\nis known to be a reliable and efficient approach when the number of features\nexceeds the number of observations. Indeed, under some conditions, dealing with\nthe inverse regression problem associated to a forward regression problem\ndrastically reduces the number of parameters to estimate and make the problem\ntractable. When both the responses and the covariates are multivariate,\nestimators constructed by the inverse regression are studied in this paper, the\nmain result being explicit asymptotic prediction regions for the response. The\nperformances of the proposed estimators and prediction regions are also\nanalyzed through a simulation study and compared with usual estimators.\n",
        "method": "Here is the methodological sentence:\n\nTo deal with high-dimensional data, inverse regression is used in this paper."
    },
    {
        "abstract": "  NA61/SHINE is a fixed target experiment at the CERN Super-Proton-\nSynchrotron. The main goals of the experiment are to discover the critical\npoint of strongly interacting matter and to study the properties of the onset\nof deconfinement. In order to reach these goals, a study of hadron production\nproperties is performed in nucleus-nucleus, proton-proton and proton-nucleus\ninteractions as a function of collision energy and size of the colliding\nnuclei. In this paper, I will review recent results on strangeness production\nin p+p, Be+Be and Ar+Sc collisions in the SPS energy range. Kinematic spectra\nand mean multiplicities of kaons obtained with various analysis methods will be\nshown. An overview of strangeness production and its dependence on system size\nin the vicinity of the phase transition will be presented as well.\n",
        "method": "Here is the methodological sentence:\n\nA study of hadron production properties is performed in nucleus-nucleus, proton-proton and proton-nucleus interactions as a function of collision energy and size of the colliding nuclei."
    },
    {
        "abstract": "  We theoretically investigate the critical current of a thermally-biased SIS\nJosephson junction formed by electrodes made by different BCS superconductors.\nThe response of the device is analyzed as a function of the asymmetry\nparameter, $r=T_{c_1} /T_{c_2}$. We highlight the appearance of jumps in the\ncritical current of an asymmetric junction, namely, when $r\\neq1$. In fact, in\nsuch case at temperatures at which the BCS superconducting gaps coincide, the\ncritical current suddenly increases or decreases. In particular, we thoroughly\ndiscuss the counterintuitively behaviour of the critical current, which\nincreases by enhancing the temperature of one lead, instead of monotonically\nreducing. In this case, we found that the largest jump of the critical current\nis obtained for moderate asymmetries, $r\\simeq3$. In view of these results, the\ndiscussed behavior can be speculatively proposed as a temperature-based\nthreshold single-photon detector with photon-counting capabilities, which\noperates non-linearly in the non-dissipative channel.\n",
        "method": "Here is the methodological sentence:\n\nWe theoretically investigate the critical current of a thermally-biased SIS Josephson junction formed by electrodes made by different BCS superconductors."
    },
    {
        "abstract": "  In this paper, we propose and develop an optimal nonconforming finite element\nmethod for the Stokes equations approximated by the Crouzix-Raviart element for\nvelocity and the continuous linear element for pressure. Previous result in\nusing the stabilization method for this finite element pair is improved and\nthen proven to be stable. Then, optimal order error estimate is obtained and\nnumerical results show the accuracy and robustness of the method.\n",
        "method": "Previous result in using the stabilization method for this finite element pair is improved and then proven to be stable."
    },
    {
        "abstract": "  We have explored some phenomenological issues during calculations of\ntransport coefficients for hadronic matter, produced in the experiments of\nheavy ion collisions. Here, we have used an ideal hadron resonance gas model to\ndemonstrate the issues. On the basis of dissipation mechanism, the hadronic zoo\nis classified into resonance and non-resonance members, who participate in\ndissipation via strong decay and scattering channels respectively. Imposing our\nphenomenological restriction, we are able to provide a rough upper and lower\nbound estimations of transport coefficients. Interestingly, we find that our\nproposed lower limit estimation for shear viscosity to entropy density ratio is\nlittle larger than its quantum lower bound. By taking a simple example, we have\ndemonstrated how our proposed restriction help to tune any estimation of\ntransport coefficients within its numerical band, proposed by us.\n",
        "method": "We used an ideal hadron resonance gas model to demonstrate phenomenological issues during calculations of transport coefficients for hadronic matter, produced in the experiments of heavy ion collisions."
    },
    {
        "abstract": "  Let $R$ be a polynomial ring and $I \\subset R$ be a perfect ideal of height\ntwo minimally generated by forms of the same degree. We provide a formula for\nthe multiplicity of the saturated special fiber ring of $I$. Interestingly,\nthis formula is equal to an elementary symmetric polynomial in terms of the\ndegrees of the syzygies of $I$. Applying ideas introduced in arXiv:1805.05180,\nwe obtain the value of the j-multiplicity of $I$ and an effective method for\ndetermining the degree and birationality of rational maps defined by\nhomogeneous generators of $I$.\n",
        "method": "Here is the methodological sentence:\n\nWe provide a formula for the multiplicity of the saturated special fiber ring of $I$."
    },
    {
        "abstract": "  This article is devoted to the study of a higher-dimensional generalisation\nof de Rham epsilon lines. To a holonomic $D$-module $M$ on a smooth variety $X$\nand a generic tuple of $1$-form $(\\nu_1,\\dots,\\nu_n)$, we associate a point of\nthe $K$-theory space $K(X,Z)$. If $X$ is proper this $K$-theory class is\nrelated to the de Rham cohomology $R\\Gamma_{dR}(X,M)$. The novel feature of our\nconstruction is that $Z$ is allowed to be of dimension $0$. Furthermore, we\nallow the tuple of $1$-forms to vary in families, and observe that this leads\nnaturally to a crystal akin to the epsilon connection for curves. Our approach\nis based on combining a construction of Patel with a homotopy invariance\nproperty of algebraic $K$-theory with respect to $(\\mathbb{P}^1,\\infty)$. This\nhomotopical viewpoint leads us naturally to the definition of an epsilon\nconnection in higher dimensions. Along the way we prove the compatibility of\nPatel's epsilon factors with the graded lines defined by Deligne and\nBeilinson--Bloch--Esnault in the case of curves.\n",
        "method": "To a holonomic $D$-module $M$ on a smooth variety $X$ and a generic tuple of 1-form $(\\nu_1,\\dots,\\nu_n)$, we associate a point of the $K$-theory space $K(X,Z)$."
    },
    {
        "abstract": "  We present a framework for accelerated iterative reconstructions using a fast\nand approximate forward model that is based on k-space methods for\nphotoacoustic tomography. The approximate model introduces aliasing artefacts\nin the gradient information for the iterative reconstruction, but these\nartefacts are highly structured and we can train a CNN that can use the\napproximate information to perform an iterative reconstruction. We show\nfeasibility of the method for human in-vivo measurements in a limited-view\ngeometry. The proposed method is able to produce superior results to total\nvariation reconstructions with a speed-up of 32 times.\n",
        "method": "We present a framework for accelerated iterative reconstructions using a fast and approximate forward model that is based on k-space methods for photoacoustic tomography."
    },
    {
        "abstract": "  Much of modern practice in financial forecasting relies on technicals, an\numbrella term for several heuristics applying visual pattern recognition to\nprice charts. Despite its ubiquity in financial media, the reliability of its\nsignals remains a contentious and highly subjective form of 'domain knowledge'.\nWe investigate the predictive value of patterns in financial time series,\napplying machine learning and signal processing techniques to 22 years of US\nequity data. By reframing technical analysis as a poorly specified, arbitrarily\npreset feature-extractive layer in a deep neural network, we show that better\nconvolutional filters can be learned directly from the data, and provide visual\nrepresentations of the features being identified. We find that an ensemble of\nshallow, thresholded CNNs optimised over different resolutions achieves\nstate-of-the-art performance on this domain, outperforming technical methods\nwhile retaining some of their interpretability.\n",
        "method": "Here is the methodological sentence:\n\nWe investigate the predictive value of patterns in financial time series, applying machine learning and signal processing techniques to 22 years of US equity data."
    },
    {
        "abstract": "  We report the first analysis of a flash produced by the impact of a meteoroid\non the lunar surface and recorded both in the near-infrared and in the visible.\nDespite the fact that similar data have been recently published by other team\nduring the refereeing process of our manuscript (Bonanos et al. 2018), our\nresult still forms the first measurement of the temperature of a telescopic\nlunar impact flash (Madiedo and Ortiz 2016, 2018). The flash exhibited a peak\nmagnitude of 5.1 $\\pm$ 0.3 in the near-infrared I band and 7.3 $\\pm$ 0.2 in the\nvisible, and the total duration of the event in these bands was 0.20 s and 0.18\ns, respectively. The origin of the meteoroid was investigated, and we inferred\nthat the most likely scenario is that the impactor that belonged to the\nsporadic background. The analysis of this event has provided for the first time\nan estimation of the emission efficiency in the near-infrared {\\eta}I for\nsporadic meteoroids impacting the Moon. We have determined that this efficiency\nis around 56% higher than in the visible band and we have found a maximum\nimpact plume temperature of ~4000 K at the initial phase followed by\ntemperatures of around 3200 K after the peak brightness. The size of the crater\nproduced as a consequence of this impact is also calculated.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We report the first analysis of a flash produced by the impact of a meteoroid on the lunar surface and recorded both in the near-infrared and in the visible.\n* The origin of the meteoroid was investigated, and we inferred that the most likely scenario is that the impactor that belonged to the sporadic background.\n* We have determined that this efficiency [emission efficiency] is around 56% higher than in the visible band...\n* The size of the crater produced as a consequence of this impact is also calculated."
    },
    {
        "abstract": "  We present and analyze a proposal for a macroscopic quantum delayed-choice\nexperiment with massive mechanical resonators. In our approach, the electronic\nspin of a single nitrogen-vacancy impurity is employed to control the coherent\ncoupling between the mechanical modes of two carbon nanotubes. We demonstrate\nthat a mechanical phonon can be in a coherent superposition of wave and\nparticle, thus exhibiting both behaviors at the same time. We also discuss the\nmechanical noise tolerable in our proposal and predict a critical temperature\nbelow which the morphing between wave and particle states can be effectively\nobserved in the presence of environment-induced fluctuations. Furthermore, we\ndescribe how to amplify single-phonon excitations of the mechanical-resonator\nsuperposition states to a macroscopic level, via squeezing the mechanical\nmodes. This approach corresponds to the phase-covariant cloning. Therefore, our\nproposal can serve as a test of macroscopic quantum superpositions of massive\nobjects even with large excitations. This work, which describes a fundamental\ntest of the limits of quantum mechanics at the macroscopic scale, would have\nimplications for quantum metrology and quantum information processing.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In our approach, the electronic spin of a single nitrogen-vacancy impurity is employed to control the coherent coupling between the mechanical modes of two carbon nanotubes.\n* We demonstrate that a mechanical phonon can be in a coherent superposition of wave and particle...\n* We also discuss the mechanical noise tolerable in our proposal..."
    },
    {
        "abstract": "  In the framework of the chiral quark model, we investigate the hidden strange\npentaquark system of the $N\\phi$ state with the quantum numbers of\nIJ=$\\frac{1}{2}$$\\frac{3}{2}$. The results show that the $N\\phi$ state can be\nbound through the interaction of the $\\sigma$ meson exchange plus the effect of\nthe channel coupling, which means that the effect of the channel coupling has\nan influence on the existence of this bound state.\n",
        "method": "We investigate the hidden strange pentaquark system of the N\u03c6 state with the quantum numbers of IJ=\u00bd\u00b3/2 in the framework of the chiral quark model."
    },
    {
        "abstract": "  Detecting exoplanets in clusters of different ages is a powerful tool for\nunderstanding a number of open questions, such as how the occurrence rate of\nplanets depends on stellar metallicity, on mass, or on stellar environment. We\npresent the first results of our HARPS long-term radial velocity (RV) survey\nwhich aims to discover exoplanets around intermediate-mass (between ~ 2 and 6\nMsun) evolved stars in open clusters. We selected 826 bona fide HARPS\nobservations of 114 giants from an initial list of 29 open clusters and\ncomputed the half peak-to-peak variability of the HARPS RV measurements, namely\nDeltaRV/2, for each target, to search for the best planet-host candidates. We\nalso performed time series analysis for a few targets with enough observations\nto search for orbital solutions. Although we attempted to rule out the presence\nof binaries on the basis of previous surveys, we detected 14 new binary\ncandidates in our sample, most of them identified from a comparison between\nHARPS and CORAVEL data. We also suggest 11 new planet-host candidates based on\na relation between the stellar surface gravity and DeltaRV/2. Ten of the\ncandidates have less than 3 Msun, showing evidence of a low planet occurrence\nrate for massive stars. One of the planet-host candidates and one of the binary\ncandidates show very clear RV periodic variations, allowing us to confirm the\ndiscovery of a new planet and to compute the orbital solution for the binary.\nThe planet is IC 4651 9122b, with a minimum mass of msini = 6.3 MJ and a\nsemi-major axis a = 2.0 AU. The binary companion is NGC 5822 201B, with a very\nlow minimum mass of msini = 0.11 Msun and a semi-major axis a = 6.5 AU, which\nis comparable to the Jupiter distance to the Sun.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We selected 826 bona fide HARPS observations of 114 giants from an initial list of 29 open clusters and computed the half peak-peak variability of the HARPS RV measurements, namely DeltaRV/2, for each target, to search for the best planet-host candidates.\n* We also performed time series analysis for a few targets with enough observations to search for orbital solutions.\n* Although we attempted to rule out the presence of binaries on the basis of previous surveys, we detected 14 new binary candidates in our sample, most of them identified from a comparison between HARPS and CORAVEL data."
    },
    {
        "abstract": "  Enterprise application integration (EAI) solutions are the centrepiece of\ncurrent enterprise IT architectures (e.g., cloud and mobile computing, business\nnetworks), however, require the formalization of their building blocks,\nrepresented by integration patterns, verification and optimization. This work\nserves as an instructive pattern formalization catalog that leads to the\nformalization of all currently known integration patterns. Therefore, we\nexplain the classification of the underlying requirements of the pattern\nsemantics and formalize representative patterns from the different categories,\nby realizing them in timed db-net. In this way, the catalog will allow for the\naddition of future patterns by assigning them to a category and applying the\ndescribed formalism.\n",
        "method": "This work serves as an instructive pattern formalization catalog that leads to the formalization of all currently known integration patterns."
    },
    {
        "abstract": "  We study the statistical properties of jump processes in a bounded domain\nthat are driven by Poisson white noise. We derive the corresponding\nKolmogorov-Feller equation and provide a general representation for its\nstationary solutions. Exact stationary solutions of this equation are found and\nanalyzed in two particular cases. All our analytical findings are confirmed by\nnumerical simulations.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe derive the corresponding Kolmogorov-Feller equation..."
    },
    {
        "abstract": "  Reduced Rank Extrapolation (RRE) is a polynomial type method used to\naccelerate the convergence of sequences of vectors $\\{\\boldsymbol{x}_m\\}$. It\nis applied successfully in different disciplines of science and engineering in\nthe solution of large and sparse systems of linear and nonlinear equations of\nvery large dimension. If $\\boldsymbol{s}$ is the solution to the system of\nequations $\\boldsymbol{x}=\\boldsymbol{f}(\\boldsymbol{x})$, first, a vector\nsequence $\\{\\boldsymbol{x}_m\\}$ is generated via the fixed-point iterative\nscheme $\\boldsymbol{x}_{m+1}=\\boldsymbol{f}(\\boldsymbol{x}_m)$, $m=0,1,\\ldots,$\nand next, RRE is applied to this sequence to accelerate its convergence. RRE\nproduces approximations $\\boldsymbol{s}_{n,k}$ to $\\boldsymbol{s}$ that are of\nthe form $\\boldsymbol{s}_{n,k}=\\sum^k_{i=0}\\gamma_i\\boldsymbol{x}_{n+i}$ for\nsome scalars $\\gamma_i$ depending (nonlinearly) on $\\boldsymbol{x}_n,\n\\boldsymbol{x}_{n+1},\\ldots,\\boldsymbol{x}_{n+k+1}$ and satisfying\n$\\sum^k_{i=0}\\gamma_i=1$. The convergence properties of RRE when applied in\nconjunction with linear $\\boldsymbol{f}(\\boldsymbol{x})$ have been analyzed in\ndifferent publications. In this work, we discuss the convergence of the\n$\\boldsymbol{s}_{n,k}$ obtained from RRE with nonlinear\n$\\boldsymbol{f}(\\boldsymbol{x})$ (i)\\,when $n\\to\\infty$ with fixed $k$, and\n(ii)\\,in two so-called {\\em cycling} modes.\n",
        "method": "The methodological sentence is:\n\nFirst, a vector sequence $\\{\\boldsymbol{x}_m\\}$ is generated via the fixed-point iterative scheme $\\boldsymbol{x}_{m+1}=\\boldsymbol{f}(\\boldsymbol{x}_m)$, $m=0,1,\\ldots,$ and next, RRE is applied to this sequence to accelerate its convergence."
    },
    {
        "abstract": "  CodRep is a machine learning competition on source code data. It is carefully\ndesigned so that anybody can enter the competition, whether professional\nresearchers, students or independent scholars, without specific knowledge in\nmachine learning or program analysis. In particular, it aims at being a common\nplayground on which the machine learning and the software engineering research\ncommunities can interact. The competition has started on April 14th 2018 and\nhas ended on October 14th 2018. The CodRep data is hosted at\nhttps://github.com/KTH/CodRep-competition/.\n",
        "method": "None. There are no methodological sentences in this abstract."
    },
    {
        "abstract": "  3D printing technologies are currently enabling the fabrication of objects\nwith complex architectures and tailored properties. In such framework, the\nproduction of 3D optical structures, which are typically based on optical\ntransparent matrices, optionally doped with active molecular compounds and\nnanoparticles, is still limited by the poor uniformity of the printed\nstructures. Both bulk inhomogeneities and surface roughness of the printed\nstructures can negatively affect the propagation of light in 3D printed optical\ncomponents. Here we investigate photopolymerization-based printing processes by\nlaser confocal microscopy. The experimental method we developed allows the\nprinting process to be investigated in-situ, with microscale spatial\nresolution, and in real-time. The modelling of the photo-polymerization\nkinetics allows the different polymerization regimes to be investigated and the\ninfluence of process variables to be rationalized. In addition, the origin of\nthe factors limiting light propagation in printed materials are rationalized,\nwith the aim of envisaging effective experimental strategies to improve optical\nproperties of printed materials.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The experimental method we developed allows the printing process to be investigated in- situ, with microscale spatial resolution, and in real-time.\""
    },
    {
        "abstract": "  We show that a Wilson-type discretization of the Gross-Neveu model, a\nfermionic N-flavor quantum field theory displaying asymptotic freedom and\nchiral symmetry breaking, can serve as a playground to explore correlated\nsymmetry-protected phases of matter using techniques borrowed from high-energy\nphysics. A large- N study, both in the Hamiltonian and Euclidean formalisms,\nyields a phase diagram with trivial, topological, and symmetry-broken phases\nseparated by critical lines that meet at a tri-critical point. We benchmark\nthese predictions using tools from condensed matter and quantum information\nscience, which show that the large-N method captures the essence of the phase\ndiagram even at N = 1. Moreover, we describe a cold-atom scheme for the quantum\nsimulation of this lattice model, which would allow to explore the\nsingle-flavor phase diagram.\n",
        "method": "A large- N study, both in the Hamiltonian and Euclidean formalisms, yields a phase diagram with trivial, topological, and symmetry-broken phases separated by critical lines that meet at a tri-critical point."
    },
    {
        "abstract": "  We describe four classical undergraduate physics experiments that were done\nwith everyday objects and low-cost sensors: mechanical oscillations,\ntransmittance of light through a slab of matter, beam deformation under load,\nand thermal relaxation due to heat loss. We used these experiments to train\nstudents for experimental homework projects but they could be used and expanded\nin a variety of contexts: lecture demonstrations, low cost students' labs,\nscience projects, distance learning courses...\n",
        "method": "Here is the methodological sentence:\n\nWe describe four classical undergraduate physics experiments that were done with everyday objects and low-cost sensors."
    },
    {
        "abstract": "  The photocatalytic splitting of water into molecular hydrogen and molecular\noxygen with sunlight is the dream reaction for solar energy conversion. Since\ndecades, transition-metal-oxide semiconductors and supramolecular\norganometallic structures have been extensively explored as photocatalysts for\nsolar water splitting. More recently, polymeric carbon nitride materials\nconsisting of triazine or heptazine building blocks have attracted considerable\nattention as hydrogen-evolution photocatalysts. The mechanism of hydrogen\nevolution with polymeric carbon nitrides is discussed throughout the current\nliterature in terms of the familiar concepts developed for photoelectrochemical\nwater splitting with semiconductors since the 1970s. We discuss in this\nperspective an alternative mechanistic paradigm for photoinduced water\nsplitting with carbon nitrides, which focusses on the specific features of the\nphotochemistry of aromatic N-heterocycles in aqueous environments. It is shown\nthat a water molecule which is hydrogen-bonded to an N-heterocycle can be\ndecomposed into hydrogen and hydroxyl radicals by two simple sequential\nphotochemical reactions. This concept is illustrated by first-principles\ncalculations of excited-state reaction paths and their energy profiles for\nhydrogen-bonded complexes of pyridine, triazine and heptazine with a water\nmolecule.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* The mechanism of hydrogen evolution with polymeric carbon nitrides is discussed throughout the current literature in terms of the familiar concepts developed for photoelectrochemical water splitting with semiconductors since the 1970s.\n* It is shown that a water molecule which is hydrogen-bonded to an N-heterocycle can be decomposed into hydrogen and hydroxyl radicals by two simple sequential photochemical reactions.\n* This concept is illustrated by first-principles calculations of excited-state reaction paths and their energy profiles for hydrogen-bonded complexes of pyridine, triazine and heptazine with a water molecule."
    },
    {
        "abstract": "  This paper deals with bandit online learning problems involving feedback of\nunknown delay that can emerge in multi-armed bandit (MAB) and bandit convex\noptimization (BCO) settings. MAB and BCO require only values of the objective\nfunction involved that become available through feedback, and are used to\nestimate the gradient appearing in the corresponding iterative algorithms.\nSince the challenging case of feedback with \\emph{unknown} delays prevents one\nfrom constructing the sought gradient estimates, existing MAB and BCO\nalgorithms become intractable. For such challenging setups, delayed\nexploration, exploitation, and exponential (DEXP3) iterations, along with\ndelayed bandit gradient descent (DBGD) iterations are developed for MAB and\nBCO, respectively. Leveraging a unified analysis framework, it is established\nthat the regret of DEXP3 and DBGD are ${\\cal O}\\big( \\sqrt{K\\bar{d}(T+D)}\n\\big)$ and ${\\cal O}\\big( \\sqrt{K(T+D)} \\big)$, respectively, where $\\bar{d}$\nis the maximum delay and $D$ denotes the delay accumulated over $T$ slots.\nNumerical tests using both synthetic and real data validate the performance of\nDEXP3 and DBGD.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nFor such challenging setups, delayed exploration, exploitation, and exponential (DEXP3) iterations, along with delayed bandit gradient descent (DBGD) iterations are developed for MAB and BCO, respectively."
    },
    {
        "abstract": "  Black-box quantum state preparation is an important subroutine in many\nquantum algorithms. The standard approach requires the quantum computer to do\narithmetic, which is a key contributor to the complexity. Here we present a new\nalgorithm that avoids arithmetic. We thereby reduce the number of gates by a\nfactor of 286-374 over the best prior work for realistic precision; the\nimprovement factor increases with the precision. As quantum state preparation\nis a crucial subroutine in many approaches to simulating physics on a quantum\ncomputer, our new method brings useful quantum simulation closer to reality.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe standard approach requires the quantum computer to do arithmetic, which is a key contributor to the complexity."
    },
    {
        "abstract": "  The main goal of this study is to investigate the LF of a sample of 142 X-ray\nselected clusters, with spectroscopic redshift confirmation and a well defined\nselection function, spanning a wide redshift and mass range, and to test the LF\ndependence on cluster global properties, in a homogeneous and unbiased way. Our\nstudy is based on the Canada-France-Hawaii Telescope Legacy Survey (CFHTLS)\nphotometric galaxy catalogue,associated with photometric redshifts. We\nconstructed LFs inside a scaled radius using a selection in photometric\nredshift around the cluster spectroscopic redshift in order to reduce\nprojection effects. The width of the photometric redshift selection was\ncarefully determined to avoid biasing the LF and depended on both the cluster\nredshift and the galaxy magnitudes. The purity was then enhanced by applying a\nprecise background subtraction. We constructed composite luminosity functions\n(CLFs) by stacking the individual LFs and studied their evolution with redshift\nand richness, analysing separately the brightest cluster galaxy (BCG) and\nnon-BCG members. We fitted the dependences of the CLFs and BCG distributions\nparameters with redshift and richness conjointly in order to distinguish\nbetween these two effects. We find that the usual photometric redshift\nselection methods can bias the LF estimate if the redshift and magnitude\ndependence of the photometric redshift quality is not taken into account. Our\nmain findings concerning the evolution of the galaxy luminosity distribution\nwith redshift and richness are that, in the inner region of clusters and in the\nredshift-mass range we probe (about $0<z<1$ and $10^{13}\nM_{\\odot}<M_{500}<5\\times10^{14}M_{\\odot}$), the bright part of the LF (BCG\nexcluded) does not depend much on mass or redshift except for its amplitude,\nwhereas the BCG luminosity increases both with redshift and richness, and its\nscatter decreases with redshift.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We constructed LFs inside a scaled radius using a selection in photometric redshift around the cluster spectroscopic redshift in order to reduce projection effects.\n* The width of the photometric redshift selection was carefully determined to avoid biasing the LF and depended on both the cluster redshift and the galaxy magnitudes.\n* We applied a precise background subtraction to enhance the purity.\n* We constructed composite luminosity functions (CLFs) by stacking the individual LFs and studied their evolution with redshift and richness, analysing separately the brightest cluster galaxy (BCG) and non-BCG members.\n* We fitted the dependences of the CLFs and BCG distributions parameters with redshift and richness conjointly in order to distinguish between these two effects."
    },
    {
        "abstract": "  The minimal supersymmetric standard model is a popular and well-motivated\nextension of the standard model. As such, it has been constrained by a large\nnumber of different experimental searches. To truly assess the impacts of these\nexperiments on the model one must perform a global fit, scanning over the\nmulti-dimensional parameter space and combining all the data in a statistically\nrigorous manner. In this talk, I presented results from global fits of\nsupersymmetric models performed with GAMBIT, the Global and Modular\nBeyond-the-Standard-Model (BSM) Inference Tool. I showed MSSM results from the\nlatest GAMBIT papers, as well as exciting preliminary results from a dedicated\nstudy of the collider constraints on the electroweakino sector.\n",
        "method": "To truly assess the impacts of these experiments on the model one must perform a global fit, scanning over the multi--dimensional parameter space and combining all the data in a statistically rigorous manner."
    },
    {
        "abstract": "  In this article we study eigenvalues and minimizers of a fractional\nnon-standard growth problem. We prove several properties on this quantities and\ntheir corresponding eigenfunctions.\n",
        "method": "Methodological sentences are not present in this abstract as it only discusses the topic and objectives of the research, without mentioning specific methods or approaches used."
    },
    {
        "abstract": "  We propose an efficient protocol for decentralized training of deep neural\nnetworks from distributed data sources. The proposed protocol allows to handle\ndifferent phases of model training equally well and to quickly adapt to concept\ndrifts. This leads to a reduction of communication by an order of magnitude\ncompared to periodically communicating state-of-the-art approaches. Moreover,\nwe derive a communication bound that scales well with the hardness of the\nserialized learning problem. The reduction in communication comes at almost no\ncost, as the predictive performance remains virtually unchanged. Indeed, the\nproposed protocol retains loss bounds of periodically averaging schemes. An\nextensive empirical evaluation validates major improvement of the trade-off\nbetween model performance and communication which could be beneficial for\nnumerous decentralized learning applications, such as autonomous driving, or\nvoice recognition and image classification on mobile phones.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe proposed protocol allows to handle different phases of model training equally well and to quickly adapt to concept drifts."
    },
    {
        "abstract": "  In this paper we consider ultra-parallel complex hyperbolic triangle groups\nof type $[m_1,m_2,0]$, i.e. groups of isometries of the complex hyperbolic\nplane, generated by complex reflections in three ultra-parallel complex\ngeodesics two of which intersect on the boundary. We prove some discreteness\nand non-discreteness results for these groups and discuss the connection\nbetween the discreteness results and ellipticity of certain group elements.\n",
        "method": "Methodological sentences:\n\nWe consider... groups of isometries of the complex hyperbolic plane, generated by complex reflections in three ultra-parallel complex geodesics two of which intersect on the boundary."
    },
    {
        "abstract": "  Security of information passing through the Internet is threatened by today's\nmost advanced malware ranging from orchestrated botnets to simpler polymorphic\nworms. These threats, as examples of zero-day attacks, are able to change their\nbehavior several times in the early phases of their existence to bypass the\nnetwork intrusion detection systems (NIDS). In fact, even well-designed, and\nfrequently-updated signature-based NIDS cannot detect the zero-day treats due\nto the lack of an adequate signature database, adaptive to intelligent attacks\non the Internet. More importantly, having an NIDS, it should be tested on\nmalicious traffic dataset that not only represents known attacks, but also can\nto some extent reflect the characteristics of unknown, zero-day attacks.\nGenerating such traffic is identified in the literature as one of the main\nobstacles for evaluating the effectiveness of NIDS. To address these issues, we\nintroduce RNNIDS that applies Recurrent Neural Networks (RNNs) to find complex\npatterns in attacks and generate similar ones. In this regard, for the first\ntime, we demonstrate that RNNs are helpful to generate new, unseen mutants of\nattacks as well as synthetic signatures from the most advanced malware to\nimprove the intrusion detection rate. Besides, to further enhance the design of\nan NIDS, RNNs can be employed to generate malicious datasets containing, e.g.,\nunseen mutants of a malware. To evaluate the feasibility of our approaches, we\nconduct extensive experiments by incorporating publicly available datasets,\nwhere we show a considerable improvement in the detection rate of an\noff-the-shelf NIDS (up to 16.67%).\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* To address these issues, we introduce RNNIDS that applies Recurrent Neural Networks (RNNs) to find complex patterns in attacks and generate similar ones.\n* In this regard, for the first time, we demonstrate that RNNs are helpful to generate new, unseen mutants of attacks as well as synthetic signatures from the most advanced malware to improve the intrusion detection rate.\n* To further enhance the design of an NIDS, RNNs can be employed to generate malicious datasets containing, e.g., unseen mutants of a malware.\n* To evaluate the feasibility of our approaches, we conduct extensive experiments by incorporating publicly available datasets."
    },
    {
        "abstract": "  Many existing optical meteor trajectory estimation methods use the\napproximation that the velocity of the meteor at the beginning of its luminous\nphase is equivalent to its velocity before atmospheric entry. Meteoroid kinetic\nenergy loss prior to the luminous phase cannot be measured, but for some masses\nand entry geometries neglecting this loss may lead to non-negligible\ndeceleration prior to thermal ablation. Using a numerical meteoroid ablation\nmodel, we simulate the kinematics of meteoroids beginning at 180 km with\ninitial velocities ranging from 11 km/s to 71 km/s, and compare model\nvelocities at the moment of detection to measurements. We validate the\nsimulations by comparing the simulated luminous beginning heights with observed\nbeginning heights of different populations of meteors detected with different\noptical systems. We find that most low-velocity meteoroids have a significant\nvelocity difference of 100 m/s to 750 m/s (depending on meteoroid type, mass,\nand observation system). This systematic underestimate of meteoroid speeds also\nresults in systematically lower semi-major axes for meteoroid orbits.\n",
        "method": "Using a numerical meteoroid ablation model, we simulate the kinematics of meteoroids beginning at 180 km with initial velocities ranging from 11 km/s to 71 km/s, and compare model velocities at the moment of detection to measurements."
    },
    {
        "abstract": "  This paper considers solving convex quadratic programs (QPs) in a real-time\nsetting using a regularized and smoothed Fischer-Burmeister method (FBRS). The\nFischer-Burmeister function is used to map the optimality conditions of the\nquadratic program to a nonlinear system of equations which is solved using\nNewton's method. Regularization and smoothing are applied to improve the\npractical performance of the algorithm and a merit function is used to\nglobalize convergence. FBRS is simple to code, easy to warmstart, robust to\nearly termination, and has attractive theoretical properties, making it\nappealing for real-time and embedded applications. Numerical experiments using\nseveral predictive control examples show that the proposed method is\ncompetitive with other state of the art solvers.\n",
        "method": "The Fischer-Burmeister function is used to map the optimality conditions of the quadratic program to a nonlinear system of equations which is solved using Newton's method."
    },
    {
        "abstract": "  Over past several years, deep learning has achieved huge successes in various\napplications. However, such a data-driven approach is often criticized for lack\nof interpretability. Recently, we proposed artificial quadratic neural networks\nconsisting of second-order neurons in potentially many layers. In each\nsecond-order neuron, a quadratic function is used in the place of the inner\nproduct in a traditional neuron, and then undergoes a nonlinear activation.\nWith a single second-order neuron, any fuzzy logic operation, such as XOR, can\nbe implemented. In this sense, any deep network constructed with quadratic\nneurons can be interpreted as a deep fuzzy logic system. Since traditional\nneural networks and second-order counterparts can represent each other and\nfuzzy logic operations are naturally implemented in second-order neural\nnetworks, it is plausible to explain how a deep neural network works with a\nsecond-order network as the system model. In this paper, we generalize and\ncategorize fuzzy logic operations implementable with individual second-order\nneurons, and then perform statistical/information theoretic analyses of\nexemplary quadratic neural networks.\n",
        "method": "Recently, we proposed artificial quadratic neural networks consisting of second-order neurons in potentially many layers."
    },
    {
        "abstract": "  The goal of this work is to demonstrate the use of the ballistocardiogram\n(BCG) signal, derived using head-mounted wearable devices, as a viable\nbiometric for authentication. The BCG signal is the measure of an person's body\nacceleration as a result of the heart's ejection of blood. It is a\ncharacterization of the cardiac cycle and can be derived non-invasively from\nthe measurement of subtle movements of a person's extremities. In this paper,\nwe use several versions of the BCG signal, derived from accelerometer and\ngyroscope sensors on a Smart Eyewear (SEW) device, for authentication. The\nderived BCG signals are used to train a convolutional neural network (CNN) as\nan authentication model, which is personalized for each subject. We evaluate\nour authentication models using data from 12 subjects and show that our\napproach has an equal error rate (EER) of 3.5% immediately after training and\n13\\% after about 2 months, in the worst case. We also explore the use of our\nauthentication approach for people with motor disabilities. Our analysis using\na separate dataset of 6 subjects with non-spastic cerebral palsy shows an EER\nof 11.2% immediately after training and 21.6% after about 2 months, in the\nworst-case.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We use several versions of the BCG signal, derived from accelerometer and gyroscope sensors on a Smart Eyewear (SEW) device, for authentication.\""
    },
    {
        "abstract": "  We give a uniqueness result in dimension 2 for the solutions to an equation\non compact Riemannian surface without boundary.\n",
        "method": "No methodological sentence is present in this abstract."
    },
    {
        "abstract": "  In the recent paper `Well-posedness and regularity for a generalized\nfractional Cahn-Hilliard system' (arXiv:1804.11290) by the same authors,\ngeneral well-posedness results have been established for a a class of\nevolutionary systems of two equations having the structure of a viscous\nCahn-Hilliard system, in which nonlinearities of double-well type occur. The\noperators appearing in the system equations are fractional versions in the\nspectral sense of general linear operators A,B having compact resolvents, which\nare densely defined, unbounded, selfadjoint, and monotone in a Hilbert space of\nfunctions defined in a smooth domain. In this work we complement the results\ngiven in arXiv:1804.11290 by studying a distributed control problem for this\nevolutionary system. The main difficulty in the analysis is to establish a\nrigorous Frechet differentiability result for the associated control-to-state\nmapping. This seems only to be possible if the state stays bounded, which, in\nturn, makes it necessary to postulate an additional global boundedness\nassumption. One typical situation, in which this assumption is satisfied,\narises when B is the negative Laplacian with zero Dirichlet boundary conditions\nand the nonlinearity is smooth with polynomial growth of at most order four.\nAlso a case with logarithmic nonlinearity can be handled. Under the global\nboundedness assumption, we establish existence and first-order necessary\noptimality conditions for the optimal control problem in terms of a variational\ninequality and the associated adjoint state system.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* In this work we complement the results given in arXiv:1804.11290 by studying a distributed control problem for this evolutionary system.\n* This seems only to be possible if the state stays bounded, which, in turn, makes it necessary to postulate an additional global boundedness assumption.\n* Under the global boundedness assumption, we establish existence and first-Order necessary optimality conditions for the optimal control problem in terms of a variational inequality and the associated adjoint state system."
    },
    {
        "abstract": "  Quantum secret sharing is a way to share secret messages among the clients in\na group with complete security. For the first time, Hillery et al. (Phys Rev A\n59:1829, 1999) proposed the quantum version of the classical secret sharing\nprotocol using GHZ states. Here, we implement the above quantum secret sharing\nprotocol in 'IBM Q 5 Tenerife' quantum processor and compare the experimentally\nobtained results with the theoretically predicted ones. Further, a new quantum\nbinary voting protocol is proposed and implemented in the 14-qubit 'IBM Q 14\nMelbourne' quantum processor. The results are analyzed through the technique of\nquantum state tomography, and the fidelity of states is calculated for a\ndifferent number of executions made in the device.\n",
        "method": "Here is the methodological sentence:\n\nHere, we implement the above quantum secret sharing protocol in 'IBM Q 5 Tenerife' quantum processor and compare the experimentally obtained results with the theoretically predicted ones."
    },
    {
        "abstract": "  Weyl semimetals can be described as the three-dimensional analogue of\ngraphene, showing linear dispersion around nodes (Weyl points) [1]. Tantalum\narsenide (TaAs) is among the most studied Weyl semimetals. It has been\ndemonstrated that TaAs has a very high value of the real part of the complex\nrefractive index in the infrared region [2]. In this work we show\none-dimensional photonic crystals alternating TaAs with SiO2 or TiO2 and a\nmicrocavity where a layer of TaAs is embedded between two SiO2-TiO2 multilayer.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nIn this work we show one-dimensional photonic crystals alternating TaAs with SiO2 or TiO2 and a microcavity where a layer of TaAs is embedded between two SiO2-TiO2 multilayer."
    },
    {
        "abstract": "  This paper tests a conjecture on discrete non-Abelian gauging of 3d\n$\\mathcal{N} = 4$ supersymmetric quiver gauge theories. Given a parent quiver\nwith a bouquet of $n$ nodes of rank $1$, invariant under a discrete $S_n$\nglobal symmetry, one can construct a daughter quiver where the bouquet is\nsubstituted by a single adjoint $n$ node. Based on the main conjecture in this\npaper, the daughter quiver corresponds to a theory where the $S_n$ discrete\nglobal symmetry is gauged and the new Coulomb branch is a non-Abelian orbifold\nof the parent Coulomb branch. We demonstrate and test the conjecture for three\nsimply laced families of bouquet quivers and a non-simply laced bouquet quiver\nwith $C_2$ factor in the global symmetry.\n",
        "method": "Given a parent quiver with a bouquet of $n$ nodes of rank 1, invariant under a discrete $S_n$ global symmetry, one can construct a daughter quiver where the bouquet is substituted by a single adjoint $n$ node."
    },
    {
        "abstract": "  We present a quantum key distribution system with a 2.5 GHz repetition rate\nusing a three-state time-bin protocol combined with a one-decoy approach.\nTaking advantage of superconducting single-photon detectors optimized for\nquantum key distribution and ultra low-loss fiber, we can distribute secret\nkeys at a maximum distance of 421 km and obtain secret key rates of 6.5 bps\nover 405 km.\n",
        "method": "We present a quantum key distribution system with a 2.5 GHz repetition rate using a three-state time-bin protocol combined with a one-decoy approach."
    },
    {
        "abstract": "  We study the problem of synthesizing a policy that maximizes the entropy of a\nMarkov decision process (MDP) subject to a temporal logic constraint. Such a\npolicy minimizes the predictability of the paths it generates, or dually,\nmaximizes the exploration of different paths in an MDP while ensuring the\nsatisfaction of a temporal logic specification. We first show that the maximum\nentropy of an MDP can be finite, infinite or unbounded. We provide necessary\nand sufficient conditions under which the maximum entropy of an MDP is finite,\ninfinite or unbounded. We then present an algorithm which is based on a convex\noptimization problem to synthesize a policy that maximizes the entropy of an\nMDP. We also show that maximizing the entropy of an MDP is equivalent to\nmaximizing the entropy of the paths that reach a certain set of states in the\nMDP. Finally, we extend the algorithm to an MDP subject to a temporal logic\nspecification. In numerical examples, we demonstrate the proposed method on\ndifferent motion planning scenarios and illustrate the relation between the\nrestrictions imposed on the paths by a specification, the maximum entropy, and\nthe predictability of paths.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We first show that the maximum entropy of an MDP can be finite, infinite or unbounded.\n* We provide necessary and sufficient conditions under which the maximum entropy of an MDP is finite, infinite or unbounded.\n* We then present an algorithm which is based on a convex optimization problem to synthesize a policy that maximizes the entropy of an MDP.\n* We also show that maximizing the entropy of an MDP is equivalent to maximizing the entropy of the paths that reach a certain set of states in the MDP."
    },
    {
        "abstract": "  This paper presents our experiences in designing, implementing, and piloting\nan intelligent vocabulary learning tutor. The design builds on several\nintelligent tutoring design concepts, including graph-based knowledge\nrepresentation, learner modeling, and adaptive learning content and assessment\nexposition. Specifically, we design a novel phased learner model approach to\nenable systematic exposure to words during vocabulary instruction. We also\nbuilt an example application over the tutor platform that uses a learning\nactivity involving videos and an assessment activity involving word to\npicture/image association. More importantly, the tutor adapts to the\nsignificant variation in children's knowledge at the beginning of kindergarten,\nand evolves the application at the speed of each individual learner. A pilot\nstudy with 180 kindergarten learners allowed the tutor to collect various kinds\nof activity information suitable for insights and interventions both at an\nindividual- and class-level. The effort also demonstrates that we can do A/B\ntesting for a variety of hypotheses at scale with such a framework.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We design a novel phased learner model approach to enable systematic exposure to words during vocabulary instruction.\""
    },
    {
        "abstract": "  This paper examines the operation of distribution networks that have large\naggregations of thermostatically controlled loads (TCLs) providing secondary\nfrequency regulation to the bulk power system. Specifically, we assess the\nprevalence of distribution network constraint violations, such as over- or\nunder-voltages and overloading of transformers. Our goal is to determine the\nset of constraints that are at increased risk of being violated when TCLs\nprovide regulation. We compare network operation in two cases: first with TCLs\noperating freely, and second with TCLs controlled to track a regulation signal.\nUsing GridLAB-D, we run power flow simulations of five real distribution\nnetworks. Our results indicate that voltage limits are at increased risk of\nviolation when TCLs provide regulation because of increased voltage variation.\nEffects on transformer aging are more nuanced and depend on the method used for\ndispatching TCLs. We find that in many distribution networks it may only be\nnecessary to consider voltage constraints when designing a TCL control strategy\nthat protects the distribution network.\n",
        "method": "Using GridLAB-D, we run power flow simulations of five real distribution networks."
    },
    {
        "abstract": "  We introduce a pointfree theory of convergence on lattices and coframes. A\nconvergence lattice is a lattice $L$ with a monotonic map $\\lim_L$ from the\nlattice of filters on $L$ to $L$, meant to be an abstract version of the map\nsending every filter of subsets to its set of limits. This construction\nexhibits the category of convergence spaces as a coreflective subcategory of\nthe opposite of the category of convergence lattices. We extend this\nconstruction to coreflections between limit spaces and the opposite of\nso-called limit lattices and limit coframes, between pretopological convergence\nspaces and the opposite of so-called pretopological convergence coframes,\nbetween adherence spaces and the opposite of so-called adherence coframes,\nbetween topological spaces and the opposite of so-called topological coframes.\nAll of our pointfree categories are complete and cocomplete, and topological\nover the category of coframes. Our final pointfree category, that of\ntopological coframes, shares with the category of frames the property of being\nin a dual adjunction with the category of topological spaces. We show that the\nlatter arises as a retract of the former, and that this retraction restricts to\na reflection between frames and so-called strong topological coframes.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThis construction exhibits the category of convergence spaces as a coreflective subcategory of the opposite of the category of convergence lattices."
    },
    {
        "abstract": "  Secure and scalable data sharing is essential for collaborative clinical\ndecision making. Conventional clinical data efforts are often siloed, however,\nwhich creates barriers to efficient information exchange and impedes effective\ntreatment decision made for patients. This paper provides four contributions to\nthe study of applying blockchain technology to clinical data sharing in the\ncontext of technical requirements defined in the \"Shared Nationwide\nInteroperability Roadmap\" from the Office of the National Coordinator for\nHealth Information Technology (ONC). First, we analyze the ONC requirements and\ntheir implications for blockchain-based systems. Second, we present FHIRChain,\nwhich is a blockchain-based architecture designed to meet ONC requirements by\nencapsulating the HL7 Fast Healthcare Interoperability Resources (FHIR)\nstandard for shared clinical data. Third, we demonstrate a FHIRChain-based\ndecentralized app using digital health identities to authenticate participants\nin a case study of collaborative decision making for remote cancer care.\nFourth, we highlight key lessons learned from our case study.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We analyze the ONC requirements and their implications for blockchain-based systems.\n* We present FHIRChain, which is a blockchain-based architecture designed to meet ONC requirements by encapsulating the HL7 Fast Healthcare Interoperability Resources (FHIR) standard for shared clinical data.\n* We demonstrate a FHIRChain-based decentralized app using digital health identities to authenticate participants in a case study of collaborative decision making for remote cancer care."
    },
    {
        "abstract": "  Femtosecond frequency combs have boosted progress in various fields of\nprecision metrology. Nevertheless, demanding applications such as front-end\nfrequency and time standards, ultrastable microwave generation or\nhigh-resolution spectroscopy still necessitate improved stability. The spectral\nbandwidth and absolute position of individual comb lines are crucial in this\ncontext. Typically, both parameters are controlled on short and long time\nscales by tight locking to external optical and microwave references which\nrepresent costly and cumbersome additions to the entire setup. Here, we\ndemonstrate fully self-controlled stabilization of a fibre-based femtosecond\nfrequency comb requiring neither optical nor radio frequency external\nreferences. In the first step, this technology allows us to optically eliminate\nthe carrier-envelope phase slip via ultrabroadband difference frequency\ngeneration. The resulting amplification of intrinsically quantum-limited phase\nnoise from the mode-locked oscillator is elegantly addressed in the second\nstep. We efficiently suppress these excess fluctuations by a direct transfer of\nthe superior short-time noise properties of the fundamental oscillator to the\noffset-free comb. Our combined scheme provides a high-precision frequency\nreference operating completely autonomously, thus marking a new era for\nfibre-based sources in advanced applications ranging from space exploration to\ntests of the invariability of fundamental constants.\n",
        "method": "Typically, both parameters are controlled on short and long time scales by tight locking to external optical and microwave references which represent costly and cumbersome additions to the entire setup."
    },
    {
        "abstract": "  We introduce a class of probability measure-valued diffusions, coined\npolynomial, of which the well-known Fleming--Viot process is a particular\nexample. The defining property of finite dimensional polynomial processes\nconsidered by Cuchiero et al. (2012) and Filipovic and Larsson (2016) is\ntransferred to this infinite dimensional setting. This leads to a\nrepresentation of conditional marginal moments via a finite dimensional linear\nPDE, whose spatial dimension corresponds to the degree of the moment. As a\nresult, the tractability of finite dimensional polynomial processes are\npreserved in this setting. We also obtain a representation of the corresponding\nextended generators, and prove well-posedness of the associated martingale\nproblems. In particular, uniqueness is obtained from the duality relationship\nwith the PDEs mentioned above.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* This leads to a representation of conditional marginal moments via a finite dimensional linear PDE, whose spatial dimension corresponds to the degree of the moment.\n* We also obtain a representation of the corresponding extended generators, and prove well-posedness of the associated martingale problems."
    },
    {
        "abstract": "  It is shown that a separable exact residually finite dimensional C*-algebra\nwith locally finitely generated (rational) even K-homology embeds in a\nuniformly hyperfinite C*-algebra.\n",
        "method": "Here is the methodological sentence:\n\nIt is shown..."
    },
    {
        "abstract": "  We explore the ability of two-dimensional periodic atom arrays to produce\nlight amplification and generate laser emission when gain is introduced through\nexternal optical pumping. Specifically, we predict that lasing can take place\nfor arbitrarily weak atomic scatterers assisted by cooperative interaction\namong atoms in a 2D lattice. We base this conclusion on analytical theory for\nthree-level scatterers, which additionally reveals a rich interplay between\nlattice and atomic resonances. Our results provide a general background to\nunderstand light amplification and lasing in periodic atomic arrays, with\npromising applications in the generation, manipulation, and control of coherent\nphoton states at the nanoscale.\n",
        "method": "We explore the ability of two-dimensional periodic atom arrays to produce light amplification and generate laser emission when gain is introduced through external optical pumping."
    },
    {
        "abstract": "  Objective: Heartbeat detection remains central to cardiac disease diagnosis\nand management, and is traditionally performed based on electrocardiogram\n(ECG). To improve robustness and accuracy of detection, especially, in certain\ncritical-care scenarios, the use of additional physiological signals such as\narterial blood pressure (BP) has recently been suggested. There, estimation of\nheartbeat location requires information fusion from multiple signals. However,\nreported efforts in this direction often obtain multimodal estimates somewhat\nindirectly, by voting among separately obtained signal-specific intermediate\nestimates. In contrast, we propose to directly fuse information from multiple\nsignals without requiring intermediate estimates, and thence estimate heartbeat\nlocation in a robust manner. Method: We propose as a heartbeat detector, a\nconvolutional neural network (CNN) that learns fused features from multiple\nphysiological signals. This method eliminates the need for hand-picked\nsignal-specific features and ad hoc fusion schemes. Further, being data-driven,\nthe same algorithm learns suitable features from arbitrary set of signals.\nResults: Using ECG and BP signals of PhysioNet 2014 Challenge database, we\nobtained a score of 94%. Further, using two ECG channels of MIT-BIH arrhythmia\ndatabase, we scored 99.92\\%. Both those scores compare favourably with\npreviously reported database-specific results. Also, our detector achieved high\naccuracy in a variety of clinical conditions. Conclusion: The proposed\nCNN-based information fusion (CIF) algorithm is generalizable, robust and\nefficient in detecting heartbeat location from multiple signals. Significance:\nIn medical signal monitoring systems, our technique would accurately estimate\nheartbeat locations even when only a subset of channels are reliable.\n",
        "method": "Method: We propose as a heartbeat detector, a convolutional neural network (CNN) that learns fused features from multiple physiological signals."
    },
    {
        "abstract": "  The classification of multi-class microarray datasets is a hard task because\nof the small samples size in each class and the heavy overlaps among classes.\nTo effectively solve these problems, we propose novel Error Correcting Output\nCode (ECOC) algorithm by Enhance Class Separability related Data Complexity\nmeasures during encoding process, named as ECOCECS. In this algorithm, two\nnearest neighbor related DC measures are deployed to extract the intrinsic\noverlapping information from microarray data. Our ECOC algorithm aims to search\nan optimal class split scheme by minimizing these measures. The class splitting\nprocess ends when each class is separated from others, and then the class\nassignment scheme is mapped as a coding matrix. Experiments are carried out on\nfive microarray datasets, and results demonstrate the effectiveness and\nrobustness of our method in comparison with six state-of-art ECOC methods. In\nshort, our work confirm the probability of applying DC to ECOC framework.\n",
        "method": "Here is the extracted methodological sentence:\n\nWe propose novel Error Correcting Output Code (ECOC) algorithm by Enhance Class Separability related Data Complexity measures during encoding process, named as ECOCECS."
    },
    {
        "abstract": "  Joint detection and estimation refers to deciding between two or more\nhypotheses and, depending on the test outcome, simultaneously estimating the\nunknown parameters of the underlying distribution. This problem is investigated\nin a sequential framework under mild assumptions on the underlying random\nprocess. We formulate an unconstrained sequential decision problem, whose cost\nfunction is the weighted sum of the expected run-length and the\ndetection/estimation errors. Then, a strong connection between the derivatives\nof the cost function with respect to the weights, which can be interpreted as\nLagrange multipliers, and the detection/estimation errors of the underlying\nscheme is shown. This property is used to characterize the solution of a\nclosely related sequential decision problem, whose objective function is the\nexpected run-length under constraints on the average detection/estimation\nerrors. We show that the solution of the constrained problem coincides with the\nsolution of the unconstrained problem with suitably chosen weights. These\nweights are characterized as the solution of a linear program, which can be\nsolved using efficient off-the-shelf solvers. The theoretical results are\nillustrated with two example problems, for which optimal sequential schemes are\ndesigned numerically and whose performance is validated via Monte Carlo\nsimulations.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe formulate an unconstrained sequential decision problem, whose cost function is the weighted sum of the expected run-length and the detection/estimation errors."
    },
    {
        "abstract": "  To study the correlation between clothing garments and body shape, we\ncollected a new dataset (Fashion Takes Shape), which includes images of users\nwith clothing category annotations. We employ our multi-photo approach to\nestimate body shapes of each user and build a conditional model of clothing\ncategories given body-shape. We demonstrate that in real-world data, clothing\ncategories and body-shapes are correlated and show that our multi-photo\napproach leads to a better predictive model for clothing categories compared to\nmodels based on single-view shape estimates or manually annotated body types.\nWe see our method as the first step towards the large-scale understanding of\nclothing preferences from body shape.\n",
        "method": "Here is the methodological sentence:\n\n\"We collected a new dataset (Fashion Takes Shape), which includes images of users with clothing category annotations.\""
    },
    {
        "abstract": "  Objective: Accurately classifying the malignancy of lesions detected in a\nscreening scan is critical for reducing false positives. Radiomics holds great\npotential to differentiate malignant from benign tumors by extracting and\nanalyzing a large number of quantitative image features. Since not all radiomic\nfeatures contribute to an effective classifying model, selecting an optimal\nfeature subset is critical. Methods: This work proposes a new multi-objective\nbased feature selection (MO-FS) algorithm that considers sensitivity and\nspecificity simultaneously as the objective functions during feature selection.\nFor MO-FS, we developed a modified entropy based termination criterion (METC)\nthat stops the algorithm automatically rather than relying on a preset number\nof generations. We also designed a solution selection methodology for\nmulti-objective learning that uses the evidential reasoning approach (SMOLER)\nto automatically select the optimal solution from the Pareto-optimal set.\nFurthermore, we developed an adaptive mutation operation to generate the\nmutation probability in MO-FS automatically. Results: We evaluated the MO-FS\nfor classifying lung nodule malignancy in low-dose CT and breast lesion\nmalignancy in digital breast tomosynthesis. Conclusion: The experimental\nresults demonstrated that the feature set selected by MO-FS achieved better\nclassification performance than features selected by other commonly used\nmethods. Significance: The proposed method is general and more effective\nradiomic feature selection strategy.\n",
        "method": "Here are the methodological sentences:\n\n* This work proposes a new multi-objective based feature selection (MO-FS) algorithm that considers sensitivity and specificity simultaneously as the objective functions during feature selection.\n* For MO-FS, we developed a modified entropy based termination criterion (METC) that stops the algorithm automatically rather than relying on a preset number of generations.\n* We also designed a solution selection methodology for multi-objective learning that uses the evidential reasoning approach (SMOLER) to automatically select the optimal solution from the Pareto-optimal set.\n* Furthermore, we developed an adaptive mutation operation to generate the mutation probability in MO-FS automatically."
    },
    {
        "abstract": "  We follow two small, magnetically isolated CME-producing solar active regions\n(ARs) from the time of their emergence until several days later, when their\ncore regions erupt to produce the CMEs. In both cases, magnetograms show: (a)\nfollowing an initial period where the poles of the emerging regions separate\nfrom each other, the poles then reverse direction and start to retract inward;\n(b) during the retraction period, flux cancelation occurs along the main\nneutral line of the regions; (c) this cancelation builds the sheared core\nfield/flux rope that eventually erupts to make the CME. In the two cases,\nrespectively 30% and 50% of the maximum flux of the region cancels prior to the\neruption. Recent studies indicate that solar coronal jets frequently result\nfrom small-scale filaments eruptions, with those \"minifilament\" eruptions also\nbeing built up and triggered by cancelation of magnetic flux. Together, the\nsmall-AR eruptions here and the coronal jet results suggest that isolated\nbipolar regions tend to erupt when some threshold fraction, perhaps in the\nrange of 50%, of the region's maximum flux has canceled. Our observed erupting\nfilaments/flux ropes form at sites of flux cancelation, in agreement with\nprevious observations. Thus, the recent finding that minifilaments that erupt\nto form jets also form via flux cancelation is further evidence that\nminifilaments are small-scale versions of the long-studied full-sized\nfilaments.\n",
        "method": "In both cases, magnetograms show: (a) following an initial period where the poles of the emerging regions separate from each other, the poles then reverse direction and start to retract inward; (b) during the retraction period, flux cancelation occurs along the main neutral line of the regions; (c) this cancelation builds the sheared core field/flux rope that eventually erupts to make the CME."
    },
    {
        "abstract": "  We introduce here a fully automated convolutional neural network-based method\nfor brain image processing to Detect Neurons in different brain Regions during\nDevelopment (DeNeRD). Our method takes a developing mouse brain as input and i)\nregisters the brain sections against a developing mouse reference atlas, ii)\ndetects various types of neurons, and iii) quantifies the neural density in\nmany unique brain regions at different postnatal (P) time points. Our method is\ninvariant to the shape, size and expression of neurons and by using DeNeRD, we\ncompare the brain-wide neural density of all GABAergic neurons in developing\nbrains of ages P4, P14 and P56. We discover and report 6 different clusters of\nregions in the mouse brain in which GABAergic neurons develop in a differential\nmanner from early age (P4) to adulthood (P56). These clusters reveal key steps\nof GABAergic cell development that seem to track with the functional\ndevelopment of diverse brain regions as the mouse transitions from a passive\nreceiver of sensory information (<P14) to an active seeker (>P14).\n",
        "method": "Here is the methodological sentence:\n\nOur method takes a developing mouse brain as input and i) registers the brain sections against a developing mouse reference atlas, ii) detects various types of neurons, and iii) quantifies the neural density in many unique brain regions at different postnatal (P) time points."
    },
    {
        "abstract": "  We briefly review helicity dynamics, inverse and bi-directional cascades in\nfluid and magnetohydrodynamic (MHD) turbulence, with an emphasis on the latter.\nThe energy of a turbulent system, an invariant in the non-dissipative case, is\ntransferred to small scales through nonlinear mode coupling. Fifty years ago,\nit was realized that, for a two-dimensional fluid, energy cascades instead to\nlarger scales, and so does magnetic helicity in three-dimensional MHD. However,\nevidence obtained recently indicates that in fact, for a range of governing\nparameters, there are systems for which their ideal invariants can be\ntransferred, with constant fluxes, to both the large scales and the small\nscales, as for MHD or rotating stratified flows, in the latter case including\nwith quasi-geostrophic forcing. Such bi-directional, split, cascades directly\naffect the rate at which mixing and dissipation occur in these flows in which\nnonlinear eddies interact with fast waves with anisotropic dispersion laws, due\nfor example to imposed rotation, stratification or uniform magnetic fields. The\ndirections of cascades can be obtained in some cases through the use of\nphenomenological arguments, one of which we derive here following classical\nlines in the case of the inverse magnetic helicity cascade in electron MHD.\nWith more highly-resolved data sets stemming from large laboratory experiments,\nhigh-performance computing and in-situ satellite observations, machine-learning\ntools are bringing novel perspectives to turbulence research, e.g. in helping\ndevise new explicit sub-grid scale parameterizations, which may lead to\nenhanced physical insight, including in the future in the case of these new\nbi-directional cascades.\n",
        "method": "The energy of a turbulent system, an invariant in the non-dissipative case, is transferred to small scales through nonlinear mode coupling."
    },
    {
        "abstract": "  The first order standard perturbation theory combined with ab initio\nprojector augmented wave operator challenges the realization of the standard\nSternheimer equation with linear computational efficiency. This efficiency\nmotivates us to describe the electron-phonon interaction in two-dimensional\n(2D) black phosphorous monolayer using generalized density functional\nperturbation theory (DFPT) with Boltzmann transport theory (BTE). Subsequently,\nlinear response phonon dynamic behaviour in terms of conductivities, seebeck\ncoefficients and transport properties are focused for its thermoelectric\napplication. The analysis reveals the crystal orientation dependence via\nstructural anisotropy and the density of states of the monolayer structure.\nMomentum dependent phonon population dynamics along with the phonon linewidth\nare efficient in terms of reciprocal space electronic states. The optimized\nvalues of thermal conductivities of electrons and Seebeck coefficients act as\ndriving force to modulate thermoelectric effects. Figure of merit is calculated\nto be 0.074 at 300 K and 0.152 at 500 K of the MLBP system as a function of the\npower factor. The value of lattice thermal conductivity is 37.15 W/mK at room\ntemperature and follows the inverse dependency with temperature. With the\nanticipated superior performance, profound thermoelectric applications can be\nachieved particularly in the monolayer black phosphorous system.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* This efficiency motivates us to describe the electron-phonon interaction in two-dimensional (2D) black phosphorous monolayer using generalized density functional perturbation theory (DFPT) with Boltzmann transport theory (BTE).\n* The analysis reveals the crystal orientation dependence via structural anisotropy and the density of states of the monolayer structure."
    },
    {
        "abstract": "  Biofilm accumulation in the porous media can cause plugging and change many\nphysical properties of porous media. Targeted bioplugging may have significant\napplications for industrial processes. A deeper understanding of the relative\ninfluences of hydrodynamic conditions including flow velocity and nutrient\nconcentration, on biofilm growth and detachment is necessary to plan and\nanalyze bioplugging experiments and field trials. The experimental results by\nmeans of microscopic imaging over a T-shape microchannel show that increase in\nfluid velocity could facilitate biofilm growth, but that above a velocity\nthreshold, biofilm detachment and inhibition of biofilm formation due to high\nshear stress were observed. High nutrient concentration prompts the biofilm\ngrowth, but was accompanied by a relatively weak adhesive strength. This letter\nprovides an overview of biofilm development in a hydrodynamic environment for\nbetter predicting and modelling the bioplugging associated with porous system\nin petroleum industry, hydrogeology, and water purification.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"The experimental results by means of microscopic imaging over a T-shape microchannel show...\""
    },
    {
        "abstract": "  In this paper, we proceed exploring the case of non-stationary helical flows\nof the Navier-Stokes equations for incompressible fluids with variable\n(spatially dependent) coefficient of proportionality between velocity and the\ncurl field of flow. Meanwhile, the system of Navier-Stokes equations (including\ncontinuity equation) has been successfully explored previously with respect to\nthe existence of analytical way for presentation of non-stationary helical\nflows of the aforementioned type. The main motivation of the current research\nis the exploring the stability of previously obtained helical flows. Conditions\nfor the stability criteria of the exact solution for the aforementioned type of\nflows are obtained, for which non-stationary helical flow with invariant\nBernoulli-function is considered. As it has been formulated before, the spatial\npart of the pressure field of the fluid flow should be determined via\nBernoulli-function, if components of the velocity of the flow are already\nobtained.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nThe system of Navier-Stokes equations (including continuity equation) has been successfully explored previously with respect to the existence of analytical way for presentation of non-stationary helical flows of the aforementioned type."
    },
    {
        "abstract": "  Let $K$ be a number field. The $K$-arithmetic type of a rational prime $\\ell$\nis the tuple $A_{K}(\\ell)=(f^{K}_{1},...,f^{K}_{g_{\\ell}})$ of the residue\ndegrees of $\\ell$ in $K$, written in ascending order. A well known result of\nPerlis from the 70's states that two number fields have the same Dedekind zeta\nfunction if and only if for almost all primes $\\ell$ the arithmetic types of\n$\\ell$ in both fields coincide. By the end of the 90's Perlis and Stuart asked\nif having the same zeta function implies that for ramified primes the sum of\nthe ramification degrees coincide. Here we study and answer their question for\nseptic number fields.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nBy the end of the 90's Perlis and Stuart asked if having the same zeta function implies that for ramified primes the sum of the ramification degrees coincide."
    },
    {
        "abstract": "  The Steepest Entropy Ascent approach is considered and applied to few-state\nsystems. When the Hamiltonian of the system is time dependent, the principle of\nmaximum entropy production can still be exploited; arguments to support this\nfact are given. In the limit of slowly varying Hamiltonians which allows for\nthe adiabatic approximation for the unitary part of the dynamics, the system\nexhibits significant robustness to the thermalization process. Specific\nexamples such as a spin in a rotating field and a generic two-state system\nundergoing an avoided crossing are considered.\n",
        "method": "When the Hamiltonian of the system is time dependent, the principle of maximum entropy production can still be exploited; arguments to support this fact are given."
    },
    {
        "abstract": "  Spectroscopic observations play essential roles in astrophysics. They are\ncrucial for determining physical parameters in the universe, providing\ninformation about the chemistry of various astronomical environments. The\nproper execution of the spectroscopic analysis requires accounting for all the\nphysical effects that are compatible to the signal-to-noise ratio. We find in\nthis paper the influence on spectroscopy from the atomic/ground state alignment\nowing to anisotropic radiation and modulated by interstellar magnetic field,\nhas significant impact on the study of interstellar gas. In different\nobservational scenarios, we comprehensively demonstrate how atomic alignment\ninfluences the spectral analysis and provide the expressions for correcting the\neffect. The variations are even more pronounced for multiplets and line ratios.\nWe show the variation of the deduced physical parameters caused by the atomic\nalignment effect, including alpha-to-iron ratio ([X/Fe]) and ionisation\nfraction. Synthetic observations are performed to illustrate the visibility of\nsuch effect with current facilities. A study of PDRs in $\\rho$ Ophiuchi cloud\nis presented to demonstrate how to account for atomic alignment in practice.\nOur work has shown that due to its potential impact, atomic alignment has to be\nincluded in an accurate spectroscopic analysis of the interstellar gas with\ncurrent observational capability.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe find in this paper the influence on spectroscopy from the atomic/ground state alignment owing to anisotropic radiation and modulated by interstellar magnetic field, has significant impact on the study of interstellar gas."
    },
    {
        "abstract": "  A Certificate Authority (CA) provides the critical authentication and\nsecurity services for Public Key Infrastructure (PKI) which are used for the\nInternet and wired networks. In MANETs (wireless and ad hoc) there is an\ninability to offer a centralized CA to provide these security services. Recent\nresearch has looked to facilitate the use of CAs within MANETs through the use\nof a Distributed Certificate Authority (DCA) for wireless and ad hoc networks.\nThis paper presents a number of different types of DCA protocols and\ncategorizes them into groups based on their factors and specifications. The\npaper concludes by proposing the best DCA security services in terms of\nperformance and level of security.\n",
        "method": "None. There are no methodological sentences in this abstract."
    },
    {
        "abstract": "  Few ideas have enjoyed as large an impact on deep learning as convolution.\nFor any problem involving pixels or spatial representations, common intuition\nholds that convolutional neural networks may be appropriate. In this paper we\nshow a striking counterexample to this intuition via the seemingly trivial\ncoordinate transform problem, which simply requires learning a mapping between\ncoordinates in (x,y) Cartesian space and one-hot pixel space. Although\nconvolutional networks would seem appropriate for this task, we show that they\nfail spectacularly. We demonstrate and carefully analyze the failure first on a\ntoy problem, at which point a simple fix becomes obvious. We call this solution\nCoordConv, which works by giving convolution access to its own input\ncoordinates through the use of extra coordinate channels. Without sacrificing\nthe computational and parametric efficiency of ordinary convolution, CoordConv\nallows networks to learn either complete translation invariance or varying\ndegrees of translation dependence, as required by the end task. CoordConv\nsolves the coordinate transform problem with perfect generalization and 150\ntimes faster with 10--100 times fewer parameters than convolution. This stark\ncontrast raises the question: to what extent has this inability of convolution\npersisted insidiously inside other tasks, subtly hampering performance from\nwithin? A complete answer to this question will require further investigation,\nbut we show preliminary evidence that swapping convolution for CoordConv can\nimprove models on a diverse set of tasks. Using CoordConv in a GAN produced\nless mode collapse as the transform between high-level spatial latents and\npixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST\nshowed 24% better IOU when using CoordConv, and in the RL domain agents playing\nAtari games benefit significantly from the use of CoordConv layers.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious.\""
    },
    {
        "abstract": "  Different types of concave plasmonic nanoresonators have been optimized to\nachieve superradiantly enhanced emission of SiV color centers in diamond.\nComparative study has been performed to consider advantages of different N\nnumber of SiV color centers, different diamond-silver (bare) and\ndiamond-silver-diamond (coated) core-shell nanoresonator types, as well as of\nspherical and ellipsoidal geometry. The complete fluorescence enhancement\n(qualified by Px factor) monitoring and the cQE corrected quantum efficiency\nweighted PxcQE objective function optimization promotes to design bad-cavities\nfor plasmonic Dicke effect. The switching into a collective Dicke state via\noptimized nanoresonators results in a radiated power proportional to N^2, which\nmanifest itself in an enhancement proportional to N both of the excitation and\nemission rates. Accordingly, enhancement proportional to N^2 of the Px factor\nand PxcQE has been reached both via four and six SiV color centers arranged in\nsymmetrical square and hexagonal patterns inside all types of inspected\nnanoresonators. Coated spherical and bare ellipsoidal nanoresonators result in\nstronger non-cooperative fluorescence enhancement, while superradiance is\nbetter achieved via bare spherical nanoresonators independently of SiV color\ncenters number, and via coated (bare) ellipsoidal nanoresonators seeded by four\n(six) SiV color centers. Indistinguishable superradiant state of four color\ncenters and line-width narrowing is achieved via bare nanoresonators. Six color\ncenters seeded bare spherical (ellipsoidal) nanoresonators result in larger\nfluorescence enhancement and more significantly overridden superradiance\nthresholds, while having slightly more (less) pronounced bad-cavity\ncharacteristics. Both phenomena are simultaneously optimized in ellipsoidal\nbare nanoresonators embedding six color centers with a slightly larger\ndetuning.\n",
        "method": "Comparative study has been performed to consider advantages of different N number of SiV color centers, different diamond-silver (bare) and diamond-silver-diamond (coated) core-shell nanoresonator types, as well as of spherical and ellipsoidal geometry."
    },
    {
        "abstract": "  We present StyleBlit---an efficient example-based style transfer algorithm\nthat can deliver high-quality stylized renderings in real-time on a single-core\nCPU. Our technique is especially suitable for style transfer applications that\nuse local guidance - descriptive guiding channels containing large spatial\nvariations. Local guidance encourages transfer of content from the source\nexemplar to the target image in a semantically meaningful way. Typical local\nguidance includes, e.g., normal values, texture coordinates or a displacement\nfield. Contrary to previous style transfer techniques, our approach does not\ninvolve any computationally expensive optimization. We demonstrate that when\nlocal guidance is used, optimization-based techniques converge to solutions\nthat can be well approximated by simple pixel-level operations. Inspired by\nthis observation, we designed an algorithm that produces results visually\nsimilar to, if not better than, the state-of-the-art, and is several orders of\nmagnitude faster. Our approach is suitable for scenarios with low computational\nbudget such as games and mobile applications.\n",
        "method": "Our technique is especially suitable for style transfer applications that use local guidance - descriptive guiding channels containing large spatial variations."
    },
    {
        "abstract": "  Phase diagram, critical properties and thermodynamic functions of the\ntwo-dimensional field-free quantum-spin-1/2 XXZ model has been calculated\nglobally using a numerical renormalization group theory. The nearest-neighbor\nspin-spin correlations and entanglement properties, as well as internal energy\nand specific heat are calculated globally at all temperatures for the whole\nrange of exchange interaction anisotropy, from XY limit to Ising limits, for\nboth antiferromagnetic and ferromagnetic cases. We show that there exists\nlong-range (quasi-long-range) order at low-temperatures, and the low-lying\nexcitations are gapped (gapless) in the Ising-like easy-axis (XY-like\neasy-plane) regime. Besides, we identify quantum phase transitions at\nzero-temperature.\n",
        "method": "Numerical renormalization group theory was used globally to calculate the phase diagram, critical properties and thermodynamic functions of the two-dimensional field-free quantum-spin-1/2 XXZ model."
    },
    {
        "abstract": "  We study the strong disorder regime of Floquet topological systems in\ndimension two, that describe independent electrons on a lattice subject to a\nperiodic driving. In the spectrum of the Floquet propagator we assume the\nexistence of an interval in which all states are localized--a mobility gap.\nFirst we generalize the relative construction from spectral to mobility gap,\ndefine a bulk index for an infinite sample and an edge index for the\nhalf-infinite one and prove the bulk-edge correspondence. Second, we consider\ncompletely localized systems where the mobility gap is the whole circle, and\ndefine alternative bulk and edge indices that circumvent the relative\nconstruction and match with quantized magnetization and pumping observables\nfrom the physics literature. Finally, we show that any system with a mobility\ngap can be reduced to a completely localized one. All the indices defined\nthroughout are equal.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe generalize the relative construction from spectral to mobility gap, define a bulk index for an infinite sample and an edge index for the half-infinite one and prove the bulk-edge correspondence."
    },
    {
        "abstract": "  In this paper we show that any static and spherically symmetric anisotropic\nsolution of the Einstein field equations can be thought as a system sourced by\ncertain deformed isotropic system in the context of Minimal Geometric\nDeformation-decoupling approach. To be more precise, we developed a mechanism\nto obtain an isotropic solution from any anisotropic solution of the Einstein\nfield equations. As an example, we implement the method to obtain the sources\nof a simple static anisotropic and spherically symmetric traversable wormhole.\n",
        "method": "Here is the extracted methodological sentence:\n\n\"We developed a mechanism to obtain an isotropic solution from any anisotropic solution of the Einstein field equations.\""
    },
    {
        "abstract": "  The power of quantum computers is still somewhat speculative. While they are\ncertainly faster than classical ones at some tasks, the class of problems they\ncan efficiently solve has not been mapped definitively onto known classical\ncomplexity theory. This means that we do not know for which calculations there\nwill be a \"quantum advantage,\" once an algorithm is found. One way to answer\nthe question is to find those algorithms, but finding truly quantum algorithms\nturns out to be very difficult. In previous work over the past three decades we\nhave pursued the idea of using techniques of machine learning to develop\nalgorithms for quantum computing. Here we compare the performance of standard\nreal- and complex-valued classical neural networks with that of one of our\nmodels for a quantum neural network, on both classical problems and on an\narchetypal quantum problem: the computation of an entanglement witness. The\nquantum network is shown to need far fewer epochs and a much smaller network to\nachieve comparable or better results.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* Here we compare the performance of standard real- and complex-valued classical neural networks with that of one of our models for a quantum neural network...\n* ...on both classical problems and on an archetypal quantum problem: the computation of an entanglement witness."
    },
    {
        "abstract": "  We propose and study an SU(3) trimer resonating-valence-bond (tRVB) state\nwith $C_{4v}$ point-group symmetry on the square lattice. By devising a\nprojected entangled-pair state representation, we show that all (connected)\ncorrelation functions between local operators in this SU(3) tRVB state decay\nexponentially, indicating its gapped nature. We further calculate the modular\n$S$ and $T$ matrices by constructing all nine topological sectors on a torus\nand establish the existence of $\\mathbb{Z}_3$ topological order in this SU(3)\ntRVB state.\n",
        "method": "By devising a projected entangled-pair state representation, we show that all (connected) correlation functions between local operators in this SU(3) tRVB state decay exponentially, indicating its gapped nature."
    },
    {
        "abstract": "  Optical control enables new high resolution probes of narrow collisional\n(Feshbach) resonances, which are strongly dependent on the relative momentum of\ncolliding atom pairs, and important for simulating neutron matter with\nultracold atomic gases. We demonstrate a two-field optical vernier, which\nexpands kHz (mG) magnetic field detunings near a narrow resonance into MHz\noptical field detunings, enabling precise control and characterization of the\nmomentum-dependent scattering amplitude. Two-photon loss spectra are measured\nfor the narrow resonance in $^6$Li, revealing rich structure in very good\nagreement with our theoretical model. However, anomalous frequency shifts\nbetween the measured and predicted two-photon spectra are not yet explained.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\n\"We demonstrate a two-field optical vernier, which expands kHz (mG) magnetic field detunings near a narrow resonance into MHz optical field detunings...\""
    },
    {
        "abstract": "  Across smart-grid and smart-city application domains, there are many problems\nwhere an ensemble of agents is to be controlled such that both the aggregate\nbehaviour and individual-level perception of the system's performance are\nacceptable. In many applications, traditional PI control is used to regulate\naggregate ensemble performance. Our principal contribution in this note is to\ndemonstrate that PI control may not be always suitable for this purpose, and in\nsome situations may lead to a loss of ergodicity for closed-loop systems.\nBuilding on this observation, a theoretical framework is proposed to both\nanalyse and design control systems for the regulation of large scale ensembles\nof agents with a probabilistic intent. Examples are given to illustrate our\nresults.\n",
        "method": "The methodological sentence in this abstract is:\n\n\"Building on this observation, a theoretical framework is proposed to both analyse and design control systems...\""
    },
    {
        "abstract": "  Lithography simulation is one of the key steps in physical verification,\nenabled by the substantial optical and resist models. A resist model bridges\nthe aerial image simulation to printed patterns. While the effectiveness of\nlearning-based solutions for resist modeling has been demonstrated, they are\nconsiderably data-demanding. Meanwhile, a set of manufactured data for a\nspecific lithography configuration is only valid for the training of one single\nmodel, indicating low data efficiency. Due to the complexity of the\nmanufacturing process, obtaining enough data for acceptable accuracy becomes\nvery expensive in terms of both time and cost, especially during the evolution\nof technology generations when the design space is intensively explored. In\nthis work, we propose a new resist modeling framework for contact layers,\nutilizing existing data from old technology nodes and active selection of data\nin a target technology node, to reduce the amount of data required from the\ntarget lithography configuration. Our framework based on transfer learning and\nactive learning techniques is effective within a competitive range of accuracy,\ni.e., 3-10X reduction on the amount of training data with comparable accuracy\nto the state-of-the-art learning approach.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* A resist model bridges the aerial image simulation to printed patterns.\n* Our framework based on transfer learning and active learning techniques is effective..."
    },
    {
        "abstract": "  We present a new method to interpolate between two matter phases that allows\nfor a description of mixed phases and can be used, e.g., for mimicking\ntransitions between pasta structures occuring in the crust as well as in the\ninner core of compact stars. This interpolation method is based on assuming\nswitch functions that are used to define a mixture of subphases while\nfulfilling constraints of thermodynamic stability. The width of the transition\ndepends on a free parameter, the pressure increment relative to the critical\npressure of a Maxwell construction. As an example we present a trigonometric\nfunction ansatz for the switch function together with a pressure increment\nduring the transition. We note that the resulting mixed phase equation of state\nbears similarities with the appearance of substitutional compounds in neutron\nstar crusts and with the sequence of transitions between different pasta phases\nin the hadron-to-quark matter transition. We apply this method to the case of a\nhadron-to-quark matter transition and test the robustness of the compact star\nmass twin phenomenon against the appearance of pasta phases modeled in this\nway.\n",
        "method": "This interpolation method is based on assuming switch functions that are used to define a mixture of subphases while fulfilling constraints of thermodynamic stability."
    },
    {
        "abstract": "  The vibrational properties of $\\mathrm{CrI_3}$ single crystals were\ninvestigated using Raman spectroscopy and were analyzed with respect to the\nchanges of the crystal structure. All but one mode are observed for both the\nlow-temperature $R\\bar{3}$ and the high-temperature C2/$m$ phase. For all\nobserved modes the energies and symmetries are in good agreement with DFT\ncalculations. The symmetry of a single-layer was identified as $p\\bar{3}1/m$.\nIn contrast to previous studies we observe the transition from the $R\\bar{3}$\nto the $\\mathrm{C2}/m$ phase at 180\\,K and find no evidence for coexistence of\nboth phases over a wide temperature range.\n",
        "method": "The vibrational properties of $\\mathrm{CrI_3}$ single crystals were investigated using Raman spectroscopy and were analyzed with respect to the changes of the crystal structure."
    },
    {
        "abstract": "  We present a mock catalogue for the Physics of the Accelerating Universe\nSurvey (PAUS) and use it to quantify the competitiveness of the narrow band\nimaging for measuring spectral features and galaxy clustering. The mock agrees\nwith observed number count and redshift distribution data. We demonstrate the\nimportance of including emission lines in the narrow band fluxes. We show that\nPAUCam has sufficient resolution to measure the strength of the 4000\\AA{} break\nto the nominal PAUS depth. We predict the evolution of a narrow band luminosity\nfunction and show how this can be affected by the OII emission line. We\nintroduce new rest frame broad bands (UV and blue) that can be derived directly\nfrom the narrow band fluxes. We use these bands along with D4000 and redshift\nto define galaxy samples and provide predictions for galaxy clustering\nmeasurements. We show that systematic errors in the recovery of the projected\nclustering due to photometric redshift errors in PAUS are significantly smaller\nthan the expected statistical errors. The galaxy clustering on two halo scales\ncan be recovered quantatively without correction, and all qualitative trends\nseen in the one halo term are recovered. In this analysis mixing between\nsamples reduces the expected contrast between the one halo clustering of red\nand blue galaxies and demonstrates the importance of a mock catalogue for\ninterpreting galaxy clustering results. The mock catalogue is available on\nrequest at https://cosmohub.pic.es/home.\n",
        "method": "Here are the methodological sentences extracted from the abstract:\n\n* We present a mock catalogue for the Physics of the Accelerating Universe Survey (PAUS) and use it to quantify the competitiveness of the narrow band imaging...\n* The mock agrees with observed number count and redshift distribution data.\n* We demonstrate the importance of including emission lines in the narrow band fluxes.\n* We show that PAUCam has sufficient resolution to measure the strength of the 4000\\AA{} break to the nominal PAUS depth.\n* We predict the evolution of a narrow band luminosity function and show how this can be affected by the OII emission line."
    },
    {
        "abstract": "  We present a theoretical framework based on an extension of dynamical density\nfunctional theory (DDFT) for describing the structure and dynamics of cells in\nliving tissues and tumours. DDFT is a microscopic statistical mechanical theory\nfor the time evolution of the density distribution of interacting many-particle\nsystems. The theory accounts for cell pair-interactions, different cell types,\nphenotypes and cell birth and death processes (including cell division), in\norder to provide a biophysically consistent description of processes bridging\nacross the scales, including describing the tissue structure down to the level\nof the individual cells. Analysis of the model is presented for a single\nspecies and a two-species cases, the latter aimed at describing competition\nbetween tumour and healthy cells. In suitable parameter regimes, model results\nare consistent with biological observations. Of particular note, divergent\ntumour growth behaviour, mirroring metastatic and benign growth\ncharacteristics, are shown to be dependent on the cell pair-interaction\nparameters.\n",
        "method": "We present a theoretical framework based on an extension of dynamical density functional theory (DDFT) for describing the structure and dynamics of cells in living tissues and tumours."
    },
    {
        "abstract": "  Two notions of \"having a derivative of logarithmic order\" have been studied.\nThey come from the study of regularity of flows and renormalized solutions for\nthe transport and continuity equation associated to weakly differentiable\ndrifts.\n",
        "method": "Here is the methodological sentence:\n\nThe study comes from the study of regularity of flows and renormalized solutions for the transport and continuity equation associated to weakly differentiable drifts."
    },
    {
        "abstract": "  This paper presents a data-driven solution to the discrete-time infinite\nhorizon LQR problem. The state feedback gain is computed directly from a batch\nof input and state data collected from the plant. Simulation examples\nillustrate the convergence of the proposed solution to the optimal LQR gain as\nthe number of Markov parameters tends to infinity. Experiments in an\nuninterruptible power supply are presented, which demonstrate the practical\napplicability of the design methodology.\n",
        "method": "The state feedback gain is computed directly from a batch of input and state data collected from the plant."
    },
    {
        "abstract": "  Kliko is a Docker-based container specification for running one or multiple\nrelated compute jobs. The key concepts of Kliko are the encapsulation of data\nprocessing software into a container and the formalization of the input, output\nand task parameters. By formalizing the parameters, the software is represented\nas abstract building blocks with a uniform and consistent interface. The main\nadvantage is enhanced scriptability and empowering pipeline composition.\nFormalization is realized by bundling a container with a Kliko file, which\ndescribes the IO and task parameters. This Kliko container can then be opened\nand run by a Kliko runner. The Kliko runner will parse the Kliko definition and\ngather the values for these parameters, for example by requesting user input or\nretrieving pre-defined values from disk. Parameters can be various primitive\ntypes, for example: float, int or the path to a file. This paper will also\ndiscuss the implementation of a support library named Kliko which can be used\nto create Kliko containers, parse Kliko definitions, chain Kliko containers in\nworkflows using a workflow manager library such as Luigi. The Kliko library can\nbe used inside the container to interact with the Kliko runner. Finally, to\nillustrate the applicability of the Kliko definition, this paper will discuss\ntwo reference implementations based on the Kliko library: RODRIGUES, a\nweb-based Kliko container scheduler, and output visualizer specifically for\nastronomical data, and VerMeerKAT, a multi-container workflow data reduction\npipeline which is being used as a prototype pipeline for the commissioning of\nthe MeerKAT radio telescope.\n",
        "method": "Formalization is realized by bundling a container with a Kliko file, which describes the IO and task parameters."
    },
    {
        "abstract": "  Expectation maximization (EM) is a technique for estimating\nmaximum-likelihood parameters of a latent variable model given observed data by\nalternating between taking expectations of sufficient statistics, and\nmaximizing the expected log likelihood. For situations where sufficient\nstatistics are intractable, stochastic approximation EM (SAEM) is often used,\nwhich uses Monte Carlo techniques to approximate the expected log likelihood.\nTwo common implementations of SAEM, Batch EM (BEM) and online EM (OEM), are\nparameterized by a \"learning rate\", and their efficiency depend strongly on\nthis parameter. We propose an extension to the OEM algorithm, termed\nIntrospective Online Expectation Maximization (IOEM), which removes the need\nfor specifying this parameter by adapting the learning rate according to trends\nin the parameter updates. We show that our algorithm matches the efficiency of\nthe optimal BEM and OEM algorithms in multiple models, and that the efficiency\nof IOEM can exceed that of BEM/OEM methods with optimal learning rates when the\nmodel has many parameters. A Python implementation is available at\nhttps://github.com/luntergroup/IOEM.git.\n",
        "method": "Methodological sentences:\n\n* Alternating between taking expectations of sufficient statistics, and maximizing the expected log likelihood.\n* Using Monte Carlo techniques to approximate the expected log likelihood in situations where sufficient statistics are intractable.\n* Parameterizing Batch EM (BEM) and online EM (OEM) algorithms by a \"learning rate\", and their efficiency depending strongly on this parameter."
    },
    {
        "abstract": "  We prove and explain several classical formulae for homotopy (co)limits in\ngeneral (combinatorial) model categories which are not necessarily simplicially\nenriched. Importantly, we prove versions of the Bousfield-Kan formula and the\nfat totalization formula in this complete generality. We finish with a proof\nthat homotopy-final functors preserve homotopy limits, again in complete\ngenerality.\n",
        "method": "Here is the methodological sentence extracted from the abstract:\n\nWe prove versions of the Bousfield-Kan formula and the fat totalization formula..."
    },
    {
        "abstract": "  We have searched for the lepton-flavor-violating decay $B^{0}\\to K^{\\ast 0}\n\\mu^{\\pm} e^{\\mp}$ using a data sample of 711 $fb^{-1}$ that contains $772\n\\times 10^{6}$ $B\\bar{B}$ pairs. The data were collected near the $\\Upsilon\n(4S)$ resonance with the Belle detector at the KEKB asymmetric-energy\n$e^{+}e^{-}$ collider. No signals were observed, and we set 90% confidence\nlevel upper limits on the branching fractions of ${\\cal B}(B^{0}\\to K^{\\ast 0}\n\\mu^{+} e^{-})< 1.2\\times 10^{-7}$, ${\\cal B}(B^{0}\\to K^{\\ast 0} \\mu^{-}\ne^{+})< 1.6\\times 10^{-7}$, and, for both decays combined, ${\\cal B}(B^{0}\\to\nK^{\\ast 0} \\mu^{\\pm} e^{\\mp}) < 1.8\\times 10^{-7}$. These are the most\nstringent limits on these decays to date.\n",
        "method": "We have searched for the lepton- flavor-violating decay $B^{0}\\to K^{\\ast 0} \\mu^{\\pm} e^{\\mp}$ using a data sample of 711 $fb^{-1}$ that contains $772\\times 10^{6}$ $B\\bar{B}$ pairs."
    },
    {
        "abstract": "  We present results from an asymptotic magnetohydrodynamic model that is\nsuited for studying the rapidly rotating, low viscosity regime typical of the\nelectrically conducting fluid interiors of planets and stars. We show that the\npresence of sufficiently strong magnetic fields prevents the formation of\nlarge-scale vortices and saturates the inverse cascade at a finite\nlength-scale. This saturation corresponds to an equilibrated state in which the\nenergetics of the depth-averaged flows are characterized by a balance of\nconvective power input and ohmic dissipation. A quantitative criteria\ndelineating the transition between finite-size flows and domain-filling\n(large-scale) vortices in electrically conducting fluids is found. By making\nuse of the inferred and observed properties of planetary interiors, our results\nsuggest that convection-driven large-scale vortices do not form in the\nelectrically conducting regions of many bodies.\n",
        "method": "We present results from an asymptotic magnetohydrodynamic model that is suited for studying the rapidly rotating, low viscosity regime typical of the electrically conducting fluid interiors of planets and stars."
    },
    {
        "abstract": "  Currently, cellular action potentials are detected using either electrical\nrecordings or exogenous fluorescent probes sensing calcium concentration or\ntransmembrane voltage. Ca imaging has low temporal resolution, while voltage\nindicators are vulnerable to phototoxicity, photobleaching and heating. Here we\nreport full-field interferometric imaging of individual action potentials by\ndetecting the movement across the entire cell membrane. Using spike-triggered\naveraging of the movies synchronized to electrical recording, we demonstrate\ndeformations of up to 3 nm (0.9 mrad) during the action potential in spiking\nHEK-293 cells, with a rise time of 4 ms. The time course of the\noptically-recorded spikes matches electrical waveforms. Since the shot noise\nlimit of the camera (~2 mrad/pix) precludes detection of the action potential\nin a single frame, for all-optical spike detection, images are acquired at 50\nkHz, and 50 frames are binned into 1 ms steps to achieve a sensitivity of 0.3\nmrad in a single pixel. Using self-reinforcing sensitivity enhancement\nalgorithm based on iteratively expanding the region of interest for spatial\naveraging, individual spikes can be detected by matching the previously\nextracted template of the action potential with the optical recording. This\nallows all-optical full-field imaging of the propagating action potentials\nwithout exogeneous labels or electrodes.\n",
        "method": "Here is the methodological sentence:\n\nUsing spike-triggered averaging of the movies synchronized to electrical recording, we demonstrate deformations of up to 3 nm (0.9 mrad) during the action potential in spiking HEK-293 cells, with a rise time of 4 ms."
    }
]